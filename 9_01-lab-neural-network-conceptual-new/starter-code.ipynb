{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Conceptual Neural Networks Lab\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "In class, you've seen how to build a neural network in Keras. This lab is intended to help you understand exactly how a neural network is built and understand the math behind it. We will walk through the process of building a *very* simple neural network by hand.\n",
    "\n",
    "Let's start at the beginning. In general, a forward pass through a neural network works like this:\n",
    "1. Take our inputs and multiply each input by a weight.\n",
    "2. Add all of the results from step 1 together.\n",
    "3. Add a bias to the result from step 2.\n",
    "4. Pass the value from step 3 through an activation function.\n",
    "5. This is now passed to the next layer.\n",
    "\n",
    "![](./images/perceptron.jpeg)\n",
    "\n",
    "([*image source*](https://www.kdnuggets.com/2016/10/beginners-guide-neural-networks-python-scikit-learn.html))\n",
    "\n",
    "Let's code a simple one-layer neural network together!\n",
    "\n",
    "**The ONLY Python libraries you will need/can use in this lab are numpy and matplotlib!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy and matplotlib - this is all you will need for this lab!\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set a random seed\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 1:** Let's write a function called `update` that takes an array of our inputs, multiplies these by their weights, and sums them together with a bias. We will start with random initialization of our weights and bias. For simplicity, the network we will build will just have 2 inputs.\n",
    "\n",
    "For example,\n",
    "```python\n",
    "inputs = np.array([12, 4])\n",
    "weights = np.array([1, 2])\n",
    "bias = 1\n",
    "\n",
    "update(inputs, weights, bias)\n",
    "```\n",
    "Should return:\n",
    "```python\n",
    "21\n",
    "```\n",
    "\n",
    "Since: $ (12*1) + (4*2) + 1 = 21 $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs and initial weights/bias:\n",
    "\n",
    "inputs = np.array([12, 4])\n",
    "weights = np.array([1, 2])\n",
    "bias = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 554,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(inputs * weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer:\n",
    "\n",
    "def update(inputs, weights, bias):\n",
    "    return sum(inputs * weights) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 556,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update(inputs, weights, bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 2:** Now, let's code our activation function. For best performance, we often use ReLU as our activation function in hidden layers. Graphically, this looks like:\n",
    "\n",
    "![](./images/relu.png)\n",
    "\n",
    "([*image source*](https://medium.com/@danqing/a-practical-guide-to-relu-b83ca804f1f7))\n",
    "\n",
    "And can be written as:\n",
    "\n",
    "$$\n",
    "f(x) = \\left\\{\\begin{matrix}\n",
    "0 \\text{  for  } x<0\\\\\n",
    "x \\text{  for  } x\\geq 0\n",
    "\\end{matrix}\\right.\n",
    "$$\n",
    "\n",
    "Write a function called `relu` that takes in a value and returns the value after it has been passed through the ReLU function.\n",
    "\n",
    "For example,\n",
    "```python\n",
    "relu(-3)\n",
    "```\n",
    "Should return:\n",
    "```python\n",
    "0\n",
    "```\n",
    "Since $-3 < 0$, and\n",
    "```python\n",
    "relu(3)\n",
    "```\n",
    "Should return:\n",
    "```python\n",
    "3\n",
    "```\n",
    "Since $3 \\geq 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Answer:\n",
    "\n",
    "# def relu(num):\n",
    "#     return num * (num > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(num):\n",
    "    if num < 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 559,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu(-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 560,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 3:** Now, let's code our activation function for our output layer! Let's assume we are building a neural network for a regression problem. What activation function should we use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Answer: \n",
    "\n",
    "`No activation as it is regression`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 4:** Write a function called `output_activation` that takes in a value and passes it through the activation function you decided on in problem 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer:\n",
    "\n",
    "def output_activation(input):\n",
    "    return input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 5**: Put it together! We will use the functions we wrote above to build a simple neural network to predict the price of a home given home size and quality on a scale of 1-5. For simplicity, we will build a network with one hidden layer with only one node.\n",
    "\n",
    "Our general network architecture will look like this:\n",
    "\n",
    "![](./images/network_drawing.png)\n",
    "\n",
    "The simple data we will work with is given here:\n",
    "\n",
    "| square_feet | quality | price |\n",
    "| --- | --- | --- |\n",
    "| 1750 | 3 | \\$250,000 |\n",
    "| 2500 | 4 | \\$600,000 |\n",
    "| 4320 | 5 | \\$895,000 |\n",
    "| 1300 | 1 | \\$195,000 |\n",
    "\n",
    "We will normalize our data to ensure all values are between 0 and 1. This will help our neural network converge and ensure that we do not lose any weights during the fitting process (since any negative values will be set to 0 when they are passed through the ReLU function):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put data from above into a numpy array for modeling\n",
    "\n",
    "housing_data = np.array([[1750, 3, 250_000],\n",
    "                        [2500, 4, 600_000],\n",
    "                        [4320, 5, 895_000],\n",
    "                        [1300, 1, 195_000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split housing data into X & y\n",
    "X = housing_data[:,[0,1]]\n",
    "y = housing_data[:,2]\n",
    "\n",
    "# Normalize each column by scaling all values into the range [0, 1]\n",
    "#   - Avoids negative values for ReLU, which would prevent weight updates\n",
    "#   - ptp: peak-to-peak, i.e. the range of each column (max - min)\n",
    "X_scaled = (X - X.min(axis=0)) / X.ptp(axis=0)\n",
    "\n",
    "# Normalize y identically\n",
    "#    - Ensures the weights do not have to be enormously adjusted with a huge learning rate\n",
    "y_scaled = (y - y.min()) / y.ptp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we will only use **one observation** of our data to start!\n",
    "\n",
    "| square_feet | quality | price |\n",
    "| --- | --- | --- |\n",
    "| 1750 | 3 | \\$250,000 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the first row of the dataset for simplicity\n",
    "\n",
    "X_scaled_simple = X_scaled[0,:]\n",
    "y_simple = y_scaled[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.14900662, 0.5       ])"
      ]
     },
     "execution_count": 565,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled_simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will run this through our network!\n",
    "\n",
    "To do this, pass the `X_scaled_simple` data, `w1` and `w1` from the given weights vector, and `b1` from the given bias vector into your `update` function. Save the returned array as `sum1`.\n",
    "\n",
    "We will use randomly generated weights and bias to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize random weights and biases\n",
    "# We need 3 weights and 2 biases\n",
    "# If you are unsure why this is, go back to\n",
    "# the picture of the network architecture above!\n",
    "\n",
    "# # Weights & bias vectors (positive to avoid reLU zeroing out)\n",
    "# weights = np.abs(np.random.normal(size=3))  # [w1, w2, w3]\n",
    "# biases = np.abs(np.random.normal(size=2))   # [b1, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [0.3190391 , 0.24937038, 1.46210794]\n",
    "biases = [2.06014071, 0.3224172 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.14900662, 0.5       ])"
      ]
     },
     "execution_count": 568,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.232364838741722"
      ]
     },
     "execution_count": 569,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get sum1:\n",
    "# Answer:\n",
    "\n",
    "sum1 = update(X_scaled_simple, weights[0:2], biases[0])\n",
    "sum1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 6:** Apply your hidden layer activation function `relu` to your `sum1`. Save the result of this as `out1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.232364838741722"
      ]
     },
     "execution_count": 570,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get out1:\n",
    "# Answer:\n",
    "\n",
    "out1 = output_activation(relu(sum1))\n",
    "out1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 7**: We will now pass the output from the hidden layer to our output layer. To do this:\n",
    "1. Pass the `out1` result, `w3` from the given weights vector, and `b2` from the given bias vector through your `update` function. Save the result as `sum2`.\n",
    "2. Apply your output layer activation function `output_activation` to your `sum2`. Save the result of this as `out2`. This is our prediction of the price of that first home!\n",
    "\n",
    "*Hint: Your `update` function takes in an array instead of just a single number! You may have to convert `out1` to an array before running it through your `update` function!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example,\n",
    "```python\n",
    "inputs = np.array([12, 4])\n",
    "weights = np.array([1, 2])\n",
    "bias = 1\n",
    "\n",
    "update(inputs, weights, bias)\n",
    "```\n",
    "Should return:\n",
    "```python\n",
    "21\n",
    "```\n",
    "\n",
    "Since: $ (12*1) + (4*2) + 1 = 21 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.5863755557010912"
      ]
     },
     "execution_count": 571,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get sum2 and out2:\n",
    "# Answer:\n",
    "\n",
    "sum2 = update(np.array([out1]), weights[2], biases[1])\n",
    "sum2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.5863755557010912"
      ]
     },
     "execution_count": 572,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2 = output_activation(relu(sum2))\n",
    "out2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 8**: Put it together! Write a function called `forward` that goes through a complete forward pass (combines all of your steps from problems 5-7). This function should take in a row of the data, weights, and biases and:\n",
    "1. Calculates `sum1`: the result of passing the row of data, the weights, and the bias through your `update` function.\n",
    "2. Calculates `out1`: the result of applying your hidden layer activation function `relu` to your `sum1` result from step 1.\n",
    "3. Calculates `sum2`: the result of passing `out1`, the weights, and the bias through your `update` function.\n",
    "4. Calculates `out2`: the result of applying your output layer activation function `output_activation` to your `sum2` result from step 3.\n",
    "5. Returns `sum1`, `out1`, `sum2`, and `out2`!\n",
    "\n",
    "You have now written a function that completes one forward pass through our neural network! To check your work, you should get the same results from problems 5-7 here (assuming you are using the same randomly generated weights and biases vectors)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Answer:\n",
    "\n",
    "# def forward(X_scaled_simple, weights, bias):\n",
    "#     sum1 = update(X_scaled_simple, weights[0:2], bias[0])\n",
    "#     out1 = output_activation(relu(sum1))\n",
    "#     sum2 = update(np.array(out1), weights[2:3], bias[1])\n",
    "#     out2 = output_activation(relu(sum2))\n",
    "\n",
    "#     return np.array([sum1,out1,sum2,out2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [0.3190391 , 0.24937038, 1.46210794]\n",
    "biases = [2.06014071, 0.3224172 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.14900662, 0.5       ])"
      ]
     },
     "execution_count": 575,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(scaled_dat, weights, bias):\n",
    "    sum1=update(scaled_dat, weights[0:2], bias[0])\n",
    "    out1=relu(sum1)\n",
    "    sum2 = update(np.array(out1), weights[2:3], bias[1])\n",
    "    out2 = output_activation(sum2)\n",
    "    return np.array([sum1, out1, sum2, out2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.23236484, 2.23236484, 3.58637556, 3.58637556])"
      ]
     },
     "execution_count": 577,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward(X_scaled_simple, weights, biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 9:** Now, we need to quantify our loss. Since this is a regression problem, we will use the mean squared error (MSE) loss, which can be written as:\n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_{true}-y_{pred})^2 \n",
    "$$\n",
    "\n",
    "Write a function called `mse` that takes in two arrays, `y_true` and `y_pred` and returns the mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loss function:\n",
    "# # Answer:\n",
    "\n",
    "# def mse(y_true,y_pred):\n",
    "#     n = y_true.shape[0]\n",
    "#     return (1/n) * sum((y_true - y_pred)**2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_true, y_pred):\n",
    "    return np.square(np.subtract(y_true,y_pred)).mean() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, find the mean squared error between your `out2` (y_pred) and `y_simple` (y_true). Since your function took in arrays and we are just working with one example for now, we will convert these values to arrays of length 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert y_simple and out2 to arrays to use in function:\n",
    "\n",
    "y_simple = np.array([y_simple])\n",
    "output = np.array([out2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.304689794307894"
      ]
     },
     "execution_count": 581,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse(y_simple, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How is your model doing? Probably very bad! This is where the magic happens: backpropagation. Backpropagation is the process of using gradient descent to update our weights and biases in order to try to minimize the loss function (mean squared error in this case). Check out the gradient descent lesson if you need a reminder on how this works.\n",
    "\n",
    "We will be using the terms $w_1$, $h_1$, $w_3$, etc. for this next part. Here is a reminder of our network that labels all of the weights with these terms. As a simple example, we will work our way backwards through one portion of the network to update $w_1$ (through $o_1$ (predicted price), $w_3$, and $h_1$).\n",
    "\n",
    "![](./images/network_drawing.png)\n",
    "\n",
    "We can define our new weights (and biases) using gradient descent. Again, for simplicity we will just work through updating $w_1$:\n",
    "\n",
    "$$\n",
    "w_1 \\leftarrow w_1 - \\lambda \\frac{\\partial L}{\\partial w_1}\n",
    "$$\n",
    "\n",
    "Since we work backwards through our network with multiple variables, we calculate the derivative of the loss with respect to $w_1$ ($ \\frac{\\partial L}{\\partial w_1} $) using [partial derivatives](https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/partial-derivative-and-gradient-articles/a/introduction-to-partial-derivatives):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_1} = \\frac{\\partial L}{\\partial y_{pred}} * \\frac{\\partial y_{pred}}{\\partial h_1} * \\frac{\\partial h_1}{\\partial w_1}\n",
    "$$\n",
    "\n",
    "Where $L$ is the loss, $w_1$ is the first weight, $h_1$ is the output from the first hidden layer, and $y_{pred}$ is the predicted value of y.\n",
    "\n",
    "For those of you familiar with calculus, feel free to try to compute these partial derivatives on your own! For those of you not as familiar, I will give you each of these for our scenario. Remember, we are using MSE as our loss function and ReLU as our hidden layer activation function:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial y_{pred}} = -2(y_{true}-y_{pred})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y_{pred}}{\\partial h_1} = w_3*f'(w_3h_1+b_2)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial h_1}{\\partial w_1} = x_1*f'(w_1x_1+w_2x_2+b_1)\n",
    "$$\n",
    "\n",
    "Where $f'$ is the derivative of the ReLU function:\n",
    "\n",
    "$$\n",
    "f'(x) = \\left\\{\\begin{matrix}\n",
    "0 \\text{  for  } x<0\\\\\n",
    "1 \\text{  for  } x\\geq 0\n",
    "\\end{matrix}\\right.\n",
    "$$\n",
    "\n",
    "These last two work because of the [chain rule](https://www.khanacademy.org/math/ap-calculus-ab/ab-differentiation-2-new/ab-3-1a/a/chain-rule-review).\n",
    "\n",
    "We now have all the pieces to update our weights and biases! We are technically using stochastic gradient descent to update our weights and biases here, since we only operate on one sample at a time. The general process for backpropagation will be:\n",
    "1. Select one sample from our dataset.\n",
    "2. Calculate the partial derivatives.\n",
    "3. Use the equation from gradient descent to update each weight and bias ($w_1 \\leftarrow w_1 - \\lambda \\frac{\\partial L}{\\partial w_1}$).\n",
    "4. Repeat for each sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 10:** Write a function called `relu_deriv` that is the derivative of the ReLU function:\n",
    "\n",
    "$$\n",
    "f'(x) = \\left\\{\\begin{matrix}\n",
    "0 \\text{  for  } x<0\\\\\n",
    "1 \\text{  for  } x\\geq 0\n",
    "\\end{matrix}\\right.\n",
    "$$\n",
    "\n",
    "This function should take in an input and return the derivative of the ReLU function at that input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Derivative of ReLU:\n",
    "# # Answer:\n",
    "\n",
    "# def relu_deriv(num):\n",
    "#     return 1 * (num > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_deriv(num):\n",
    "    if num < 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "\n",
    "We are now ready to create a neural network from scratch! \n",
    "\n",
    "Again, for simplicity all we will do right now is write code that updates $w_1$ to start. (In order to actually minimize our loss function, we will need to expand this to update $w_2$, $w_3$, $b_1$, and $b_2$ as well. We will do this later on.)\n",
    "\n",
    "![](./images/network_drawing.png)\n",
    "\n",
    "*Note*: this is definitely not going to find an optimal model since this is an extremely simplified example. Hopefully you have developed more of an appreciation for Keras!\n",
    "\n",
    "**Problem 11:** Write a function called `update_weights` that takes in your X data, y data, weights, biases, and a learning rate ($\\lambda$) in order to update the weights using gradient descent. This function should:\n",
    "1. Go through one forward pass of your network to generate a prediction (use your `forward` function here) for each row in your dataset.\n",
    "2. Goes through the process of backpropagation to find $\\frac{\\partial L}{\\partial w_1}$: $\\frac{\\partial L}{\\partial w_1} = \\frac{\\partial L}{\\partial y_{pred}} * \\frac{\\partial y_{pred}}{\\partial h_1} * \\frac{\\partial h_1}{\\partial w_1}$ (this code is given for you).\n",
    "3. Updates $w_1$ using gradient descent ($w_1 \\leftarrow w_1 - \\lambda \\frac{\\partial L}{\\partial w_1}$).\n",
    "4. Calculates the loss.\n",
    "5. Returns the loss.\n",
    "\n",
    "Comments and starter code is given to help you work through this problem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.23236484, 2.23236484, 3.58637556, 3.58637556])"
      ]
     },
     "execution_count": 584,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward(X_scaled_simple, weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(X, y_true, weights, biases, learning_rate):\n",
    "    \"\"\" Updates the weight values. Returns the resulting loss. \"\"\"\n",
    "    \n",
    "    # Initialize prediction array with 0's\n",
    "    y_preds = np.zeros_like(y_true)\n",
    "    \n",
    "    # loop through each row in the dataset\n",
    "    for i, (row,y_actual) in enumerate(zip(X, y_true)):\n",
    "        # TO DO: get values for sum1, out1, sum2, and out2 using your forward function\n",
    "        sum1,out1,sum2,out2 = forward(row,weights,biases)\n",
    "        \n",
    "        # TO DO: save out2 as the corresponding position in y_pred\n",
    "        y_preds[i] = out2\n",
    "\n",
    "        # Backpropagation\n",
    "        dl_dypred = 2 * (out2 - y_actual)\n",
    "        dypred_dh1 = weights[2] * relu_deriv(sum2)\n",
    "        dh1_dw1 = row[0] * relu_deriv(sum1)\n",
    "\n",
    "        # TO DO: use gradient descent to update w1\n",
    "        weights[0] -= learning_rate * dl_dypred * dypred_dh1 * dh1_dw1\n",
    "        print(weights)\n",
    "    # TO DO: calculate loss\n",
    "    loss = mse(y_true,y_preds)\n",
    "    \n",
    "    # TO DO: return the loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def update_weights(X, y_true, weights, biases, learning_rate):\n",
    "#     \"\"\" Updates the weight values. Returns the resulting loss. \"\"\"\n",
    "    \n",
    "#     # Initialize prediction array with 0's\n",
    "#     y_preds = np.zeros_like(y_true)\n",
    "    \n",
    "#     # loop through each row in the dataset\n",
    "#     for i, (row,y_actual) in enumerate(zip(X, y_true)):\n",
    "#         # TO DO: get values for sum1, out1, sum2, and out2 using your forward function\n",
    "#         sum1,out1,sum2,out2 = forward(X, weights, biases)\n",
    "        \n",
    "#         # TO DO: save out2 as the corresponding position in y_pred\n",
    "#         y_preds[i] = out2\n",
    "\n",
    "#         # Backpropagation\n",
    "#         dl_dypred = 2 * (out2 - y_actual)\n",
    "#         dypred_dh1 = weights[2] * relu_deriv(sum2)\n",
    "#         dh1_dw1 = row[0] * relu_deriv(sum1)\n",
    "\n",
    "#         # TO DO: use gradient descent to update w1\n",
    "#         weights[0] -= dl_dypred*dypred_dh1*dh1_dw1*learning_rate\n",
    "    \n",
    "#     # TO DO: calculate loss\n",
    "#     loss = mse(y_true, y_preds)\n",
    "    \n",
    "#     # TO DO: return the loss\n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.05557869407125275, 0.004347907201697416, 0.5406787635235273]\n",
      "[-0.05621949295675986, 0.004347907201697416, 0.5406787635235273]\n",
      "[-0.04833142634117117, 0.004347907201697416, 0.5406787635235273]\n",
      "[-0.04833142634117117, 0.004347907201697416, 0.5406787635235273]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.22920677920801674"
      ]
     },
     "execution_count": 601,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_weights(X_scaled, y_scaled, weights, biases, learning_rate = 0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats! You've updated $w_1$ using gradient descent and returned your loss! Now, lets put this all together to update $w_1$ in our network to actually train our model. Since this is challenging, this function is given for you. As long as you have successfully completed the problems up to this point, your neural network should work and we can actually train our model!\n",
    "\n",
    "The first function is called `plot_predictions` and allows us to visualize how our model is doing by plotting the loss via gradient descent and the difference between the true square footage and our model's predictions.\n",
    "\n",
    "The second function is called `train` and trains our model by taking in your X data, y data, a learning rate, and a certain number of epochs. For each epoch, the network:\n",
    "1. Goes through a forward pass using your `forward` function.\n",
    "2. Goes through a backward pass to calculate the loss using your `update_weights` function.\n",
    "3. Uses the `plot_predictions` function to visualize how our model is doing!\n",
    "4. Repeats steps 1-3 for the given number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(X, y_true, weights, biases):\n",
    "    # Loss scatter plot titles\n",
    "    plt.title('Loss via Gradient Descent')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    \n",
    "    # Side-by-side prediction plots\n",
    "    fig, ax = plt.subplots(1, 2, sharey=True)\n",
    "    fig.suptitle('Actual (blue) vs. predicted (orange)')\n",
    "    ax[0].set_ylabel('price (normalized)')\n",
    "    ax[0].set_xlabel('square feet (normalized)')\n",
    "    ax[1].set_xlabel('quality (normalized)')\n",
    "    \n",
    "    # Display each predicted value\n",
    "    for i, (row, y_actual) in enumerate(zip(X, y_true)):\n",
    "        *_, y_pred = forward(row, weights, biases)\n",
    "        ax[0].scatter(row[0], y_pred, c='orange')\n",
    "        ax[0].scatter(row[0], y_actual, c='b')\n",
    "        ax[1].scatter(row[1], y_pred, c='orange')\n",
    "        ax[1].scatter(row[1], y_actual, c='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create code for training\n",
    "def train(X, y_true, learning_rate, epochs):\n",
    "    \"\"\" Initialize and train netork for given epochs.\"\"\"\n",
    "    \n",
    "    # Initialize random weights and biases:\n",
    "    weights = np.abs(np.random.normal(size=3))  # [w1, w2, w3]\n",
    "    biases = np.abs(np.random.normal(size=2))   # [b1, b2]\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass + Backpropagation + Gradient Descent\n",
    "        loss = update_weights(X, y_true, weights, biases, learning_rate)\n",
    "      \n",
    "        #print(f'Loss at epoch {epoch}: {loss}')\n",
    "        plt.scatter(epoch, loss, c='b')\n",
    "    \n",
    "    print('Final loss:', loss)\n",
    "    plot_predictions(X, y_true, weights, biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the `train` function on the `X_scaled` and `y_scaled` data! Your loss should be decreasing each epoch, but this will be a sub-optimal model since we are only updating $w_1$ for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0837531  0.99734545 0.2829785 ]\n",
      "[1.08033574 0.99734545 0.2829785 ]\n",
      "[1.07362589 0.99734545 0.2829785 ]\n",
      "[1.07362589 0.99734545 0.2829785 ]\n",
      "[1.07174924 0.99734545 0.2829785 ]\n",
      "[1.06833795 0.99734545 0.2829785 ]\n",
      "[1.06166652 0.99734545 0.2829785 ]\n",
      "[1.06166652 0.99734545 0.2829785 ]\n",
      "[1.05979073 0.99734545 0.2829785 ]\n",
      "[1.05638548 0.99734545 0.2829785 ]\n",
      "[1.04975234 0.99734545 0.2829785 ]\n",
      "[1.04975234 0.99734545 0.2829785 ]\n",
      "[1.04787739 0.99734545 0.2829785 ]\n",
      "[1.04447817 0.99734545 0.2829785 ]\n",
      "[1.03788317 0.99734545 0.2829785 ]\n",
      "[1.03788317 0.99734545 0.2829785 ]\n",
      "[1.03600907 0.99734545 0.2829785 ]\n",
      "[1.03261585 0.99734545 0.2829785 ]\n",
      "[1.02605885 0.99734545 0.2829785 ]\n",
      "[1.02605885 0.99734545 0.2829785 ]\n",
      "[1.02418559 0.99734545 0.2829785 ]\n",
      "[1.02079835 0.99734545 0.2829785 ]\n",
      "[1.01427919 0.99734545 0.2829785 ]\n",
      "[1.01427919 0.99734545 0.2829785 ]\n",
      "[1.01240677 0.99734545 0.2829785 ]\n",
      "[1.00902549 0.99734545 0.2829785 ]\n",
      "[1.00254404 0.99734545 0.2829785 ]\n",
      "[1.00254404 0.99734545 0.2829785 ]\n",
      "[1.00067245 0.99734545 0.2829785 ]\n",
      "[0.99729711 0.99734545 0.2829785 ]\n",
      "[0.99085323 0.99734545 0.2829785 ]\n",
      "[0.99085323 0.99734545 0.2829785 ]\n",
      "[0.98898247 0.99734545 0.2829785 ]\n",
      "[0.98561304 0.99734545 0.2829785 ]\n",
      "[0.97920659 0.99734545 0.2829785 ]\n",
      "[0.97920659 0.99734545 0.2829785 ]\n",
      "[0.97733666 0.99734545 0.2829785 ]\n",
      "[0.97397311 0.99734545 0.2829785 ]\n",
      "[0.96760394 0.99734545 0.2829785 ]\n",
      "[0.96760394 0.99734545 0.2829785 ]\n",
      "[0.96573484 0.99734545 0.2829785 ]\n",
      "[0.96237716 0.99734545 0.2829785 ]\n",
      "[0.95604513 0.99734545 0.2829785 ]\n",
      "[0.95604513 0.99734545 0.2829785 ]\n",
      "[0.95417685 0.99734545 0.2829785 ]\n",
      "[0.95082502 0.99734545 0.2829785 ]\n",
      "[0.94452999 0.99734545 0.2829785 ]\n",
      "[0.94452999 0.99734545 0.2829785 ]\n",
      "[0.94266253 0.99734545 0.2829785 ]\n",
      "[0.93931652 0.99734545 0.2829785 ]\n",
      "[0.93305836 0.99734545 0.2829785 ]\n",
      "[0.93305836 0.99734545 0.2829785 ]\n",
      "[0.93119171 0.99734545 0.2829785 ]\n",
      "[0.9278515  0.99734545 0.2829785 ]\n",
      "[0.92163006 0.99734545 0.2829785 ]\n",
      "[0.92163006 0.99734545 0.2829785 ]\n",
      "[0.91976423 0.99734545 0.2829785 ]\n",
      "[0.9164298  0.99734545 0.2829785 ]\n",
      "[0.91024495 0.99734545 0.2829785 ]\n",
      "[0.91024495 0.99734545 0.2829785 ]\n",
      "[0.90837992 0.99734545 0.2829785 ]\n",
      "[0.90505125 0.99734545 0.2829785 ]\n",
      "[0.89890284 0.99734545 0.2829785 ]\n",
      "[0.89890284 0.99734545 0.2829785 ]\n",
      "[0.89703862 0.99734545 0.2829785 ]\n",
      "[0.89371568 0.99734545 0.2829785 ]\n",
      "[0.88760359 0.99734545 0.2829785 ]\n",
      "[0.88760359 0.99734545 0.2829785 ]\n",
      "[0.88574017 0.99734545 0.2829785 ]\n",
      "[0.88242295 0.99734545 0.2829785 ]\n",
      "[0.87634702 0.99734545 0.2829785 ]\n",
      "[0.87634702 0.99734545 0.2829785 ]\n",
      "[0.87448441 0.99734545 0.2829785 ]\n",
      "[0.87117288 0.99734545 0.2829785 ]\n",
      "[0.86513299 0.99734545 0.2829785 ]\n",
      "[0.86513299 0.99734545 0.2829785 ]\n",
      "[0.86327117 0.99734545 0.2829785 ]\n",
      "[0.85996531 0.99734545 0.2829785 ]\n",
      "[0.85396131 0.99734545 0.2829785 ]\n",
      "[0.85396131 0.99734545 0.2829785 ]\n",
      "[0.85210029 0.99734545 0.2829785 ]\n",
      "[0.84880008 0.99734545 0.2829785 ]\n",
      "[0.84283185 0.99734545 0.2829785 ]\n",
      "[0.84283185 0.99734545 0.2829785 ]\n",
      "[0.84097162 0.99734545 0.2829785 ]\n",
      "[0.83767704 0.99734545 0.2829785 ]\n",
      "[0.83174444 0.99734545 0.2829785 ]\n",
      "[0.83174444 0.99734545 0.2829785 ]\n",
      "[0.82988499 0.99734545 0.2829785 ]\n",
      "[0.82659602 0.99734545 0.2829785 ]\n",
      "[0.82069891 0.99734545 0.2829785 ]\n",
      "[0.82069891 0.99734545 0.2829785 ]\n",
      "[0.81884025 0.99734545 0.2829785 ]\n",
      "[0.81555686 0.99734545 0.2829785 ]\n",
      "[0.80969511 0.99734545 0.2829785 ]\n",
      "[0.80969511 0.99734545 0.2829785 ]\n",
      "[0.80783724 0.99734545 0.2829785 ]\n",
      "[0.80455941 0.99734545 0.2829785 ]\n",
      "[0.79873289 0.99734545 0.2829785 ]\n",
      "[0.79873289 0.99734545 0.2829785 ]\n",
      "[0.79687579 0.99734545 0.2829785 ]\n",
      "[0.79360351 0.99734545 0.2829785 ]\n",
      "[0.78781208 0.99734545 0.2829785 ]\n",
      "[0.78781208 0.99734545 0.2829785 ]\n",
      "[0.78595576 0.99734545 0.2829785 ]\n",
      "[0.782689   0.99734545 0.2829785 ]\n",
      "[0.77693253 0.99734545 0.2829785 ]\n",
      "[0.77693253 0.99734545 0.2829785 ]\n",
      "[0.77507699 0.99734545 0.2829785 ]\n",
      "[0.77181573 0.99734545 0.2829785 ]\n",
      "[0.76609408 0.99734545 0.2829785 ]\n",
      "[0.76609408 0.99734545 0.2829785 ]\n",
      "[0.76423931 0.99734545 0.2829785 ]\n",
      "[0.76098353 0.99734545 0.2829785 ]\n",
      "[0.75529659 0.99734545 0.2829785 ]\n",
      "[0.75529659 0.99734545 0.2829785 ]\n",
      "[0.75344258 0.99734545 0.2829785 ]\n",
      "[0.75019226 0.99734545 0.2829785 ]\n",
      "[0.74453988 0.99734545 0.2829785 ]\n",
      "[0.74453988 0.99734545 0.2829785 ]\n",
      "[0.74268664 0.99734545 0.2829785 ]\n",
      "[0.73944176 0.99734545 0.2829785 ]\n",
      "[0.73382382 0.99734545 0.2829785 ]\n",
      "[0.73382382 0.99734545 0.2829785 ]\n",
      "[0.73197134 0.99734545 0.2829785 ]\n",
      "[0.72873188 0.99734545 0.2829785 ]\n",
      "[0.72314824 0.99734545 0.2829785 ]\n",
      "[0.72314824 0.99734545 0.2829785 ]\n",
      "[0.72129652 0.99734545 0.2829785 ]\n",
      "[0.71806246 0.99734545 0.2829785 ]\n",
      "[0.71251299 0.99734545 0.2829785 ]\n",
      "[0.71251299 0.99734545 0.2829785 ]\n",
      "[0.71066203 0.99734545 0.2829785 ]\n",
      "[0.70743334 0.99734545 0.2829785 ]\n",
      "[0.70191792 0.99734545 0.2829785 ]\n",
      "[0.70191792 0.99734545 0.2829785 ]\n",
      "[0.70006771 0.99734545 0.2829785 ]\n",
      "[0.69684439 0.99734545 0.2829785 ]\n",
      "[0.69136289 0.99734545 0.2829785 ]\n",
      "[0.69136289 0.99734545 0.2829785 ]\n",
      "[0.68951343 0.99734545 0.2829785 ]\n",
      "[0.68629544 0.99734545 0.2829785 ]\n",
      "[0.68084772 0.99734545 0.2829785 ]\n",
      "[0.68084772 0.99734545 0.2829785 ]\n",
      "[0.67899901 0.99734545 0.2829785 ]\n",
      "[0.67578634 0.99734545 0.2829785 ]\n",
      "[0.67037229 0.99734545 0.2829785 ]\n",
      "[0.67037229 0.99734545 0.2829785 ]\n",
      "[0.66852432 0.99734545 0.2829785 ]\n",
      "[0.66531695 0.99734545 0.2829785 ]\n",
      "[0.65993643 0.99734545 0.2829785 ]\n",
      "[0.65993643 0.99734545 0.2829785 ]\n",
      "[0.65808921 0.99734545 0.2829785 ]\n",
      "[0.65488711 0.99734545 0.2829785 ]\n",
      "[0.64954    0.99734545 0.2829785 ]\n",
      "[0.64954    0.99734545 0.2829785 ]\n",
      "[0.64769352 0.99734545 0.2829785 ]\n",
      "[0.64449668 0.99734545 0.2829785 ]\n",
      "[0.63918285 0.99734545 0.2829785 ]\n",
      "[0.63918285 0.99734545 0.2829785 ]\n",
      "[0.6373371  0.99734545 0.2829785 ]\n",
      "[0.6341455  0.99734545 0.2829785 ]\n",
      "[0.62886483 0.99734545 0.2829785 ]\n",
      "[0.62886483 0.99734545 0.2829785 ]\n",
      "[0.62701981 0.99734545 0.2829785 ]\n",
      "[0.62383343 0.99734545 0.2829785 ]\n",
      "[0.61858579 0.99734545 0.2829785 ]\n",
      "[0.61858579 0.99734545 0.2829785 ]\n",
      "[0.6167415  0.99734545 0.2829785 ]\n",
      "[0.61356032 0.99734545 0.2829785 ]\n",
      "[0.60834558 0.99734545 0.2829785 ]\n",
      "[0.60834558 0.99734545 0.2829785 ]\n",
      "[0.60650202 0.99734545 0.2829785 ]\n",
      "[0.60332602 0.99734545 0.2829785 ]\n",
      "[0.59814406 0.99734545 0.2829785 ]\n",
      "[0.59814406 0.99734545 0.2829785 ]\n",
      "[0.59630123 0.99734545 0.2829785 ]\n",
      "[0.59313038 0.99734545 0.2829785 ]\n",
      "[0.58798108 0.99734545 0.2829785 ]\n",
      "[0.58798108 0.99734545 0.2829785 ]\n",
      "[0.58613898 0.99734545 0.2829785 ]\n",
      "[0.58297327 0.99734545 0.2829785 ]\n",
      "[0.5778565  0.99734545 0.2829785 ]\n",
      "[0.5778565  0.99734545 0.2829785 ]\n",
      "[0.57601512 0.99734545 0.2829785 ]\n",
      "[0.57285453 0.99734545 0.2829785 ]\n",
      "[0.56777017 0.99734545 0.2829785 ]\n",
      "[0.56777017 0.99734545 0.2829785 ]\n",
      "[0.5659295  0.99734545 0.2829785 ]\n",
      "[0.56277402 0.99734545 0.2829785 ]\n",
      "[0.55772195 0.99734545 0.2829785 ]\n",
      "[0.55772195 0.99734545 0.2829785 ]\n",
      "[0.55588199 0.99734545 0.2829785 ]\n",
      "[0.55273159 0.99734545 0.2829785 ]\n",
      "[0.54771169 0.99734545 0.2829785 ]\n",
      "[0.54771169 0.99734545 0.2829785 ]\n",
      "[0.54587244 0.99734545 0.2829785 ]\n",
      "[0.5427271  0.99734545 0.2829785 ]\n",
      "[0.53773925 0.99734545 0.2829785 ]\n",
      "[0.53773925 0.99734545 0.2829785 ]\n",
      "[0.53590071 0.99734545 0.2829785 ]\n",
      "[0.53276041 0.99734545 0.2829785 ]\n",
      "[0.52780448 0.99734545 0.2829785 ]\n",
      "[0.52780448 0.99734545 0.2829785 ]\n",
      "[0.52596665 0.99734545 0.2829785 ]\n",
      "[0.52283138 0.99734545 0.2829785 ]\n",
      "[0.51790725 0.99734545 0.2829785 ]\n",
      "[0.51790725 0.99734545 0.2829785 ]\n",
      "[0.51607013 0.99734545 0.2829785 ]\n",
      "[0.51293985 0.99734545 0.2829785 ]\n",
      "[0.50804741 0.99734545 0.2829785 ]\n",
      "[0.50804741 0.99734545 0.2829785 ]\n",
      "[0.50621099 0.99734545 0.2829785 ]\n",
      "[0.5030857  0.99734545 0.2829785 ]\n",
      "[0.49822482 0.99734545 0.2829785 ]\n",
      "[0.49822482 0.99734545 0.2829785 ]\n",
      "[0.4963891  0.99734545 0.2829785 ]\n",
      "[0.49326878 0.99734545 0.2829785 ]\n",
      "[0.48843934 0.99734545 0.2829785 ]\n",
      "[0.48843934 0.99734545 0.2829785 ]\n",
      "[0.48660431 0.99734545 0.2829785 ]\n",
      "[0.48348894 0.99734545 0.2829785 ]\n",
      "[0.47869083 0.99734545 0.2829785 ]\n",
      "[0.47869083 0.99734545 0.2829785 ]\n",
      "[0.4768565  0.99734545 0.2829785 ]\n",
      "[0.47374606 0.99734545 0.2829785 ]\n",
      "[0.46897915 0.99734545 0.2829785 ]\n",
      "[0.46897915 0.99734545 0.2829785 ]\n",
      "[0.46714551 0.99734545 0.2829785 ]\n",
      "[0.46403998 0.99734545 0.2829785 ]\n",
      "[0.45930417 0.99734545 0.2829785 ]\n",
      "[0.45930417 0.99734545 0.2829785 ]\n",
      "[0.45747121 0.99734545 0.2829785 ]\n",
      "[0.45437057 0.99734545 0.2829785 ]\n",
      "[0.44966573 0.99734545 0.2829785 ]\n",
      "[0.44966573 0.99734545 0.2829785 ]\n",
      "[0.44783346 0.99734545 0.2829785 ]\n",
      "[0.4447377  0.99734545 0.2829785 ]\n",
      "[0.44006371 0.99734545 0.2829785 ]\n",
      "[0.44006371 0.99734545 0.2829785 ]\n",
      "[0.43823212 0.99734545 0.2829785 ]\n",
      "[0.43514122 0.99734545 0.2829785 ]\n",
      "[0.43049797 0.99734545 0.2829785 ]\n",
      "[0.43049797 0.99734545 0.2829785 ]\n",
      "[0.42866706 0.99734545 0.2829785 ]\n",
      "[0.42558099 0.99734545 0.2829785 ]\n",
      "[0.42096836 0.99734545 0.2829785 ]\n",
      "[0.42096836 0.99734545 0.2829785 ]\n",
      "[0.41913813 0.99734545 0.2829785 ]\n",
      "[0.41605688 0.99734545 0.2829785 ]\n",
      "[0.41147476 0.99734545 0.2829785 ]\n",
      "[0.41147476 0.99734545 0.2829785 ]\n",
      "[0.40964521 0.99734545 0.2829785 ]\n",
      "[0.40656876 0.99734545 0.2829785 ]\n",
      "[0.40201703 0.99734545 0.2829785 ]\n",
      "[0.40201703 0.99734545 0.2829785 ]\n",
      "[0.40018814 0.99734545 0.2829785 ]\n",
      "[0.39711648 0.99734545 0.2829785 ]\n",
      "[0.39259502 0.99734545 0.2829785 ]\n",
      "[0.39259502 0.99734545 0.2829785 ]\n",
      "[0.39076681 0.99734545 0.2829785 ]\n",
      "[0.38769991 0.99734545 0.2829785 ]\n",
      "[0.38320862 0.99734545 0.2829785 ]\n",
      "[0.38320862 0.99734545 0.2829785 ]\n",
      "[0.38138107 0.99734545 0.2829785 ]\n",
      "[0.37831892 0.99734545 0.2829785 ]\n",
      "[0.37385767 0.99734545 0.2829785 ]\n",
      "[0.37385767 0.99734545 0.2829785 ]\n",
      "[0.3720308  0.99734545 0.2829785 ]\n",
      "[0.36897337 0.99734545 0.2829785 ]\n",
      "[0.36454206 0.99734545 0.2829785 ]\n",
      "[0.36454206 0.99734545 0.2829785 ]\n",
      "[0.36271584 0.99734545 0.2829785 ]\n",
      "[0.35966313 0.99734545 0.2829785 ]\n",
      "[0.35526164 0.99734545 0.2829785 ]\n",
      "[0.35526164 0.99734545 0.2829785 ]\n",
      "[0.35343608 0.99734545 0.2829785 ]\n",
      "[0.35038806 0.99734545 0.2829785 ]\n",
      "[0.34601628 0.99734545 0.2829785 ]\n",
      "[0.34601628 0.99734545 0.2829785 ]\n",
      "[0.34419138 0.99734545 0.2829785 ]\n",
      "[0.34114803 0.99734545 0.2829785 ]\n",
      "[0.33680585 0.99734545 0.2829785 ]\n",
      "[0.33680585 0.99734545 0.2829785 ]\n",
      "[0.33498161 0.99734545 0.2829785 ]\n",
      "[0.33194291 0.99734545 0.2829785 ]\n",
      "[0.32763022 0.99734545 0.2829785 ]\n",
      "[0.32763022 0.99734545 0.2829785 ]\n",
      "[0.32580663 0.99734545 0.2829785 ]\n",
      "[0.32277257 0.99734545 0.2829785 ]\n",
      "[0.31848925 0.99734545 0.2829785 ]\n",
      "[0.31848925 0.99734545 0.2829785 ]\n",
      "[0.31666631 0.99734545 0.2829785 ]\n",
      "[0.31363688 0.99734545 0.2829785 ]\n",
      "[0.30938282 0.99734545 0.2829785 ]\n",
      "[0.30938282 0.99734545 0.2829785 ]\n",
      "[0.30756052 0.99734545 0.2829785 ]\n",
      "[0.3045357  0.99734545 0.2829785 ]\n",
      "[0.30031079 0.99734545 0.2829785 ]\n",
      "[0.30031079 0.99734545 0.2829785 ]\n",
      "[0.29848914 0.99734545 0.2829785 ]\n",
      "[0.29546891 0.99734545 0.2829785 ]\n",
      "[0.29127304 0.99734545 0.2829785 ]\n",
      "[0.29127304 0.99734545 0.2829785 ]\n",
      "[0.28945203 0.99734545 0.2829785 ]\n",
      "[0.28643637 0.99734545 0.2829785 ]\n",
      "[0.28226943 0.99734545 0.2829785 ]\n",
      "[0.28226943 0.99734545 0.2829785 ]\n",
      "[0.28044906 0.99734545 0.2829785 ]\n",
      "[0.27743795 0.99734545 0.2829785 ]\n",
      "[0.27329984 0.99734545 0.2829785 ]\n",
      "[0.27329984 0.99734545 0.2829785 ]\n",
      "[0.27148011 0.99734545 0.2829785 ]\n",
      "[0.26847353 0.99734545 0.2829785 ]\n",
      "[0.26436413 0.99734545 0.2829785 ]\n",
      "[0.26436413 0.99734545 0.2829785 ]\n",
      "[0.26254504 0.99734545 0.2829785 ]\n",
      "[0.25954298 0.99734545 0.2829785 ]\n",
      "[0.25546219 0.99734545 0.2829785 ]\n",
      "[0.25546219 0.99734545 0.2829785 ]\n",
      "[0.25364373 0.99734545 0.2829785 ]\n",
      "[0.25064617 0.99734545 0.2829785 ]\n",
      "[0.24659387 0.99734545 0.2829785 ]\n",
      "[0.24659387 0.99734545 0.2829785 ]\n",
      "[0.24477605 0.99734545 0.2829785 ]\n",
      "[0.24178297 0.99734545 0.2829785 ]\n",
      "[0.23775907 0.99734545 0.2829785 ]\n",
      "[0.23775907 0.99734545 0.2829785 ]\n",
      "[0.23594187 0.99734545 0.2829785 ]\n",
      "[0.23295326 0.99734545 0.2829785 ]\n",
      "[0.22895764 0.99734545 0.2829785 ]\n",
      "[0.22895764 0.99734545 0.2829785 ]\n",
      "[0.22714106 0.99734545 0.2829785 ]\n",
      "[0.22415691 0.99734545 0.2829785 ]\n",
      "[0.22018946 0.99734545 0.2829785 ]\n",
      "[0.22018946 0.99734545 0.2829785 ]\n",
      "[0.21837351 0.99734545 0.2829785 ]\n",
      "[0.21539379 0.99734545 0.2829785 ]\n",
      "[0.21145441 0.99734545 0.2829785 ]\n",
      "[0.21145441 0.99734545 0.2829785 ]\n",
      "[0.20963908 0.99734545 0.2829785 ]\n",
      "[0.20666377 0.99734545 0.2829785 ]\n",
      "[0.20275236 0.99734545 0.2829785 ]\n",
      "[0.20275236 0.99734545 0.2829785 ]\n",
      "[0.20093764 0.99734545 0.2829785 ]\n",
      "[0.19796674 0.99734545 0.2829785 ]\n",
      "[0.19408318 0.99734545 0.2829785 ]\n",
      "[0.19408318 0.99734545 0.2829785 ]\n",
      "[0.19226909 0.99734545 0.2829785 ]\n",
      "[0.18930257 0.99734545 0.2829785 ]\n",
      "[0.18544676 0.99734545 0.2829785 ]\n",
      "[0.18544676 0.99734545 0.2829785 ]\n",
      "[0.18363328 0.99734545 0.2829785 ]\n",
      "[0.18067113 0.99734545 0.2829785 ]\n",
      "[0.17684297 0.99734545 0.2829785 ]\n",
      "[0.17684297 0.99734545 0.2829785 ]\n",
      "[0.1750301  0.99734545 0.2829785 ]\n",
      "[0.1720723  0.99734545 0.2829785 ]\n",
      "[0.16827168 0.99734545 0.2829785 ]\n",
      "[0.16827168 0.99734545 0.2829785 ]\n",
      "[0.16645942 0.99734545 0.2829785 ]\n",
      "[0.16350596 0.99734545 0.2829785 ]\n",
      "[0.15973278 0.99734545 0.2829785 ]\n",
      "[0.15973278 0.99734545 0.2829785 ]\n",
      "[0.15792112 0.99734545 0.2829785 ]\n",
      "[0.15497198 0.99734545 0.2829785 ]\n",
      "[0.15122613 0.99734545 0.2829785 ]\n",
      "[0.15122613 0.99734545 0.2829785 ]\n",
      "[0.14941508 0.99734545 0.2829785 ]\n",
      "[0.14647024 0.99734545 0.2829785 ]\n",
      "[0.14275162 0.99734545 0.2829785 ]\n",
      "[0.14275162 0.99734545 0.2829785 ]\n",
      "[0.14094118 0.99734545 0.2829785 ]\n",
      "[0.13800062 0.99734545 0.2829785 ]\n",
      "[0.13430914 0.99734545 0.2829785 ]\n",
      "[0.13430914 0.99734545 0.2829785 ]\n",
      "[0.13249929 0.99734545 0.2829785 ]\n",
      "[0.129563   0.99734545 0.2829785 ]\n",
      "[0.12589854 0.99734545 0.2829785 ]\n",
      "[0.12589854 0.99734545 0.2829785 ]\n",
      "[0.1240893  0.99734545 0.2829785 ]\n",
      "[0.12115726 0.99734545 0.2829785 ]\n",
      "[0.11751973 0.99734545 0.2829785 ]\n",
      "[0.11751973 0.99734545 0.2829785 ]\n",
      "[0.11571108 0.99734545 0.2829785 ]\n",
      "[0.11278328 0.99734545 0.2829785 ]\n",
      "[0.10917256 0.99734545 0.2829785 ]\n",
      "[0.10917256 0.99734545 0.2829785 ]\n",
      "[0.10736451 0.99734545 0.2829785 ]\n",
      "[0.10444093 0.99734545 0.2829785 ]\n",
      "[0.10085694 0.99734545 0.2829785 ]\n",
      "[0.10085694 0.99734545 0.2829785 ]\n",
      "[0.09904947 0.99734545 0.2829785 ]\n",
      "[0.0961301  0.99734545 0.2829785 ]\n",
      "[0.09257273 0.99734545 0.2829785 ]\n",
      "[0.09257273 0.99734545 0.2829785 ]\n",
      "[0.09076585 0.99734545 0.2829785 ]\n",
      "[0.08785067 0.99734545 0.2829785 ]\n",
      "[0.08431982 0.99734545 0.2829785 ]\n",
      "[0.08431982 0.99734545 0.2829785 ]\n",
      "[0.08251353 0.99734545 0.2829785 ]\n",
      "[0.07960252 0.99734545 0.2829785 ]\n",
      "[0.07609808 0.99734545 0.2829785 ]\n",
      "[0.07609808 0.99734545 0.2829785 ]\n",
      "[0.07429238 0.99734545 0.2829785 ]\n",
      "[0.07138553 0.99734545 0.2829785 ]\n",
      "[0.06790741 0.99734545 0.2829785 ]\n",
      "[0.06790741 0.99734545 0.2829785 ]\n",
      "[0.06610229 0.99734545 0.2829785 ]\n",
      "[0.06319958 0.99734545 0.2829785 ]\n",
      "[0.05974769 0.99734545 0.2829785 ]\n",
      "[0.05974769 0.99734545 0.2829785 ]\n",
      "[0.05794315 0.99734545 0.2829785 ]\n",
      "[0.05504456 0.99734545 0.2829785 ]\n",
      "[0.05161879 0.99734545 0.2829785 ]\n",
      "[0.05161879 0.99734545 0.2829785 ]\n",
      "[0.04981483 0.99734545 0.2829785 ]\n",
      "[0.04692036 0.99734545 0.2829785 ]\n",
      "[0.04352061 0.99734545 0.2829785 ]\n",
      "[0.04352061 0.99734545 0.2829785 ]\n",
      "[0.04171722 0.99734545 0.2829785 ]\n",
      "[0.03882684 0.99734545 0.2829785 ]\n",
      "[0.03545301 0.99734545 0.2829785 ]\n",
      "[0.03545301 0.99734545 0.2829785 ]\n",
      "[0.0336502  0.99734545 0.2829785 ]\n",
      "[0.0307639  0.99734545 0.2829785 ]\n",
      "[0.0274159  0.99734545 0.2829785 ]\n",
      "[0.0274159  0.99734545 0.2829785 ]\n",
      "[0.02561366 0.99734545 0.2829785 ]\n",
      "[0.02273143 0.99734545 0.2829785 ]\n",
      "[0.01940916 0.99734545 0.2829785 ]\n",
      "[0.01940916 0.99734545 0.2829785 ]\n",
      "[0.01760749 0.99734545 0.2829785 ]\n",
      "[0.0147293  0.99734545 0.2829785 ]\n",
      "[0.01143266 0.99734545 0.2829785 ]\n",
      "[0.01143266 0.99734545 0.2829785 ]\n",
      "[0.00963156 0.99734545 0.2829785 ]\n",
      "[0.0067574  0.99734545 0.2829785 ]\n",
      "[0.0034863  0.99734545 0.2829785 ]\n",
      "[0.0034863  0.99734545 0.2829785 ]\n",
      "[0.00168576 0.99734545 0.2829785 ]\n",
      "[-0.00118438  0.99734545  0.2829785 ]\n",
      "[-0.00443004  0.99734545  0.2829785 ]\n",
      "[-0.00443004  0.99734545  0.2829785 ]\n",
      "[-0.00623002  0.99734545  0.2829785 ]\n",
      "[-0.00909615  0.99734545  0.2829785 ]\n",
      "[-0.01231647  0.99734545  0.2829785 ]\n",
      "[-0.01231647  0.99734545  0.2829785 ]\n",
      "[-0.01411589  0.99734545  0.2829785 ]\n",
      "[-0.01697803  0.99734545  0.2829785 ]\n",
      "[-0.02017311  0.99734545  0.2829785 ]\n",
      "[-0.02017311  0.99734545  0.2829785 ]\n",
      "[-0.02197197  0.99734545  0.2829785 ]\n",
      "[-0.02483014  0.99734545  0.2829785 ]\n",
      "[-0.02800006  0.99734545  0.2829785 ]\n",
      "[-0.02800006  0.99734545  0.2829785 ]\n",
      "[-0.02979836  0.99734545  0.2829785 ]\n",
      "[-0.03265258  0.99734545  0.2829785 ]\n",
      "[-0.03579745  0.99734545  0.2829785 ]\n",
      "[-0.03579745  0.99734545  0.2829785 ]\n",
      "[-0.03759519  0.99734545  0.2829785 ]\n",
      "[-0.04044546  0.99734545  0.2829785 ]\n",
      "[-0.04356537  0.99734545  0.2829785 ]\n",
      "[-0.04356537  0.99734545  0.2829785 ]\n",
      "[-0.04536257  0.99734545  0.2829785 ]\n",
      "[-0.04820891  0.99734545  0.2829785 ]\n",
      "[-0.05130395  0.99734545  0.2829785 ]\n",
      "[-0.05130395  0.99734545  0.2829785 ]\n",
      "[-0.05310059  0.99734545  0.2829785 ]\n",
      "[-0.05594302  0.99734545  0.2829785 ]\n",
      "[-0.05901329  0.99734545  0.2829785 ]\n",
      "[-0.05901329  0.99734545  0.2829785 ]\n",
      "[-0.06080938  0.99734545  0.2829785 ]\n",
      "[-0.06364791  0.99734545  0.2829785 ]\n",
      "[-0.0666935   0.99734545  0.2829785 ]\n",
      "[-0.0666935   0.99734545  0.2829785 ]\n",
      "[-0.06848905  0.99734545  0.2829785 ]\n",
      "[-0.0713237   0.99734545  0.2829785 ]\n",
      "[-0.0743447   0.99734545  0.2829785 ]\n",
      "[-0.0743447   0.99734545  0.2829785 ]\n",
      "[-0.07613971  0.99734545  0.2829785 ]\n",
      "[-0.07897048  0.99734545  0.2829785 ]\n",
      "[-0.08196699  0.99734545  0.2829785 ]\n",
      "[-0.08196699  0.99734545  0.2829785 ]\n",
      "[-0.08376146  0.99734545  0.2829785 ]\n",
      "[-0.08658838  0.99734545  0.2829785 ]\n",
      "[-0.08956049  0.99734545  0.2829785 ]\n",
      "[-0.08956049  0.99734545  0.2829785 ]\n",
      "[-0.09135441  0.99734545  0.2829785 ]\n",
      "[-0.09417749  0.99734545  0.2829785 ]\n",
      "[-0.0971253   0.99734545  0.2829785 ]\n",
      "[-0.0971253   0.99734545  0.2829785 ]\n",
      "[-0.09891868  0.99734545  0.2829785 ]\n",
      "[-0.10173794  0.99734545  0.2829785 ]\n",
      "[-0.10466152  0.99734545  0.2829785 ]\n",
      "[-0.10466152  0.99734545  0.2829785 ]\n",
      "[-0.10645437  0.99734545  0.2829785 ]\n",
      "[-0.10926981  0.99734545  0.2829785 ]\n",
      "[-0.11216928  0.99734545  0.2829785 ]\n",
      "[-0.11216928  0.99734545  0.2829785 ]\n",
      "[-0.11396159  0.99734545  0.2829785 ]\n",
      "[-0.11677324  0.99734545  0.2829785 ]\n",
      "[-0.11964866  0.99734545  0.2829785 ]\n",
      "[-0.11964866  0.99734545  0.2829785 ]\n",
      "[-0.12144045  0.99734545  0.2829785 ]\n",
      "[-0.12424831  0.99734545  0.2829785 ]\n",
      "[-0.1270998   0.99734545  0.2829785 ]\n",
      "[-0.1270998   0.99734545  0.2829785 ]\n",
      "[-0.12889105  0.99734545  0.2829785 ]\n",
      "[-0.13169515  0.99734545  0.2829785 ]\n",
      "[-0.13452278  0.99734545  0.2829785 ]\n",
      "[-0.13452278  0.99734545  0.2829785 ]\n",
      "[-0.1363135   0.99734545  0.2829785 ]\n",
      "[-0.13911385  0.99734545  0.2829785 ]\n",
      "[-0.14191771  0.99734545  0.2829785 ]\n",
      "[-0.14191771  0.99734545  0.2829785 ]\n",
      "[-0.14370791  0.99734545  0.2829785 ]\n",
      "[-0.14650452  0.99734545  0.2829785 ]\n",
      "[-0.14928471  0.99734545  0.2829785 ]\n",
      "[-0.14928471  0.99734545  0.2829785 ]\n",
      "[-0.15107439  0.99734545  0.2829785 ]\n",
      "[-0.15386727  0.99734545  0.2829785 ]\n",
      "[-0.15662388  0.99734545  0.2829785 ]\n",
      "[-0.15662388  0.99734545  0.2829785 ]\n",
      "[-0.15841303  0.99734545  0.2829785 ]\n",
      "[-0.1612022   0.99734545  0.2829785 ]\n",
      "[-0.16393532  0.99734545  0.2829785 ]\n",
      "[-0.16393532  0.99734545  0.2829785 ]\n",
      "[-0.16572395  0.99734545  0.2829785 ]\n",
      "[-0.16850942  0.99734545  0.2829785 ]\n",
      "[-0.17121913  0.99734545  0.2829785 ]\n",
      "[-0.17121913  0.99734545  0.2829785 ]\n",
      "[-0.17300725  0.99734545  0.2829785 ]\n",
      "[-0.17578904  0.99734545  0.2829785 ]\n",
      "[-0.17847543  0.99734545  0.2829785 ]\n",
      "[-0.17847543  0.99734545  0.2829785 ]\n",
      "[-0.18026303  0.99734545  0.2829785 ]\n",
      "[-0.18304115  0.99734545  0.2829785 ]\n",
      "[-0.18570431  0.99734545  0.2829785 ]\n",
      "[-0.18570431  0.99734545  0.2829785 ]\n",
      "[-0.1874914   0.99734545  0.2829785 ]\n",
      "[-0.19026586  0.99734545  0.2829785 ]\n",
      "[-0.19290588  0.99734545  0.2829785 ]\n",
      "[-0.19290588  0.99734545  0.2829785 ]\n",
      "[-0.19469246  0.99734545  0.2829785 ]\n",
      "[-0.19746328  0.99734545  0.2829785 ]\n",
      "[-0.20008025  0.99734545  0.2829785 ]\n",
      "[-0.20008025  0.99734545  0.2829785 ]\n",
      "[-0.20186631  0.99734545  0.2829785 ]\n",
      "[-0.2046335   0.99734545  0.2829785 ]\n",
      "[-0.20722751  0.99734545  0.2829785 ]\n",
      "[-0.20722751  0.99734545  0.2829785 ]\n",
      "[-0.20901306  0.99734545  0.2829785 ]\n",
      "[-0.21177664  0.99734545  0.2829785 ]\n",
      "[-0.21434776  0.99734545  0.2829785 ]\n",
      "[-0.21434776  0.99734545  0.2829785 ]\n",
      "[-0.21613281  0.99734545  0.2829785 ]\n",
      "[-0.21889279  0.99734545  0.2829785 ]\n",
      "[-0.22144112  0.99734545  0.2829785 ]\n",
      "[-0.22144112  0.99734545  0.2829785 ]\n",
      "[-0.22322566  0.99734545  0.2829785 ]\n",
      "[-0.22598205  0.99734545  0.2829785 ]\n",
      "[-0.22850767  0.99734545  0.2829785 ]\n",
      "[-0.22850767  0.99734545  0.2829785 ]\n",
      "[-0.23029171  0.99734545  0.2829785 ]\n",
      "[-0.23304453  0.99734545  0.2829785 ]\n",
      "[-0.23554753  0.99734545  0.2829785 ]\n",
      "[-0.23554753  0.99734545  0.2829785 ]\n",
      "[-0.23733107  0.99734545  0.2829785 ]\n",
      "[-0.24008033  0.99734545  0.2829785 ]\n",
      "[-0.2425608   0.99734545  0.2829785 ]\n",
      "[-0.2425608   0.99734545  0.2829785 ]\n",
      "[-0.24434384  0.99734545  0.2829785 ]\n",
      "[-0.24708955  0.99734545  0.2829785 ]\n",
      "[-0.24954756  0.99734545  0.2829785 ]\n",
      "[-0.24954756  0.99734545  0.2829785 ]\n",
      "[-0.25133011  0.99734545  0.2829785 ]\n",
      "[-0.25407228  0.99734545  0.2829785 ]\n",
      "[-0.25650793  0.99734545  0.2829785 ]\n",
      "[-0.25650793  0.99734545  0.2829785 ]\n",
      "[-0.25828998  0.99734545  0.2829785 ]\n",
      "[-0.26102864  0.99734545  0.2829785 ]\n",
      "[-0.263442    0.99734545  0.2829785 ]\n",
      "[-0.263442    0.99734545  0.2829785 ]\n",
      "[-0.26522356  0.99734545  0.2829785 ]\n",
      "[-0.26795871  0.99734545  0.2829785 ]\n",
      "[-0.27034988  0.99734545  0.2829785 ]\n",
      "[-0.27034988  0.99734545  0.2829785 ]\n",
      "[-0.27213094  0.99734545  0.2829785 ]\n",
      "[-0.2748626   0.99734545  0.2829785 ]\n",
      "[-0.27723166  0.99734545  0.2829785 ]\n",
      "[-0.27723166  0.99734545  0.2829785 ]\n",
      "[-0.27901223  0.99734545  0.2829785 ]\n",
      "[-0.28174041  0.99734545  0.2829785 ]\n",
      "[-0.28408744  0.99734545  0.2829785 ]\n",
      "[-0.28408744  0.99734545  0.2829785 ]\n",
      "[-0.28586752  0.99734545  0.2829785 ]\n",
      "[-0.28859223  0.99734545  0.2829785 ]\n",
      "[-0.29091731  0.99734545  0.2829785 ]\n",
      "[-0.29091731  0.99734545  0.2829785 ]\n",
      "Final loss: 0.6332028608389628\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAkfklEQVR4nO3df5hcZX338feHhPBTCCZbKglkgwQx+GjUIZWilVJ/BB+fRH0qJl0otGqualGgagvSq9JYW6ttETW1DbYKGEgxRZ7UagMFWtQGzUb5YRKDISQkEWQBI9BYIeH7/HHfIyfD7GZmd87O7M7ndV1z7Zxz7jPznZPsfPd73+fcRxGBmZlZrQPaHYCZmXUmJwgzM6vLCcLMzOpygjAzs7qcIMzMrC4nCDMzq8sJwsY8SR+W9Pl2xzEUSf8h6V35eZ+km9odk9n+OEFYUyRtlfS6dsdRFBF/HhHvGu7+kl4v6TZJT0h6VNKdkv5I0sGtjLMqIpZHxBta8VqSQtIJQ2w/T9JeSU/mx/2SviDpxFa8fxk68f9Yt3KCsK4m6e3ASuBaYEZETAHeAUwHjh1kn4mjF2FLrImIw4EjgdcBPwPWSXpJe8OyTucEYS0h6SBJn5L0o/z4lKSD8rapkr4qaZekxyR9Q9IBedsfSdqZ/3rfJOk36rz2r0h6SNKEwrq3Sro7P79M0pcK276c2/9U0u2STh4kZgF/AyyJiCsj4jGAiNgUEe+LiB8WXn+lpC9Jehw4T9JcSWvyZ3pQ0mclTSq89usl/SDH8FlAhW3nSfpmYfkkSTfnY7NJ0lmFbV+UtFTSv+Zj9G1JL8zbbs/N7srVwTuG+jeKiL0RcV9EvBf4T+Cywvu8StJ/5c9zl6TTa+Ldkt//fkl9hW3vlrQxb9sg6RV5/TGS/lnSQN7n/YV9LpN0vaSr837rJVXytmuA44B/yZ/pD4f6TFayiPDDj4YfwFbgdXXWLwHuAH4J6AH+C/ho3vYXwN8BB+bHa0hfmC8CtgPH5Ha9wAsHed/7gNcXlr8MXJyfXwZ8qbDtd4HnAQcBnwLuHOQ1TwIC6N3PZ74MeBp4C+mPqkOAVwKvAibmuDcCF+b2U4EngN/Mn/ciYA/wrrz9POCb+flh+Rj8Tn6tlwOPALPz9i8CjwJz8/blwIpCbAGcMETsv3ivmvW/C/w4P5+W3+NN+fO9Pi/35PgeB16U274AODk/fzuwEzgl/3ueAMzIr7EO+BNgEnA8sAV4Y+F4/k9+vwn5/8cd+/s/5sfoP1xBWKv0kf4SfzgiBoA/Bc7J254mfbHMiIinI+Ibkb4J9pK+xGdLOjAitkbEfYO8/nXAIgBJzyN9uVxXr2FE/GNEPBERPyd9Gb1M0pF1mk7NPx+qrpC0Iv8VvVvSOYW2ayLixoh4JiJ+FhHrIuKOiNgTEVuBvwdem9u+CVgfESsj4mlSknqI+t4MbI2IL+TX+h7wz6Qv36qvRMR3ImIPKUHMGeS1mvEj4Pn5+dnA1yLia/nz3Qz0588B8AzwEkmHRMSDEbE+r38X8ImIWBvJ5ojYRkoYPRGxJCKeiogtwJXAwsL7fzO/317gGuBlLfhM1mJOENYqxwDbCsvb8jqATwKbgZtyV8XFABGxGbiQ9CX+cP5yPob6rgXelrut3gZ8N38Z7UPSBEkfl3Rf7g7amjdNrW1L+isZUvIix7QwIiYD3yX9dVu1veZ9TszdZg/l9/nzwnscU2yfk+E++xfMAH4lJ6VdknaRku0vF9oUk8tu4PBBXqsZ04DHCjG8vSaGVwMviIj/Jo3J/B7wYO7qOinvdyypsqv3mY6peb0PA0cP8ZkO1tgb2xn3nCCsVX5E+mKoOi6vI/81/4GIOB6YD/xBdawhIq6NiFfnfQP4y3ovHhEbSEnnTOC3SAmjnt8CFpAGY48kdf9AYQygYBOpi+RtDXy+2mmPPwf8AJgVEUeQvgCr7/EghQHuPNZRd8CblDj+MyImFx6HR8R7GohpJN4KfKMQwzU1MRwWER8HiIjVEfF6UiL9AakaqO73wjqvvR24v+b1nhcRb6rTth5PMd0hnCBsOA6UdHDhMZHU3fPHknokTSX1P38JQNKbJZ2Qvyh/SupaekbSiySdkauC/yGdXfPMEO97LXAB8GukMYh6ngf8nFQdHEr6y76uiHgG+ADwkTzYepSSWez71+5g7/M48GT+i7r4hf6vwMmS3paPzfvZtyIo+ipwoqRzJB2YH6dIevF+3r/qx6Q+/v3K1dVMSZ8BTid1A0L6d/o/kt6Y2xws6XRJ0yUdLWmBpMNIx/VJnv03+jzwQUmvzMftBEkzgO8ATyidgHBIfs2XSDql1Z/JyuUEYcPxNdKXefVxGfBnpH7ru4F7SF00f5bbzwL+nfTlsgb424i4jTT+8HHSoOxDpAHuS4Z43+tI/fy3RsQjg7S5mlRp7AQ2kAbOBxUR/wScReqH355juR5YxuBJCOCDpGrlCdJf1P9UeM1HSGMIHyclqlnAtwZ5/yeAN5D6539EOg5/STo2jbgMuCp35Zw1SJtTJT1JSmj/ARwBnBIR9+QYtpOqrg8DA6Tj8CHS98MBwB/k2B4jHf/35P2+DHyMlLifAG4Enp/HFd5MGiu5n3RMP0+q6BrxF6Q/NnZJ+mCD+1gJlLpHzczM9uUKwszM6nKCMDOzupwgzMysLicIMzOrq9QLUyTNA64gXXD0+ep51YXtxwFXAZNzm4sj4mt52yXAO0mnRL4/IlYP9V5Tp06N3t7eVn8EM7Nxbd26dY9ERE+9baUlCKWJ1ZaS5nXZAayVtCpf8FT1x8D1EfE5SbNJp0/25ucLgZNJV6X+u6QT8+lzdfX29tLf31/WxzEzG5ckPWdGgqoyu5jmApsjYktEPAWsIJ1rXRSkc7IhnSP9o/x8AWlCsp9HxP2kaRrmlhirmZnVKDNBTGPf+Wd25HVFlwFnS9pBqh7e18S+SFosqV9S/8DAQKviNjMz2j9IvQj4YkRMJ80ceY3yfQIaERHLIqISEZWenrpdaGZmNkxlDlLvZN8JyqbndUXvBOYBRMQapVs8Tm1wXzMzK1GZFcRaYFaeHGwSadB5VU2bB4DfAMiTkx1MmgtmFbBQ6S5lM0lz2XynxFjNzKxGaQki39zkfGA16W5b10fEeklLJM3PzT4AvFvSXaSJ2M7LNx5ZT5owbQPwb8DvD3UG00gsXw69vXDAAenn8uVlvIuZ2dgzbibrq1Qq0exprsuXw+LFsHv3s+sOPRSWLYO+vsH3MzMbLySti4hKvW3tHqRuq0sv3Tc5QFo+91xXEmZmXZ0gHnig/vq9e1Nl4SRhZt2sqxPEcccNvm337lRhmJl1q65OEB/7WBpzGMy2ba4izKx7lTpZX6erDkSfe27qVqpn8eJ925qZdYuuriAgffFfddXglYQHrc2sW3V1BVFVrQ7OPrv+9uqgdbGtmdl41/UVRFVfH8yYMfh2VxJm1m2cIAr2N2jt01/NrJu4i6mgkUHraiVRbG9mNh65gqixv0FrcCVhZt3BFUQdriTMzFxBDMqVhJl1O1cQQ3AlYWbdzBXEfriSMLNu5QqiAa4kzKwblVpBSJonaZOkzZIurrP9ckl35se9knYVtn1C0npJGyV9WpLKjHV/XEmYWbcprYKQNAFYCrwe2AGslbQqIjZU20TERYX27wNenp//KnAa8NK8+ZvAa4H/KCveRriSMLNuUmYFMRfYHBFbIuIpYAWwYIj2i0j3pQYI4GBgEnAQcCDw4xJjbZgrCTPrFmUmiGnA9sLyjrzuOSTNAGYCtwJExBrgNuDB/FgdERvr7LdYUr+k/oGBgRaHP7i+vnTf6gkTBm/jGw6Z2VjXKWcxLQRWRsReAEknAC8GppOSyhmSXlO7U0Qsi4hKRFR6enpGNeBGKgnfcMjMxrIyE8RO4NjC8vS8rp6FPNu9BPBW4I6IeDIingS+DpxaSpQj0Egl4a4mMxurykwQa4FZkmZKmkRKAqtqG0k6CTgKWFNY/QDwWkkTJR1IGqB+ThdTJ/ANh8xsvCotQUTEHuB8YDXpy/36iFgvaYmk+YWmC4EVERGFdSuB+4B7gLuAuyLiX8qKdaSqlcRgPGhtZmOR9v1eHrsqlUr09/e3NYbe3jTuMJgJE1K14dNfzaxTSFoXEZV62zplkHpc8A2HzGw88VQbLeQL6cxsPHEF0WK+kM7MxgtXECVwJWFm44EriJK4kjCzsc4VRIlcSZjZWOYKomSuJMxsrHIFMQpcSZjZWOQKYpS4kjCzscYVxChyJWFmY4kriFHWaCVx9tkwdaqrCTNrH1cQbdBIJQHw6KOpy6m4j5nZaHEF0SaNVBLg6cLNrH1cQbRRo5VEdfC6uI+ZWdlcQbRZM5XE2WenKcVdTZjZaCg1QUiaJ2mTpM2SLq6z/XJJd+bHvZJ2FbYdJ+kmSRslbZDUW2as7VS94dCUKftvu22bT4U1s9FRWoKQNAFYCpwJzAYWSZpdbBMRF0XEnIiYA3wGuKGw+WrgkxHxYmAu8HBZsXaCvj545BH40peGvsc1eFzCzEZHmRXEXGBzRGyJiKeAFcCCIdovAq4DyIlkYkTcDBART0bE7hJj7RiNdjn5ojozK1uZCWIasL2wvCOvew5JM4CZwK151YnALkk3SPqepE/miqR2v8WS+iX1DwwMtDj89ql2Oc2YMXQ7VxJmVqZOGaReCKyMiOq5PBOB1wAfBE4BjgfOq90pIpZFRCUiKj09PaMV66jo64OtW1OXk6fnMLN2KDNB7ASOLSxPz+vqWUjuXsp2AHfm7qk9wI3AK8oIstNVq4mhxiVcSZhZGcpMEGuBWZJmSppESgKrahtJOgk4ClhTs+9kSdWy4AxgQ4mxdjRP9Gdm7VBagsh/+Z8PrAY2AtdHxHpJSyTNLzRdCKyIiCjsu5fUvXSLpHsAAVeWFetY4ErCzEabCt/LY1qlUon+/v52h1G65ctTpbB7iHO6Dj00JRNfdW1m+yNpXURU6m3zVBtjjKcMN7PR0ilnMVkTPGW4mY0GVxBjlKcMN7OyuYIYwzxluJmVyRXEGOcpw82sLK4gxgFXEmZWBieIcaLRKcN9QZ2ZNcoJYhxpdMpwVxJm1ggniHHIU3OYWSt4kHqc8gV1ZjZSriDGMV9QZ2Yj4QpinPMFdWY2XK4guoBPgzWz4XAF0SV8QZ2ZNcsVRBdxJWFmzXCC6DK+oM7MGlVqgpA0T9ImSZslXVxn++WS7syPeyXtqtl+hKQdkj5bZpzdxhfUmVkjSksQkiYAS4EzgdnAIkmzi20i4qKImBMRc4DPADfUvMxHgdvLirHb+TRYMxtKmRXEXGBzRGyJiKeAFcCCIdovAq6rLkh6JXA0cFOJMXa9Ru51Dc+eBuskYdY9ykwQ04DtheUded1zSJoBzARuzcsHAH8NfHCoN5C0WFK/pP6BgYGWBN2NPHhtZvV0yiD1QmBlRFRPwHwv8LWI2DHUThGxLCIqEVHp6ekpPcjxrNFKwoPXZt2jzOsgdgLHFpan53X1LAR+v7B8KvAaSe8FDgcmSXoyIp4z0G2tU73uYfHiVC0MxnM4mXWHMiuItcAsSTMlTSIlgVW1jSSdBBwFrKmui4i+iDguInpJ3UxXOzmMjmZOg/Xgtdn4VlqCiIg9wPnAamAjcH1ErJe0RNL8QtOFwIqIiLJiseY0ehosePDabDzTePlerlQq0d/f3+4wxp3ly/ff5QQpkVx1lbuczMYaSesiolJvm+disiF5Diez7tUpZzFZB/NpsGbdyQnCGuLBa7Pu4wRhDfPgtVl3cYKwprnLyaw7OEHYsDRz5fU554AEvb1OFmZjiROEDVujlUT1TOpt29ztZDaWOEHYiDQ6eF3lbiezscMJwkasmcFr8IR/ZmOFE4S1TKNdTuBKwmwscIKwlqp2Oc2YkZalwdv6mgmzzuYEYS3X1wdbt6bB6Wuu8TUTZmOVE4SVytdMmI1dThBWumaumXCXk1nncIKwUdHMALa7nMw6Q6kJQtI8SZskbZb0nDvCSbpc0p35ca+kXXn9HElrJK2XdLekd5QZp42OZq6ZcJeTWfuVliAkTQCWAmcCs4FFkmYX20TERRExJyLmAJ8BbsibdgO/HREnA/OAT0maXFasNnqauWbCXU5m7VVmBTEX2BwRWyLiKWAFsGCI9ouA6wAi4t6I+GF+/iPgYaCnxFhtlLnLyazzNZQgJB0m6YD8/ERJ8yUduJ/dpgHbC8s78rp6rz8DmAncWmfbXGAScF+dbYsl9UvqHxgYaOSjWAdxl5NZZ2u0grgdOFjSNOAm4Bzgiy2MYyGwMiL2uamlpBcA1wC/ExHP1O4UEcsiohIRlZ4eFxhjkbuczDpXowlCEbEbeBvwtxHxduDk/eyzEzi2sDw9r6tnIbl76RdvKB0B/CtwaUTc0WCcNka5y8ms8zScICSdCvSRvrQB9jct21pglqSZkiaRksCqOi98EnAUsKawbhLwFeDqiFjZYIw2xrnLyayzNJogLgQuAb4SEeslHQ/cNtQOEbEHOB9YDWwErs/7LpE0v9B0IbAionrXAADOAn4NOK9wGuycBmO1McxdTmadQ/t+LzewQxqsPjwiHi8npOGpVCrR39/f7jCshZYvT11Ju3fvv+2hh6bqo6+v/LjMxhNJ6yKiUm9bo2cxXSvpCEmHAd8HNkj6UCuDNKvlLiez9mq0i2l2rhjeAnyddErqOWUFZVblLiez9mk0QRyYr3t4C7AqIp4GmuubMhsBn+VkNvoaTRB/D2wFDgNuzxe2ddQYhI1/7nIyG10NJYiI+HRETIuIN0WyDfj1kmMzew53OZmNnkYHqY+U9DfVaS0k/TWpmjBrC3c5mZWv0S6mfwSeIF2fcBape+kLZQVl1gh3OZmVq9EE8cKI+EiemXVLRPwpcHyZgZk1wl1OZuVpNEH8TNKrqwuSTgN+Vk5IZs1rtsvJicJs/yY22O73gKslHZmXfwKcW05IZsNTvYr6ggtSEtif6thEcV8ze1ajZzHdFREvA14KvDQiXg6cUWpkZsPQTJcTeGzCbChN3VEuIh4vzMH0ByXEY9YSzXQ5eWzCrL6R3HJULYvCrATNnOUEHpswqzWSBOGpNqzjFbucmkkUvm7CbD8JQtITkh6v83gCOGaUYjQbMY9NmDVvyAQREc+LiCPqPJ4XEY2eAWXWMTw2Yda4kXQx7ZekeZI2Sdos6eI62y8v3DHuXkm7CtvOlfTD/PAptdYywxmbcJeTdaPSEoSkCcBS4ExgNrBI0uxim4i4KCLmRMQc4DPADXnf5wMfAX4FmAt8RNJRZcVq3afZsYndu1M10dvrRGHdo8wKYi6wOU/N8RSwAlgwRPtFwHX5+RuBmyPisYj4CXAzMK/EWK1LNTs2sW2bu52se5SZIKYB2wvLO/K658j3l5gJ3NrMvpIWV2eYHRgYaEnQ1p2aGZsAnxJr3aHUMYgmLARWRsTeZnaKiGURUYmISk9PT0mhWbdodmwCnChsfCszQewEji0sT8/r6lnIs91Lze5r1jLFLqcZMxrfzwPZNh6VmSDWArMkzZQ0iZQEVtU2knQScBSwprB6NfAGSUflwek35HVmo6KvD7ZuTYmi0W4nXzth401pCSIi9gDnk77YNwLXR8R6SUskzS80XQisiIgo7PsY8FFSklkLLMnrzEZVs91OvnbCxhMVvpfHtEqlEv39/e0Ow8ax5csbn0q8asoUuOIKTydunUvSuoio1NvWKYPUZh3P8zpZt3GCMGuS53WybuEEYTZMntfJxjsnCLMR8D0nbDxzgjAboeGOTThRWKdzgjBrkWbHJsCD2NbZnCDMWqzZeZ08iG2dygnCrAS+wM7GAycIs5J4bMLGOicIs5I5UdhY5QRhNkqGO4jtRGHt4gRhNsqaHcQGn+1k7eEEYdYGw7k5kc92stHmBGHWJsMZm/DZTjaanCDM2syD2NapSk0QkuZJ2iRps6SLB2lzlqQNktZLuraw/hN53UZJn5akMmM1azcnCus0pSUISROApcCZwGxgkaTZNW1mAZcAp0XEycCFef2vAqcBLwVeApwCvLasWM06ic92sk5RZgUxF9gcEVsi4ilgBbCgps27gaUR8ROAiHg4rw/gYGAScBBwIPDjEmM16zjDPdvJicJapcwEMQ3YXljekdcVnQicKOlbku6QNA8gItYAtwEP5sfqiNhYYqxmHWk4ZzuBT4u11mj3IPVEYBZwOrAIuFLSZEknAC8GppOSyhmSXlO7s6TFkvol9Q8MDIxi2GajZzhjE+DTYm3kykwQO4FjC8vT87qiHcCqiHg6Iu4H7iUljLcCd0TEkxHxJPB14NTaN4iIZRFRiYhKT09PKR/CrFMM97TYc84BCXp7nSysOWUmiLXALEkzJU0CFgKratrcSKoekDSV1OW0BXgAeK2kiZIOJA1Qu4vJjOYTRUT6uW2bxyesOaUliIjYA5wPrCZ9uV8fEeslLZE0PzdbDTwqaQNpzOFDEfEosBK4D7gHuAu4KyL+paxYzcai4XY9eSDbGqWo/nkxxlUqlejv7293GGZts3x5GnPYu7e5/aZMgSuuSAnHuo+kdRFRqbet3YPUZtYiwzktFlxR2OCcIMzGkeppsTNmpOVm5h9worBaThBm40xfH2zdmganr7lmeNdQOFEYOEGYjWvDHcgGJwpzgjDrCk4UNhxOEGZdxInCmuEEYdaFnCisEU4QZl3MicKG4gRhZk4UVpcThJn9wkgThacYH1+cIMzsOTzFuIEThJkNYbhTjLvLaXxwgjCz/RpOovDYxNjnBGFmDXOi6C5OEGbWtGKimDChsX2qiWLCBN/hbqxwgjCzYRvOFOPPPJN++g53na/UBCFpnqRNkjZLuniQNmdJ2iBpvaRrC+uPk3STpI15e2+ZsZrZ8FSnGG/2tNgqd0F1rtIShKQJwFLgTGA2sEjS7Jo2s4BLgNMi4mTgwsLmq4FPRsSLgbnAw2XFamYjM5LrJ6qcKDpPmRXEXGBzRGyJiKeAFcCCmjbvBpZGxE8AIuJhgJxIJkbEzXn9kxGxu8RYzawFnCjGlzITxDRge2F5R15XdCJwoqRvSbpD0rzC+l2SbpD0PUmfzBXJPiQtltQvqX9gYKCUD2FmzXOiGB/aPUg9EZgFnA4sAq6UNDmvfw3wQeAU4HjgvNqdI2JZRFQiotLT0zNKIZtZo4qJYji3QYWUKM45x2c+tUOZCWIncGxheXpeV7QDWBURT0fE/cC9pISxA7gzd0/tAW4EXlFirGZWouJtUJ95pvnKIiL99JlPo6vMBLEWmCVppqRJwEJgVU2bG0nVA5KmkrqWtuR9J0uqlgVnABtKjNXMRtFIu6Dc/TQ6SksQ+S//84HVwEbg+ohYL2mJpPm52WrgUUkbgNuAD0XEoxGxl9S9dIukewABV5YVq5m1hxNFZ1NUa7cxrlKpRH9/f7vDMLMRWL4cLrggffEPx5QpcMUVKfFYYySti4hKvW3tHqQ2M/uFkQ5qu6JoLScIM+s4xUHta67xXe7axQnCzDpaK26H6gkCh8cJwszGhJEkCk8QODxOEGY2pvgq7dHjBGFmY5ITRfmcIMxsTHOiKI8ThJmNC04UrecEYWbjSqsmCPTZT04QZjZOjXSCQPDZT04QZtYV3AXVPCcIM+sqThSNc4Iws67kRLF/ThBm1tVamSjG26C2E4SZGa05+2m8DWqXmiAkzZO0SdJmSRcP0uYsSRskrZd0bc22IyTtkPTZMuM0M6tqxdlPVWP9ftqlJQhJE4ClwJnAbGCRpNk1bWYBlwCnRcTJwIU1L/NR4PayYjQz25+RdkHV3k97LHVDlVlBzAU2R8SWiHgKWAEsqGnzbmBpRPwEICIerm6Q9ErgaOCmEmM0M2tIK8YqYGx1Q5WZIKYB2wvLO/K6ohOBEyV9S9IdkuYBSDoA+GvSfanNzDpGqxJFVSefCdXuQeqJwCzgdGARcKWkycB7ga9FxI6hdpa0WFK/pP6BgYGyYzUz+4VWDGoXdWKiKDNB7ASOLSxPz+uKdgCrIuLpiLgfuJeUME4Fzpe0Ffgr4Lclfbz2DSJiWURUIqLS09NTxmcwMxtSvUHtkSSMTjpltswEsRaYJWmmpEnAQmBVTZsbSdUDkqaSupy2RERfRBwXEb2kbqarI6LuWVBmZp1kpPfTruqEsYrSEkRE7AHOB1YDG4HrI2K9pCWS5udmq4FHJW0AbgM+FBGPlhWTmdloamU3VDsqC0X1HKwxrlKpRH9/f7vDMDPbr+XL4YIL0pf+SE2ZAldckZLRcEhaFxGVetvaPUhtZtZ1WnkmVJmD204QZmZt0upEsXhxa5OEE4SZWZu1aqxi92649NLWxeUEYWbWIVoxD9QDD7QuHicIM7MONZzK4rjjWvf+ThBmZh2u0cri0EPhYx9r3fs6QZiZjTG1lYWUfi5bNvzTXeuZ2LqXMjOz0dTX19qEUMsVhJmZ1eUEYWZmdTlBmJlZXU4QZmZWlxOEmZnVNW5mc5U0AGwbwUtMBR5pUThl6fQYOz0+cIyt4hhboxNinBERde+4Nm4SxEhJ6h9syttO0ekxdnp84BhbxTG2RqfH6C4mMzOrywnCzMzqcoJ41rJ2B9CATo+x0+MDx9gqjrE1OjpGj0GYmVldriDMzKwuJwgzM6ur6xOEpHmSNknaLOnidscDIOlYSbdJ2iBpvaQL8vrnS7pZ0g/zz6M6INYJkr4n6at5eaakb+fj+U+SJrU5vsmSVkr6gaSNkk7tpOMo6aL8b/x9SddJOrgTjqGkf5T0sKTvF9bVPW5KPp3jvVvSK9oU3yfzv/Pdkr4iaXJh2yU5vk2S3lh2fIPFWNj2AUkhaWpeHvVj2IiuThCSJgBLgTOB2cAiSbPbGxUAe4APRMRs4FXA7+e4LgZuiYhZwC15ud0uADYWlv8SuDwiTgB+AryzLVE96wrg3yLiJOBlpFg74jhKmga8H6hExEuACcBCOuMYfhGYV7NusON2JjArPxYDn2tTfDcDL4mIlwL3ApcA5N+dhcDJeZ+/zb/77YgRSccCbwCKNwdtxzHcr65OEMBcYHNEbImIp4AVwII2x0REPBgR383PnyB9qU0jxXZVbnYV8Ja2BJhJmg78b+DzeVnAGcDK3KStMUo6Evg14B8AIuKpiNhFZx3HicAhkiYChwIP0gHHMCJuBx6rWT3YcVsAXB3JHcBkSS8Y7fgi4qaI2JMX7wCmF+JbERE/j4j7gc2k3/1SDXIMAS4H/hAoniE06sewEd2eIKYB2wvLO/K6jiGpF3g58G3g6Ih4MG96CDi6XXFlnyL9R38mL08BdhV+Sdt9PGcCA8AXcjfY5yUdRoccx4jYCfwV6S/JB4GfAuvorGNYNNhx68Tfo98Fvp6fd0x8khYAOyPirppNHRNjUbcniI4m6XDgn4ELI+Lx4rZI5ye37RxlSW8GHo6Ide2KoQETgVcAn4uIlwP/TU13UjuPY+7DX0BKZMcAh1GnS6ITtfv/31AkXUrqpl3e7liKJB0KfBj4k3bH0qhuTxA7gWMLy9PzuraTdCApOSyPiBvy6h9Xy8788+F2xQecBsyXtJXUNXcGqb9/cu4ugfYfzx3Ajoj4dl5eSUoYnXIcXwfcHxEDEfE0cAPpuHbSMSwa7Lh1zO+RpPOANwN98exFXp0S3wtJfwzclX9vpgPflfTLdE6M++j2BLEWmJXPGplEGsha1eaYqn35/wBsjIi/KWxaBZybn58L/L/Rjq0qIi6JiOkR0Us6brdGRB9wG/CbuVm7Y3wI2C7pRXnVbwAb6Jzj+ADwKkmH5n/zanwdcwxrDHbcVgG/nc/EeRXw00JX1KiRNI/U5Tk/InYXNq0CFko6SNJM0kDwd0Y7voi4JyJ+KSJ68+/NDuAV+f9pRxzD54iIrn4AbyKd8XAfcGm748kxvZpUvt8N3JkfbyL18d8C/BD4d+D57Y41x3s68NX8/HjSL99m4MvAQW2ObQ7Qn4/ljcBRnXQcgT8FfgB8H7gGOKgTjiFwHWlc5GnSF9k7BztugEhnA94H3EM6K6sd8W0m9eNXf2f+rtD+0hzfJuDMdh3Dmu1bgantOoaNPDzVhpmZ1dXtXUxmZjYIJwgzM6vLCcLMzOpygjAzs7qcIMzMrC4nCLMmSNor6c7Co2UT/UnqrTfzp1m7TNx/EzMr+FlEzGl3EGajwRWEWQtI2irpE5LukfQdSSfk9b2Sbs1z/N8i6bi8/uh8z4K78uNX80tNkHSl0j0ibpJ0SNs+lHU9Jwiz5hxS08X0jsK2n0bE/wI+S5rpFuAzwFWR7lGwHPh0Xv9p4D8j4mWk+aHW5/WzgKURcTKwC/i/pX4asyH4SmqzJkh6MiIOr7N+K3BGRGzJEy0+FBFTJD0CvCAins7rH4yIqZIGgOkR8fPCa/QCN0e6IQ+S/gg4MCL+bBQ+mtlzuIIwa50Y5Hkzfl54vhePE1obOUGYtc47Cj/X5Of/RZrtFqAP+EZ+fgvwHvjFfb2PHK0gzRrlv07MmnOIpDsLy/8WEdVTXY+SdDepCliU172PdEe7D5Hubvc7ef0FwDJJ7yRVCu8hzfxp1jE8BmHWAnkMohIRj7Q7FrNWcReTmZnV5QrCzMzqcgVhZmZ1OUGYmVldThBmZlaXE4SZmdXlBGFmZnX9f6SttQbAU0TEAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(X_scaled, y_scaled, 0.02, 150);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still not satisfied with the performance of your model? Well that's because we were only updating one of the weights! What happens when we update **all** of our weights and biases using this process? The code below will do that for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update ALL weights and biases\n",
    "def update_weights(X, y_true, weights, biases, learning_rate):\n",
    "    \"\"\" Updates the weight values. Returns the resulting loss. \"\"\"\n",
    "    \n",
    "    # Initialize prediction array with 0's\n",
    "    y_preds = np.zeros_like(y_true)\n",
    "    \n",
    "    for i, (row,y_actual) in enumerate(zip(X, y_true)):\n",
    "        # Forward pass\n",
    "        sum1,out1, sum2,out2 = forward(row, weights, biases)\n",
    "        y_preds[i] = out2\n",
    "\n",
    "        # Backpropagation\n",
    "        dl_dypred = 2 * (out2 - y_actual)\n",
    "        dypred_dh1 = weights[2] * relu_deriv(sum2)\n",
    "\n",
    "        dh1_dw1 = row[0] * relu_deriv(sum1)\n",
    "        dh1_dw2 = row[1] * relu_deriv(sum1)\n",
    "        dh1_db1 = relu_deriv(sum1)      \n",
    "        \n",
    "        dypred_dw3 = out1 * relu_deriv(sum2)\n",
    "        dypred_db2 = relu_deriv(sum2)\n",
    "        \n",
    "        # Gradient descent - Update weights and biases\n",
    "        weights[0] -= learning_rate * dl_dypred * dypred_dh1 * dh1_dw1\n",
    "        weights[1] -= learning_rate * dl_dypred * dypred_dh1 * dh1_dw2\n",
    "        weights[2] -= learning_rate * dl_dypred * dypred_dw3\n",
    "\n",
    "        biases[0] -= learning_rate * dl_dypred * dypred_dh1 * dh1_db1\n",
    "        biases[1] -= learning_rate * dl_dypred * dypred_db2\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss = mse(y_true, y_preds)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss: 0.01642072126621692\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(X_scaled, y_scaled, 0.004, 400);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! You've created a *very* simple neural network using only numpy! Hopefully by now you have a better understanding of the math behind a neural network.\n",
    "\n",
    "*Credit: inspiration for this lab comes from [this blog post](https://victorzhou.com/blog/intro-to-neural-networks/).*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
