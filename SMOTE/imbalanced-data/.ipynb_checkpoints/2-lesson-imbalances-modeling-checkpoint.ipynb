{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "#  Class Imbalances 2: Modeling\n",
    "\n",
    "_Author_: Dan Wilhelm (LA)\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we'll cover techniques for:\n",
    "\n",
    "- Organizing larger projects.\n",
    "- Handling class imbalance.\n",
    "- Optimizing models for precision/recall rather than accuracy.\n",
    "\n",
    "Because this is a larger project than usual, there is no solution notebook. All code is contained inside the lesson notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson Outline (Continued)\n",
    "\n",
    "1. [Loading Results from Another Notebook](#load)\n",
    "2. [Baseline Models](#models)\n",
    "    - [Dummy Classifier](#dummy)\n",
    "    - [Logistic Regression](#logistic)\n",
    "    - [Random Forests](#rfs)\n",
    "    - [Nearest Neighbors](#nn)\n",
    "3. [Class Imbalance Techniques](#imbalance)\n",
    "    - [Undersample More Frequent Class](#undersample)\n",
    "    - [Oversample Less Frequent Class](#oversample)\n",
    "    - [SMOTE (Synthetic Minority Over-sampling Technique)](#smote)\n",
    "    - [Weight each class in the models](#weight)\n",
    "    - [Adjust the class cutoff threshold (ROC)](#roc)\n",
    "4. [Next Steps](#next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# CONSTANTS\n",
    "\n",
    "# We might normally store this in a shared external Python file\n",
    "INPUT_PICKLE_FILE = 'adult_data-vec.pickle'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "<a id=\"load\"></a>\n",
    "\n",
    "# Loading Results from Another Notebook\n",
    "\n",
    "Let's use `pickle` again to load our last results from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# We might normally store this in a shared external Python file\n",
    "def load_adult_pickle(filename, verbose=False):\n",
    "    \"\"\" Load train/test data from pickled file. \"\"\"\n",
    "    if verbose:\n",
    "        print(f'Loading from \"{filename}\" ...')\n",
    "\n",
    "    # 'rb' - Read + Binary File\n",
    "    with open(filename, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'\\tImported vectorized training data from \"{data[\"train_file\"]}\".')\n",
    "        print(f'\\tImported vectorized test data from \"{data[\"test_file\"]}\".\\n')\n",
    "\n",
    "        print(f'Data keys: {list(data.keys())}\\n')\n",
    "    \n",
    "    return data['X_train_df'], data['y_train'], data['X_test_df'], data['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from \"adult_data-vec.pickle\" ...\n",
      "\tImported vectorized training data from \"./data/adult.data\".\n",
      "\tImported vectorized test data from \"./data/adult.test\".\n",
      "\n",
      "Data keys: ['train_file', 'test_file', 'encoders', 'X_train_df', 'y_train', 'X_test_df', 'y_test']\n",
      "\n",
      "Train: (30162, 104), (30162,)\n",
      "Test: (15060, 104), (15060,)\n"
     ]
    }
   ],
   "source": [
    "X_train_df, y_train, X_test_df, y_test = \\\n",
    "    load_adult_pickle(INPUT_PICKLE_FILE, verbose=True)\n",
    "\n",
    "print(f'Train: {X_train_df.shape}, {y_train.shape}')\n",
    "print(f'Test: {X_test_df.shape}, {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"models\"></a>\n",
    "# Baseline Models\n",
    "Never just give a single accuracy or error value! Always compare your model's with other models, including a baseline.\n",
    "\n",
    "The baseline model is typically the simplest model -- i.e. always predict the most common class.\n",
    "\n",
    "\n",
    "<a id=\"dummy\"></a>\n",
    "# Baseline 1: Null Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "y_train Class Distribution: \n",
      " <=50K    0.751078\n",
      ">50K     0.248922\n",
      "Name: income, dtype: float64\n",
      "\n",
      "y_test Class Distribution: \n",
      " <=50K    0.754316\n",
      ">50K     0.245684\n",
      "Name: income, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('\\ny_train Class Distribution: \\n', y_train.value_counts(normalize=True))\n",
    "\n",
    "print('\\ny_test Class Distribution: \\n', y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This indicates our test set has about the same distribution of target classes.\n",
    "\n",
    "So, the null model should perform approximately the same on both since it just predicts `<=50K` each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  0.7510775147536636\n",
      "Test Accuracy:  0.7543160690571049\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dc = DummyClassifier(strategy='most_frequent')\n",
    "dc.fit(X_train_df, y_train)\n",
    "\n",
    "y_pred_train = dc.predict(X_train_df)\n",
    "y_pred_test = dc.predict(X_test_df)\n",
    "\n",
    "print('Train Accuracy: ', accuracy_score(y_train, y_pred_train))\n",
    "print('Test Accuracy: ', accuracy_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"logistic\"></a>\n",
    "# Model: Logistic Regression\n",
    "\n",
    "This model typically doesn't have as high accuracy as others, but it is great for its interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  0.7902990517870168\n",
      "Test Accuracy:  0.7926958831341302\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(solver='lbfgs')\n",
    "lr.fit(X_train_df, y_train)\n",
    "\n",
    "y_pred_train = lr.predict(X_train_df)\n",
    "y_pred_test = lr.predict(X_test_df)\n",
    "\n",
    "print('Train Accuracy: ', accuracy_score(y_train, y_pred_train))\n",
    "print('Test Accuracy: ', accuracy_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Q: How does this compare to the dummy classifier?</summary>\n",
    "The accuracy is only slightly higher than baseline. :(\n",
    "</details>\n",
    "\n",
    "Did we fare better with our precision and recall?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order of classes:  ['<=50K' '>50K']\n",
      "\n",
      "Test Confusion Matrix: \n",
      "               predicted <=50K  predicted >50K\n",
      "actual <=50K            10946             414\n",
      "actual >50K              2708             992\n"
     ]
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "\n",
    "print('Order of classes: ', lr.classes_)\n",
    "\n",
    "confusion_df = pd.DataFrame(\n",
    "                    data=confusion_matrix(y_test, y_pred_test),\n",
    "                    index=[f'actual {target_class}' for target_class in lr.classes_],\n",
    "                    columns=[f'predicted {target_class}' for target_class in lr.classes_])\n",
    "\n",
    "print('\\nTest Confusion Matrix: \\n', confusion_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we see that our initial model makes few false positives and many false negatives.\n",
    "\n",
    "From this table, we can also derive the classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.80      0.96      0.88     11360\n",
      "        >50K       0.71      0.27      0.39      3700\n",
      "\n",
      "    accuracy                           0.79     15060\n",
      "   macro avg       0.75      0.62      0.63     15060\n",
      "weighted avg       0.78      0.79      0.76     15060\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's interpret the precision and recall:\n",
    "+ **Recall of `>50K` - 27%**: We only identified 27% of the actual >50K customers! We earlier said this outcome was not preferred.\n",
    "+ **Precision of `>50K` - 71%**: When we predicted >50K, we got it right 71% of the time. So, 29% of the marketing materials were sent to <=50K. We earlier said this was acceptable, since perhaps they were close to >50K.\n",
    "\n",
    "Other than that, here are the other terms for the precision column:\n",
    "+ **Macro avg**: The average of the two precisions.\n",
    "+ **Weighted avg**: The average of the two precisions, with each weighted by its proportion of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"rfs\"></a>\n",
    "# Baseline 2: Random Forests\n",
    "\n",
    "Another good starting baseline model for comparison. It typically overfits but performs well with few tweaks.\n",
    "\n",
    "First, let's take the lengthy analysis above and condense it into a function so we can apply the same to new models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_baseline(mdl, \n",
    "                 X_train_df, y_train, X_test_df, y_test,\n",
    "                 verbose=True):\n",
    "    \"\"\"Return a dict of model performance indicators.\"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # 1. Fit the model on the training set\n",
    "    mdl.fit(X_train_df, y_train)\n",
    "\n",
    "    # 2. Predict on the training and test sets\n",
    "    y_pred_train = mdl.predict(X_train_df)\n",
    "    y_pred_test = mdl.predict(X_test_df)\n",
    "    \n",
    "    # 3. Train & test accuracy\n",
    "    results['train_accuracy'] = accuracy_score(y_train, y_pred_train)\n",
    "    results['test_accuracy'] = accuracy_score(y_test, y_pred_test)\n",
    "    \n",
    "    if verbose:\n",
    "        print('Train Accuracy: ', results['train_accuracy'])\n",
    "        print('Test Accuracy: ', results['test_accuracy'])\n",
    "        print('\\nOrder of classes: ', lr.classes_)\n",
    "    \n",
    "    \n",
    "    # 4. Test confusion matrix\n",
    "    confusion_df = pd.DataFrame(\\\n",
    "                        data=confusion_matrix(y_test, y_pred_test),\n",
    "                        index=[f'actual {target_class}' for target_class in lr.classes_],\n",
    "                        columns=[f'predicted {target_class}' for target_class in lr.classes_])\n",
    "\n",
    "    if verbose:\n",
    "        print('\\nTest Confusion Matrix: \\n', confusion_df)\n",
    "        print(classification_report(y_test, y_pred_test))\n",
    "    \n",
    "    # 5. Test recall and precision (if no samples, set recall/precision to 0)\n",
    "    results['test_recall'] = recall_score(y_test, y_pred_test, \n",
    "                                          pos_label='>50K', zero_division=0)\n",
    "    results['test_precision'] = precision_score(y_test, y_pred_test, \n",
    "                                                pos_label='>50K', zero_division=0)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  0.9999336913997745\n",
      "Test Accuracy:  0.8447543160690572\n",
      "\n",
      "Order of classes:  ['<=50K' '>50K']\n",
      "\n",
      "Test Confusion Matrix: \n",
      "               predicted <=50K  predicted >50K\n",
      "actual <=50K            10442             918\n",
      "actual >50K              1420            2280\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.88      0.92      0.90     11360\n",
      "        >50K       0.71      0.62      0.66      3700\n",
      "\n",
      "    accuracy                           0.84     15060\n",
      "   macro avg       0.80      0.77      0.78     15060\n",
      "weighted avg       0.84      0.84      0.84     15060\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_accuracy': 0.9999336913997745,\n",
       " 'test_accuracy': 0.8447543160690572,\n",
       " 'test_recall': 0.6162162162162163,\n",
       " 'test_precision': 0.7129455909943715}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# Consistent accuracy scores with `adult.names`!\n",
    "#   -- C4.5       : 84.46+-0.30   -- ours (below) is ~84.55%\n",
    "run_baseline(rfc, X_train_df, y_train, X_test_df, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Q: Did the model overfit? Can you explain its precision and recall values?</summary>\n",
    "Clearly, overfitting occurred since the training accuracy is nearly 1, yet the test accuracy is only 85%. The recall is 0.62 - double that of logistic regression! So, we now identify 62% of all >50K people correctly. Luckily, the precision did not decrease much at all.\n",
    "</details>\n",
    "\n",
    "\n",
    "With random forests, to reduce overfitting we can:\n",
    "\n",
    "+ Produce shallower trees.\n",
    "+ Prune the trees more.\n",
    "+ Use regularization.\n",
    "+ Use a tree method that optimizes for things other than accuracy.\n",
    "\n",
    "We can also tweak its recall vs precision by using some of the imbalance techniques below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"nn\"></a>\n",
    "# Baseline 3: Nearest Neighbor\n",
    "\n",
    "Nearest neighbor may not perform as well as other models, but it is often used as a baseline for comparing models since it works in a straightforward way. Note that NN has poor interpretability, unless the objective is directly comparing distances between the datapoints!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "run_baseline(knn, X_train_df, y_train, X_test_df, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Q: How did this perform? Better or worse than random forests?</summary>\n",
    "Significantly worse on all metrics!</details>\n",
    "\n",
    "Of course, we could tweak this model by adjusting `k` and observing the \"elbow curve\". However, it's unlikely to outperform random forests. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Let's summarize so far:\n",
    "\n",
    "| Syntax         | Accuracy    | Recall >50k | Precision >50k   | Notes              |\n",
    "| -----------    | ----------- | ----------- | -----------      |  -----------       |\n",
    "| Requirements:  | High        | Preferred   | Not as important |                    |\n",
    "| Null           | 75%         | 0%*         | 0%*              | * never guesses >50 |\n",
    "| KNN            | 77%         | 33%         | 55%              |                    |\n",
    "| Logistic Regr  | 79%         | 27%         | 71%              |                    |\n",
    "| Random Forests | 85%         | 62%         | 72%              |                    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "We will be tweaking things that affect all models.\n",
    "\n",
    "So, let's write a function that generates this chart automatedly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_models(models, X_train_df, y_train, X_test_df, y_test,\n",
    "                verbose=False):\n",
    "    \"\"\"Returns DataFrame of baseline results \n",
    "       given a dict `models` of names/sklearn models.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Fit each model and store how it performs on the test set\n",
    "    for name,model in models.items():\n",
    "        if verbose:\n",
    "            print('\\nRunning {} - {}'.format(name, model))\n",
    "        \n",
    "        results[name] = run_baseline(model, \n",
    "                                     X_train_df, y_train, \n",
    "                                     X_test_df, y_test, \n",
    "                                     verbose=False)\n",
    "        if verbose:\n",
    "            print('Results: ', results[name])\n",
    "\n",
    "    return pd.DataFrame.from_dict(results, orient='index').sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Most Frequent - DummyClassifier(strategy='most_frequent')\n",
      "Results:  {'train_accuracy': 0.7510775147536636, 'test_accuracy': 0.7543160690571049, 'test_recall': 0.0, 'test_precision': 0.0}\n",
      "\n",
      "Running Nearest Neighbors - KNeighborsClassifier()\n",
      "Results:  {'train_accuracy': 0.8309462237252172, 'test_accuracy': 0.7691235059760956, 'test_recall': 0.3254054054054054, 'test_precision': 0.5510297482837528}\n",
      "\n",
      "Running Random Forest - RandomForestClassifier()\n",
      "Results:  {'train_accuracy': 0.9999668456998873, 'test_accuracy': 0.8472775564409031, 'test_recall': 0.6181081081081081, 'test_precision': 0.7205419029615627}\n",
      "\n",
      "Running Logistic Regression - LogisticRegression(max_iter=10000)\n",
      "Results:  {'train_accuracy': 0.7902990517870168, 'test_accuracy': 0.7926958831341302, 'test_recall': 0.2681081081081081, 'test_precision': 0.705547652916074}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>test_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.790299</td>\n",
       "      <td>0.792696</td>\n",
       "      <td>0.268108</td>\n",
       "      <td>0.705548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Most Frequent</th>\n",
       "      <td>0.751078</td>\n",
       "      <td>0.754316</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nearest Neighbors</th>\n",
       "      <td>0.830946</td>\n",
       "      <td>0.769124</td>\n",
       "      <td>0.325405</td>\n",
       "      <td>0.551030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.999967</td>\n",
       "      <td>0.847278</td>\n",
       "      <td>0.618108</td>\n",
       "      <td>0.720542</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     train_accuracy  test_accuracy  test_recall  \\\n",
       "Logistic Regression        0.790299       0.792696     0.268108   \n",
       "Most Frequent              0.751078       0.754316     0.000000   \n",
       "Nearest Neighbors          0.830946       0.769124     0.325405   \n",
       "Random Forest              0.999967       0.847278     0.618108   \n",
       "\n",
       "                     test_precision  \n",
       "Logistic Regression        0.705548  \n",
       "Most Frequent              0.000000  \n",
       "Nearest Neighbors          0.551030  \n",
       "Random Forest              0.720542  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try to duplicate the earlier chart we manually compiled!\n",
    "# Ignore the warning --\n",
    "#    the DummyClassifier doesn't have a precision score since one class is never predicted.\n",
    "\n",
    "models = {'Most Frequent': DummyClassifier(strategy='most_frequent'),\n",
    "          'Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),\n",
    "          'Random Forest': RandomForestClassifier(n_estimators=100),\n",
    "          'Logistic Regression': LogisticRegression(solver='lbfgs', max_iter=10000)}\n",
    "\n",
    "model_results = test_models(models, \n",
    "                            X_train_df, y_train, \n",
    "                            X_test_df, y_test, \n",
    "                            verbose=True)\n",
    "\n",
    "model_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, this looks good. However, while the accuracy is high, our recall is still low.\n",
    "\n",
    "Out of all >50K households, we only correctly identify 61%!\n",
    "\n",
    "Let's try to improve the recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"imbalance\"></a>\n",
    "# Class Imbalance Techniques\n",
    "\n",
    "Our goal is to improve the recall on our test set without impacting precision much.\n",
    "\n",
    "<a id=\"undersample\"></a>\n",
    "# Imbalance Technique 1: Undersample More Frequent Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For resampling our training set, we will use the library `imbalanced-learn`.\n",
    "\n",
    "See: https://imbalanced-learn.org/stable/user_guide.html\n",
    "\n",
    "Install it using: `conda install -c conda-forge imbalanced-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import NearMiss, RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sampling_stats(X_train_df, X_train_sampled,\n",
    "                         y_train, y_train_sampled,\n",
    "                         y_test):\n",
    "    \n",
    "    print('Original dataset classes: {}'.format(Counter(y_train)))\n",
    "    print('Resampled dataset classes: {}'.format(Counter(y_train_sampled)))\n",
    "    print()\n",
    "    print('Test dataset classes still the same to model real world: {}'.format(Counter(y_test)))\n",
    "\n",
    "    print()\n",
    "    print('X_train shape (under/reg): ', X_train_sampled.shape, X_train_df.shape)\n",
    "    print('y_train shape (under/reg): ', y_train_sampled.shape, y_train.shape)\n",
    "    print('X_test, shape y_test: ', X_test_df.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset classes: Counter({'<=50K': 22654, '>50K': 7508})\n",
      "Resampled dataset classes: Counter({'<=50K': 7508, '>50K': 7508})\n",
      "\n",
      "Test dataset classes still the same to model real world: Counter({'<=50K': 11360, '>50K': 3700})\n",
      "\n",
      "X_train shape (under/reg):  (15016, 104) (30162, 104)\n",
      "y_train shape (under/reg):  (15016,) (30162,)\n",
      "X_test, shape y_test:  (15060, 104) (15060,)\n"
     ]
    }
   ],
   "source": [
    "nm = RandomUnderSampler()\n",
    "\n",
    "X_train_under, y_train_under = nm.fit_resample(X_train_df, y_train)\n",
    "\n",
    "print_sampling_stats(X_train_df, X_train_under,\n",
    "                     y_train, y_train_under,\n",
    "                     y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "# Let's see how we did!\n",
    "\n",
    "undersample_results = test_models(models, \n",
    "                                  X_train_under, y_train_under, \n",
    "                                  X_test_df, y_test, \n",
    "                                  verbose=False)\n",
    "\n",
    "print('DONE!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>test_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.790299</td>\n",
       "      <td>0.792696</td>\n",
       "      <td>0.268108</td>\n",
       "      <td>0.705548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Most Frequent</th>\n",
       "      <td>0.751078</td>\n",
       "      <td>0.754316</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nearest Neighbors</th>\n",
       "      <td>0.830946</td>\n",
       "      <td>0.769124</td>\n",
       "      <td>0.325405</td>\n",
       "      <td>0.551030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.999967</td>\n",
       "      <td>0.847278</td>\n",
       "      <td>0.618108</td>\n",
       "      <td>0.720542</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     train_accuracy  test_accuracy  test_recall  \\\n",
       "Logistic Regression        0.790299       0.792696     0.268108   \n",
       "Most Frequent              0.751078       0.754316     0.000000   \n",
       "Nearest Neighbors          0.830946       0.769124     0.325405   \n",
       "Random Forest              0.999967       0.847278     0.618108   \n",
       "\n",
       "                     test_precision  \n",
       "Logistic Regression        0.705548  \n",
       "Most Frequent              0.000000  \n",
       "Nearest Neighbors          0.551030  \n",
       "Random Forest              0.720542  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Original\n",
    "print('Original')\n",
    "model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Undersample\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>test_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.615876</td>\n",
       "      <td>0.657703</td>\n",
       "      <td>0.515676</td>\n",
       "      <td>0.361981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Most Frequent</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.754316</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nearest Neighbors</th>\n",
       "      <td>0.750400</td>\n",
       "      <td>0.618327</td>\n",
       "      <td>0.567027</td>\n",
       "      <td>0.336003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.999933</td>\n",
       "      <td>0.805511</td>\n",
       "      <td>0.821081</td>\n",
       "      <td>0.572667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     train_accuracy  test_accuracy  test_recall  \\\n",
       "Logistic Regression        0.615876       0.657703     0.515676   \n",
       "Most Frequent              0.500000       0.754316     0.000000   \n",
       "Nearest Neighbors          0.750400       0.618327     0.567027   \n",
       "Random Forest              0.999933       0.805511     0.821081   \n",
       "\n",
       "                     test_precision  \n",
       "Logistic Regression        0.361981  \n",
       "Most Frequent              0.000000  \n",
       "Nearest Neighbors          0.336003  \n",
       "Random Forest              0.572667  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Undersample\n",
    "print('Undersample')\n",
    "undersample_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Q: How did undersampling perform vs. baseline?</summary>\n",
    "For Random Forests, the `test_recall` increased greatly (from 61% to 82%).\n",
    "\n",
    "But:\n",
    "   + At the expense of overall accuracy (dropped from 84% to 76%).\n",
    "   + At the expense of precision (dropped from 71% to 51%).\n",
    "      - Now only half of our positive guesses are correct!</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: There are smarter ways of undersampling than random! For example, see the [NearMiss](https://imbalanced-learn.org/stable/generated/imblearn.under_sampling.NearMiss.html?highlight=nearmiss) undersampler. For example, it can use nearest neighbors to selectively sample only the points nearest to those in the majority class. Here are some [illustrations](https://imbalanced-learn.org/stable/auto_examples/under-sampling/plot_illustration_nearmiss.html?highlight=nearmiss) of the three strategies it supports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"oversample\"></a>\n",
    "# Imbalance Technique 2: Oversample Less Frequent Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chooses new samples at random with replacement.\n",
    "\n",
    "We might predict this will perform worse, because one of the classes will contain many repeated members."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset classes: Counter({'<=50K': 22654, '>50K': 7508})\n",
      "Resampled dataset classes: Counter({'<=50K': 22654, '>50K': 22654})\n",
      "\n",
      "Test dataset classes still the same to model real world: Counter({'<=50K': 11360, '>50K': 3700})\n",
      "\n",
      "X_train shape (under/reg):  (45308, 104) (30162, 104)\n",
      "y_train shape (under/reg):  (45308,) (30162,)\n",
      "X_test, shape y_test:  (15060, 104) (15060,)\n"
     ]
    }
   ],
   "source": [
    "ros = RandomOverSampler()\n",
    "\n",
    "X_train_over, y_train_over = ros.fit_resample(X_train_df, y_train)\n",
    "\n",
    "print_sampling_stats(X_train_df, X_train_over,\n",
    "                     y_train, y_train_over,\n",
    "                     y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "# Let's see how we did!\n",
    "\n",
    "oversample_results = test_models(models, \n",
    "                                 X_train_over, y_train_over, \n",
    "                                 X_test_df, y_test, \n",
    "                                 verbose=False)\n",
    "\n",
    "print('DONE!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>test_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.790299</td>\n",
       "      <td>0.792696</td>\n",
       "      <td>0.268108</td>\n",
       "      <td>0.705548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Most Frequent</th>\n",
       "      <td>0.751078</td>\n",
       "      <td>0.754316</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nearest Neighbors</th>\n",
       "      <td>0.830946</td>\n",
       "      <td>0.769124</td>\n",
       "      <td>0.325405</td>\n",
       "      <td>0.551030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.999967</td>\n",
       "      <td>0.847278</td>\n",
       "      <td>0.618108</td>\n",
       "      <td>0.720542</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     train_accuracy  test_accuracy  test_recall  \\\n",
       "Logistic Regression        0.790299       0.792696     0.268108   \n",
       "Most Frequent              0.751078       0.754316     0.000000   \n",
       "Nearest Neighbors          0.830946       0.769124     0.325405   \n",
       "Random Forest              0.999967       0.847278     0.618108   \n",
       "\n",
       "                     test_precision  \n",
       "Logistic Regression        0.705548  \n",
       "Most Frequent              0.000000  \n",
       "Nearest Neighbors          0.551030  \n",
       "Random Forest              0.720542  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Original\n",
    "print('Original')\n",
    "model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversample\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>test_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.616403</td>\n",
       "      <td>0.651461</td>\n",
       "      <td>0.526486</td>\n",
       "      <td>0.357759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Most Frequent</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.754316</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nearest Neighbors</th>\n",
       "      <td>0.841044</td>\n",
       "      <td>0.631341</td>\n",
       "      <td>0.588919</td>\n",
       "      <td>0.350886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.999978</td>\n",
       "      <td>0.838579</td>\n",
       "      <td>0.681081</td>\n",
       "      <td>0.668258</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     train_accuracy  test_accuracy  test_recall  \\\n",
       "Logistic Regression        0.616403       0.651461     0.526486   \n",
       "Most Frequent              0.500000       0.754316     0.000000   \n",
       "Nearest Neighbors          0.841044       0.631341     0.588919   \n",
       "Random Forest              0.999978       0.838579     0.681081   \n",
       "\n",
       "                     test_precision  \n",
       "Logistic Regression        0.357759  \n",
       "Most Frequent              0.000000  \n",
       "Nearest Neighbors          0.350886  \n",
       "Random Forest              0.668258  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Oversample\n",
    "print('Oversample')\n",
    "oversample_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Undersample\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>test_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.615876</td>\n",
       "      <td>0.657703</td>\n",
       "      <td>0.515676</td>\n",
       "      <td>0.361981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Most Frequent</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.754316</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nearest Neighbors</th>\n",
       "      <td>0.750400</td>\n",
       "      <td>0.618327</td>\n",
       "      <td>0.567027</td>\n",
       "      <td>0.336003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.999933</td>\n",
       "      <td>0.805511</td>\n",
       "      <td>0.821081</td>\n",
       "      <td>0.572667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     train_accuracy  test_accuracy  test_recall  \\\n",
       "Logistic Regression        0.615876       0.657703     0.515676   \n",
       "Most Frequent              0.500000       0.754316     0.000000   \n",
       "Nearest Neighbors          0.750400       0.618327     0.567027   \n",
       "Random Forest              0.999933       0.805511     0.821081   \n",
       "\n",
       "                     test_precision  \n",
       "Logistic Regression        0.361981  \n",
       "Most Frequent              0.000000  \n",
       "Nearest Neighbors          0.336003  \n",
       "Random Forest              0.572667  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Undersample\n",
    "print('Undersample')\n",
    "undersample_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Q: How did oversampling perform?</summary>\n",
    "Compared to the original Random Forest, the accuracy did not change. The recall is slightly better (now 68%, orig 62%) while precision slightly worse (now 67%, orig 72%). \n",
    "\n",
    "This model could be considered slightly better with minimal downside.</details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"smote\"></a>\n",
    "# Imbalance Technique 3: SMOTE (Synthetic Minority Over-sampling Technique)\n",
    "\n",
    "This technique does not just randomly oversample with replacement. Instead, it uses nearest neighbors to synthetically create fake datapoints to use for oversampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset classes: Counter({'<=50K': 22654, '>50K': 7508})\n",
      "Resampled dataset classes: Counter({'<=50K': 22654, '>50K': 22654})\n",
      "\n",
      "Test dataset classes still the same to model real world: Counter({'<=50K': 11360, '>50K': 3700})\n",
      "\n",
      "X_train shape (under/reg):  (45308, 104) (30162, 104)\n",
      "y_train shape (under/reg):  (45308,) (30162,)\n",
      "X_test, shape y_test:  (15060, 104) (15060,)\n"
     ]
    }
   ],
   "source": [
    "smo = SMOTE()\n",
    "\n",
    "X_train_smote, y_train_smote = smo.fit_resample(X_train_df, y_train)\n",
    "\n",
    "print_sampling_stats(X_train_df, X_train_smote,\n",
    "                     y_train, y_train_smote,\n",
    "                     y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "# Let's see how we did!\n",
    "\n",
    "smote_results = test_models(models, \n",
    "                            X_train_over, y_train_over, \n",
    "                            X_test_df, y_test, \n",
    "                            verbose=False)\n",
    "\n",
    "print('DONE!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>test_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.790299</td>\n",
       "      <td>0.792696</td>\n",
       "      <td>0.268108</td>\n",
       "      <td>0.705548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Most Frequent</th>\n",
       "      <td>0.751078</td>\n",
       "      <td>0.754316</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nearest Neighbors</th>\n",
       "      <td>0.830946</td>\n",
       "      <td>0.769124</td>\n",
       "      <td>0.325405</td>\n",
       "      <td>0.551030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.999967</td>\n",
       "      <td>0.847278</td>\n",
       "      <td>0.618108</td>\n",
       "      <td>0.720542</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     train_accuracy  test_accuracy  test_recall  \\\n",
       "Logistic Regression        0.790299       0.792696     0.268108   \n",
       "Most Frequent              0.751078       0.754316     0.000000   \n",
       "Nearest Neighbors          0.830946       0.769124     0.325405   \n",
       "Random Forest              0.999967       0.847278     0.618108   \n",
       "\n",
       "                     test_precision  \n",
       "Logistic Regression        0.705548  \n",
       "Most Frequent              0.000000  \n",
       "Nearest Neighbors          0.551030  \n",
       "Random Forest              0.720542  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Original')\n",
    "model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>test_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.616403</td>\n",
       "      <td>0.651461</td>\n",
       "      <td>0.526486</td>\n",
       "      <td>0.357759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Most Frequent</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.754316</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nearest Neighbors</th>\n",
       "      <td>0.841044</td>\n",
       "      <td>0.631341</td>\n",
       "      <td>0.588919</td>\n",
       "      <td>0.350886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.999934</td>\n",
       "      <td>0.838380</td>\n",
       "      <td>0.677568</td>\n",
       "      <td>0.668890</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     train_accuracy  test_accuracy  test_recall  \\\n",
       "Logistic Regression        0.616403       0.651461     0.526486   \n",
       "Most Frequent              0.500000       0.754316     0.000000   \n",
       "Nearest Neighbors          0.841044       0.631341     0.588919   \n",
       "Random Forest              0.999934       0.838380     0.677568   \n",
       "\n",
       "                     test_precision  \n",
       "Logistic Regression        0.357759  \n",
       "Most Frequent              0.000000  \n",
       "Nearest Neighbors          0.350886  \n",
       "Random Forest              0.668890  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('SMOTE')\n",
    "smote_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Q: What is your conclusion about using SMOTE?</summary>\n",
    "Conclusion: Very similar to the oversampling results. Perhaps because so much oversampling had to occur, the data integrity was in question.</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"weight\"></a>\n",
    "# Imbalance Technique 4: Weight each class in the models\n",
    "\n",
    "Some models (including Random Forests and Logistic Regression) allow you to weight each class. The weight assigned to each class penalizes that data point by the given weight when computing the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "# 'balanced' adjusts weights inversely proportional to class frequencies\n",
    "#     For class y, its weight becomes: n_samples / (n_classes * np.bincount(y))\n",
    "models['Random Forest (weighted)'] = RandomForestClassifier(n_estimators=100, \n",
    "                                                            class_weight='balanced_subsample')\n",
    "\n",
    "model_results = test_models(models, \n",
    "                            X_train_over, y_train_over, \n",
    "                            X_test_df, y_test, \n",
    "                            verbose=False)\n",
    "\n",
    "print('DONE!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>test_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.616403</td>\n",
       "      <td>0.651461</td>\n",
       "      <td>0.526486</td>\n",
       "      <td>0.357759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Most Frequent</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.754316</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nearest Neighbors</th>\n",
       "      <td>0.841044</td>\n",
       "      <td>0.631341</td>\n",
       "      <td>0.588919</td>\n",
       "      <td>0.350886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.999978</td>\n",
       "      <td>0.839309</td>\n",
       "      <td>0.681622</td>\n",
       "      <td>0.670032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest (weighted)</th>\n",
       "      <td>0.999978</td>\n",
       "      <td>0.837118</td>\n",
       "      <td>0.677838</td>\n",
       "      <td>0.665428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          train_accuracy  test_accuracy  test_recall  \\\n",
       "Logistic Regression             0.616403       0.651461     0.526486   \n",
       "Most Frequent                   0.500000       0.754316     0.000000   \n",
       "Nearest Neighbors               0.841044       0.631341     0.588919   \n",
       "Random Forest                   0.999978       0.839309     0.681622   \n",
       "Random Forest (weighted)        0.999978       0.837118     0.677838   \n",
       "\n",
       "                          test_precision  \n",
       "Logistic Regression             0.357759  \n",
       "Most Frequent                   0.000000  \n",
       "Nearest Neighbors               0.350886  \n",
       "Random Forest                   0.670032  \n",
       "Random Forest (weighted)        0.665428  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Q: Does this perform any benefit over earlier techniques?</summary>\n",
    "From the final two rows, we see that weighting the Random Forest model did not improve the metrics much. We could perhaps play around with specific class weights, but it is unlikely to drastically improve.\n",
    "\n",
    "Perhaps we could apply this technique following class oversampling.</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"roc\"></a>\n",
    "# Imbalance Technique 5: Adjust the class cutoff threshold (ROC)\n",
    "\n",
    "After predicting the probability of a class, we can then apply a threshold.\n",
    "\n",
    "By adjusting this threshold, we tradeoff between the Recall (true positive rate) and false positive rate (fpr).\n",
    "\n",
    "`false positive rate = out of all times it was actually negative, what % were FPs?`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Unfortunately, our model overfits so we'll have to generate our test set's ROC.\n",
    "\n",
    "Typically, this will be tuned on the training set.\n",
    "\n",
    "If we choose a different threshold from here, then we must test it on yet another validation set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import plot_roc_curve\n",
    "\n",
    "plot_roc_curve(models['Random Forest'], X_test_df, y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- y axis = Recall.\n",
    "- x axis = False Positive Rate. \n",
    "\n",
    "The FPR is not precision, but it can still be useful:\n",
    "    \n",
    "    - Precision = true positives / total positive predictions.\n",
    "    - False Positive Rate = false positives / total negatives.\n",
    "\n",
    "---\n",
    "Now, just choose your threshold from the following list to balance fpr/tpr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# For each test X, predict the probability of class 1\n",
    "rf = models['Random Forest']\n",
    "pred_proba = [p[1] for p in rf.predict_proba(X_test_df)]\n",
    "\n",
    "# Compute ROC values manually\n",
    "fpr, tpr, thresholds = roc_curve(y_test,\n",
    "                                 pred_proba,\n",
    "                                 pos_label='>50K')\n",
    "\n",
    "# Compute Area Under the Curve\n",
    "roc_auc = auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fpr</th>\n",
       "      <th>tpr</th>\n",
       "      <th>thresholds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000528</td>\n",
       "      <td>0.059459</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001408</td>\n",
       "      <td>0.099459</td>\n",
       "      <td>0.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.002641</td>\n",
       "      <td>0.135405</td>\n",
       "      <td>0.980000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.003169</td>\n",
       "      <td>0.160811</td>\n",
       "      <td>0.970000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.004401</td>\n",
       "      <td>0.185135</td>\n",
       "      <td>0.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.006690</td>\n",
       "      <td>0.206486</td>\n",
       "      <td>0.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.008099</td>\n",
       "      <td>0.222703</td>\n",
       "      <td>0.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.009243</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.010563</td>\n",
       "      <td>0.255135</td>\n",
       "      <td>0.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.012148</td>\n",
       "      <td>0.273514</td>\n",
       "      <td>0.910000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.012236</td>\n",
       "      <td>0.273514</td>\n",
       "      <td>0.909091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.013996</td>\n",
       "      <td>0.287568</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.015757</td>\n",
       "      <td>0.302162</td>\n",
       "      <td>0.890000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.017254</td>\n",
       "      <td>0.316757</td>\n",
       "      <td>0.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.018926</td>\n",
       "      <td>0.331351</td>\n",
       "      <td>0.870000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.020863</td>\n",
       "      <td>0.345405</td>\n",
       "      <td>0.860000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.022271</td>\n",
       "      <td>0.360541</td>\n",
       "      <td>0.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.024032</td>\n",
       "      <td>0.372432</td>\n",
       "      <td>0.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.024120</td>\n",
       "      <td>0.372432</td>\n",
       "      <td>0.838571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.025352</td>\n",
       "      <td>0.383514</td>\n",
       "      <td>0.830000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.027553</td>\n",
       "      <td>0.395676</td>\n",
       "      <td>0.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.030106</td>\n",
       "      <td>0.409459</td>\n",
       "      <td>0.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.031690</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.033539</td>\n",
       "      <td>0.429459</td>\n",
       "      <td>0.790000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.035299</td>\n",
       "      <td>0.441081</td>\n",
       "      <td>0.780000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.038116</td>\n",
       "      <td>0.454054</td>\n",
       "      <td>0.770000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.040141</td>\n",
       "      <td>0.463514</td>\n",
       "      <td>0.760000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.041725</td>\n",
       "      <td>0.474324</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.044366</td>\n",
       "      <td>0.485405</td>\n",
       "      <td>0.740000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.046303</td>\n",
       "      <td>0.495676</td>\n",
       "      <td>0.730000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.047711</td>\n",
       "      <td>0.504324</td>\n",
       "      <td>0.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.047799</td>\n",
       "      <td>0.504324</td>\n",
       "      <td>0.715000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.049912</td>\n",
       "      <td>0.512162</td>\n",
       "      <td>0.710000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.049912</td>\n",
       "      <td>0.512432</td>\n",
       "      <td>0.708000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.053257</td>\n",
       "      <td>0.521081</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.056074</td>\n",
       "      <td>0.527297</td>\n",
       "      <td>0.690000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.058891</td>\n",
       "      <td>0.537027</td>\n",
       "      <td>0.680000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.058891</td>\n",
       "      <td>0.537297</td>\n",
       "      <td>0.677424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.061180</td>\n",
       "      <td>0.545676</td>\n",
       "      <td>0.670000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.061268</td>\n",
       "      <td>0.545676</td>\n",
       "      <td>0.664429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.063820</td>\n",
       "      <td>0.555946</td>\n",
       "      <td>0.660000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.066549</td>\n",
       "      <td>0.562432</td>\n",
       "      <td>0.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.069366</td>\n",
       "      <td>0.573514</td>\n",
       "      <td>0.640000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.071479</td>\n",
       "      <td>0.585135</td>\n",
       "      <td>0.630000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.074120</td>\n",
       "      <td>0.591892</td>\n",
       "      <td>0.620000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.076937</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.610000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.080282</td>\n",
       "      <td>0.606216</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.083363</td>\n",
       "      <td>0.615405</td>\n",
       "      <td>0.590000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.083363</td>\n",
       "      <td>0.615676</td>\n",
       "      <td>0.585833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.086708</td>\n",
       "      <td>0.624595</td>\n",
       "      <td>0.580000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.086708</td>\n",
       "      <td>0.624865</td>\n",
       "      <td>0.573000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.089965</td>\n",
       "      <td>0.630541</td>\n",
       "      <td>0.570000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.091813</td>\n",
       "      <td>0.639730</td>\n",
       "      <td>0.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.094454</td>\n",
       "      <td>0.645405</td>\n",
       "      <td>0.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.098415</td>\n",
       "      <td>0.653514</td>\n",
       "      <td>0.540000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.102025</td>\n",
       "      <td>0.662432</td>\n",
       "      <td>0.530000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.105898</td>\n",
       "      <td>0.673784</td>\n",
       "      <td>0.520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.109331</td>\n",
       "      <td>0.681622</td>\n",
       "      <td>0.510000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.112940</td>\n",
       "      <td>0.691351</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.112940</td>\n",
       "      <td>0.691622</td>\n",
       "      <td>0.499091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.113028</td>\n",
       "      <td>0.691622</td>\n",
       "      <td>0.498000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.116989</td>\n",
       "      <td>0.701081</td>\n",
       "      <td>0.490000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.120599</td>\n",
       "      <td>0.707568</td>\n",
       "      <td>0.480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.120687</td>\n",
       "      <td>0.707568</td>\n",
       "      <td>0.470471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.125088</td>\n",
       "      <td>0.712973</td>\n",
       "      <td>0.470000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.128081</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.460000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.132130</td>\n",
       "      <td>0.725405</td>\n",
       "      <td>0.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.136884</td>\n",
       "      <td>0.732703</td>\n",
       "      <td>0.440000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.141637</td>\n",
       "      <td>0.740270</td>\n",
       "      <td>0.430000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.146215</td>\n",
       "      <td>0.747568</td>\n",
       "      <td>0.420000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.150264</td>\n",
       "      <td>0.754324</td>\n",
       "      <td>0.410000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.150264</td>\n",
       "      <td>0.754595</td>\n",
       "      <td>0.400381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.156162</td>\n",
       "      <td>0.762162</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.162060</td>\n",
       "      <td>0.769189</td>\n",
       "      <td>0.390000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.166989</td>\n",
       "      <td>0.775946</td>\n",
       "      <td>0.380000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.172711</td>\n",
       "      <td>0.782703</td>\n",
       "      <td>0.370000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.176585</td>\n",
       "      <td>0.788378</td>\n",
       "      <td>0.360000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.182130</td>\n",
       "      <td>0.792432</td>\n",
       "      <td>0.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.186708</td>\n",
       "      <td>0.797568</td>\n",
       "      <td>0.340000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.191989</td>\n",
       "      <td>0.805946</td>\n",
       "      <td>0.330000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.198239</td>\n",
       "      <td>0.812973</td>\n",
       "      <td>0.320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.202905</td>\n",
       "      <td>0.819730</td>\n",
       "      <td>0.310000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.210387</td>\n",
       "      <td>0.827027</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.215493</td>\n",
       "      <td>0.835946</td>\n",
       "      <td>0.290000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.221919</td>\n",
       "      <td>0.844054</td>\n",
       "      <td>0.280000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.227553</td>\n",
       "      <td>0.850811</td>\n",
       "      <td>0.270000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.233451</td>\n",
       "      <td>0.855946</td>\n",
       "      <td>0.260000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.240493</td>\n",
       "      <td>0.863514</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.247975</td>\n",
       "      <td>0.871622</td>\n",
       "      <td>0.240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.256514</td>\n",
       "      <td>0.878649</td>\n",
       "      <td>0.230000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.263292</td>\n",
       "      <td>0.885676</td>\n",
       "      <td>0.220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.271567</td>\n",
       "      <td>0.892162</td>\n",
       "      <td>0.210000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.279754</td>\n",
       "      <td>0.896486</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.288908</td>\n",
       "      <td>0.903784</td>\n",
       "      <td>0.190000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.297887</td>\n",
       "      <td>0.908649</td>\n",
       "      <td>0.180000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.309595</td>\n",
       "      <td>0.915405</td>\n",
       "      <td>0.170000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.319630</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.330194</td>\n",
       "      <td>0.927838</td>\n",
       "      <td>0.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.343046</td>\n",
       "      <td>0.931892</td>\n",
       "      <td>0.140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.353609</td>\n",
       "      <td>0.938378</td>\n",
       "      <td>0.130000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.368310</td>\n",
       "      <td>0.943514</td>\n",
       "      <td>0.120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.382658</td>\n",
       "      <td>0.948108</td>\n",
       "      <td>0.110000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.396303</td>\n",
       "      <td>0.952973</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0.413996</td>\n",
       "      <td>0.957568</td>\n",
       "      <td>0.090000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0.435827</td>\n",
       "      <td>0.961892</td>\n",
       "      <td>0.080000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.458627</td>\n",
       "      <td>0.965135</td>\n",
       "      <td>0.070000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.483803</td>\n",
       "      <td>0.970270</td>\n",
       "      <td>0.060000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0.514349</td>\n",
       "      <td>0.975676</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>0.544454</td>\n",
       "      <td>0.982703</td>\n",
       "      <td>0.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>0.584595</td>\n",
       "      <td>0.987027</td>\n",
       "      <td>0.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0.637236</td>\n",
       "      <td>0.990811</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>0.725176</td>\n",
       "      <td>0.994324</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          fpr       tpr  thresholds\n",
       "0    0.000000  0.000000    2.000000\n",
       "1    0.000528  0.059459    1.000000\n",
       "2    0.001408  0.099459    0.990000\n",
       "3    0.002641  0.135405    0.980000\n",
       "4    0.003169  0.160811    0.970000\n",
       "5    0.004401  0.185135    0.960000\n",
       "6    0.006690  0.206486    0.950000\n",
       "7    0.008099  0.222703    0.940000\n",
       "8    0.009243  0.240000    0.930000\n",
       "9    0.010563  0.255135    0.920000\n",
       "10   0.012148  0.273514    0.910000\n",
       "11   0.012236  0.273514    0.909091\n",
       "12   0.013996  0.287568    0.900000\n",
       "13   0.015757  0.302162    0.890000\n",
       "14   0.017254  0.316757    0.880000\n",
       "15   0.018926  0.331351    0.870000\n",
       "16   0.020863  0.345405    0.860000\n",
       "17   0.022271  0.360541    0.850000\n",
       "18   0.024032  0.372432    0.840000\n",
       "19   0.024120  0.372432    0.838571\n",
       "20   0.025352  0.383514    0.830000\n",
       "21   0.027553  0.395676    0.820000\n",
       "22   0.030106  0.409459    0.810000\n",
       "23   0.031690  0.420000    0.800000\n",
       "24   0.033539  0.429459    0.790000\n",
       "25   0.035299  0.441081    0.780000\n",
       "26   0.038116  0.454054    0.770000\n",
       "27   0.040141  0.463514    0.760000\n",
       "28   0.041725  0.474324    0.750000\n",
       "29   0.044366  0.485405    0.740000\n",
       "30   0.046303  0.495676    0.730000\n",
       "31   0.047711  0.504324    0.720000\n",
       "32   0.047799  0.504324    0.715000\n",
       "33   0.049912  0.512162    0.710000\n",
       "34   0.049912  0.512432    0.708000\n",
       "35   0.053257  0.521081    0.700000\n",
       "36   0.056074  0.527297    0.690000\n",
       "37   0.058891  0.537027    0.680000\n",
       "38   0.058891  0.537297    0.677424\n",
       "39   0.061180  0.545676    0.670000\n",
       "40   0.061268  0.545676    0.664429\n",
       "41   0.063820  0.555946    0.660000\n",
       "42   0.066549  0.562432    0.650000\n",
       "43   0.069366  0.573514    0.640000\n",
       "44   0.071479  0.585135    0.630000\n",
       "45   0.074120  0.591892    0.620000\n",
       "46   0.076937  0.600000    0.610000\n",
       "47   0.080282  0.606216    0.600000\n",
       "48   0.083363  0.615405    0.590000\n",
       "49   0.083363  0.615676    0.585833\n",
       "50   0.086708  0.624595    0.580000\n",
       "51   0.086708  0.624865    0.573000\n",
       "52   0.089965  0.630541    0.570000\n",
       "53   0.091813  0.639730    0.560000\n",
       "54   0.094454  0.645405    0.550000\n",
       "55   0.098415  0.653514    0.540000\n",
       "56   0.102025  0.662432    0.530000\n",
       "57   0.105898  0.673784    0.520000\n",
       "58   0.109331  0.681622    0.510000\n",
       "59   0.112940  0.691351    0.500000\n",
       "60   0.112940  0.691622    0.499091\n",
       "61   0.113028  0.691622    0.498000\n",
       "62   0.116989  0.701081    0.490000\n",
       "63   0.120599  0.707568    0.480000\n",
       "64   0.120687  0.707568    0.470471\n",
       "65   0.125088  0.712973    0.470000\n",
       "66   0.128081  0.720000    0.460000\n",
       "67   0.132130  0.725405    0.450000\n",
       "68   0.136884  0.732703    0.440000\n",
       "69   0.141637  0.740270    0.430000\n",
       "70   0.146215  0.747568    0.420000\n",
       "71   0.150264  0.754324    0.410000\n",
       "72   0.150264  0.754595    0.400381\n",
       "73   0.156162  0.762162    0.400000\n",
       "74   0.162060  0.769189    0.390000\n",
       "75   0.166989  0.775946    0.380000\n",
       "76   0.172711  0.782703    0.370000\n",
       "77   0.176585  0.788378    0.360000\n",
       "78   0.182130  0.792432    0.350000\n",
       "79   0.186708  0.797568    0.340000\n",
       "80   0.191989  0.805946    0.330000\n",
       "81   0.198239  0.812973    0.320000\n",
       "82   0.202905  0.819730    0.310000\n",
       "83   0.210387  0.827027    0.300000\n",
       "84   0.215493  0.835946    0.290000\n",
       "85   0.221919  0.844054    0.280000\n",
       "86   0.227553  0.850811    0.270000\n",
       "87   0.233451  0.855946    0.260000\n",
       "88   0.240493  0.863514    0.250000\n",
       "89   0.247975  0.871622    0.240000\n",
       "90   0.256514  0.878649    0.230000\n",
       "91   0.263292  0.885676    0.220000\n",
       "92   0.271567  0.892162    0.210000\n",
       "93   0.279754  0.896486    0.200000\n",
       "94   0.288908  0.903784    0.190000\n",
       "95   0.297887  0.908649    0.180000\n",
       "96   0.309595  0.915405    0.170000\n",
       "97   0.319630  0.920000    0.160000\n",
       "98   0.330194  0.927838    0.150000\n",
       "99   0.343046  0.931892    0.140000\n",
       "100  0.353609  0.938378    0.130000\n",
       "101  0.368310  0.943514    0.120000\n",
       "102  0.382658  0.948108    0.110000\n",
       "103  0.396303  0.952973    0.100000\n",
       "104  0.413996  0.957568    0.090000\n",
       "105  0.435827  0.961892    0.080000\n",
       "106  0.458627  0.965135    0.070000\n",
       "107  0.483803  0.970270    0.060000\n",
       "108  0.514349  0.975676    0.050000\n",
       "109  0.544454  0.982703    0.040000\n",
       "110  0.584595  0.987027    0.030000\n",
       "111  0.637236  0.990811    0.020000\n",
       "112  0.725176  0.994324    0.010000\n",
       "113  1.000000  1.000000    0.000000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE: true positive rate = recall\n",
    "#      false positive rate = out of all times it was actually negative, what % were FPs?\n",
    "\n",
    "pd.set_option('display.max_rows', 120)\n",
    "pd.DataFrame.from_dict({'fpr': fpr, 'tpr': tpr, 'thresholds': thresholds})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"next\"></a>\n",
    "# Next Steps - Possible Ideas\n",
    "\n",
    "+ Instead of only comparing models using Recall, try using AUC instead. This makes sense because AUC measures how far apart the two classes are in \"model space\". We ultimately want the classes to be very distinct from each other, so that should work better than just optimizing for Recall alone.\n",
    "+ We did not attempt to optimize the random forest model. So, we are missing obvious sources of improvement.\n",
    "\n",
    "    + Immediately, we notice that the random forest model overfits. So, we would want to try pruning the trees or making them have less depth to begin with. We could also use CV to adjust some parameters, since we are given a validation set from the beginning!\n",
    "\n",
    "+ We relied on the test set exclusively for estimating OOS performance. Instead, it would be nice to get a more accurate estimate with error bounds by using CV.\n",
    "+ Ideally, to productionalize this we would have it output a CSV with a list of the customers predicted to make >50K.\n",
    "+ Apply results from the EDA to improve the model. For example, removing non-correlated columns or log-transforming the results might improve the usability of the features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, I would choose random forest with the slight improvement from oversampling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
