{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# NLP I: Language Data Pre-Processing and Sentiment Analysis\n",
    "\n",
    "_Authors: Matt Brems, Noelle Brown_\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "1. Define and implement tokenizing, lemmatizing, and stemming.\n",
    "2. Preprocess text data.\n",
    "3. Define and apply sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before we begin, try running this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cat'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize(\"cats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you get an error with the above code, run this & follow below directions:\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ran into issues with the above:\n",
    "\n",
    "1. Run `nltk.download()`. A new screen will pop up outside your Jupyter notebook. (It may be hidden behind other windows.)\n",
    "2. Once this box opens up, click `all`, then `download`. Once this is done, restart your Jupyter notebook and try running the first three cells again.\n",
    "3. Run:\n",
    "```python\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize(\"cats\")```\n",
    "\n",
    "    - If this returns `cat`, then fantastic! You’re done. \n",
    "    - If not, head to http://www.nltk.org/install.html and follow instructions for your computer, then try running the first three cells again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which of these was machine generated?\n",
    "\n",
    "- A: \"Kilimanjaro is a mountain of 19,710 feet covered with snow and is said to be the highest mountain in Africa. The summit of the west is called “Ngaje Ngai” in Masai, the house of God. Near the top of the west there is a dry and frozen dead body of leopard. No one has ever explained what leopard wanted at that altitude.\"\n",
    "\n",
    "- B: \"Kilimanjaro is a snow-covered mountain 19,710 feet high, and is said to be the highest mountain in Africa. Its western summit is called the Masai “Ngaje Ngai,” the House of God. Close to the western summit there is the dried and frozen carcass of a leopard. No one has explained what the leopard was seeking at that altitude.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Answer:</summary>\n",
    "\n",
    "- Item B was written by [Ernest Hemingway](https://en.wikipedia.org/wiki/Ernest_Hemingway) in \"The Snows of Kilimanjaro.\"\n",
    "\n",
    "- Item A was produced by a Japanese author translating \"The Snows of Kilimanjaro\" into Japanese, then this Japanese version was passed through Google Translate so that it could be \"translated back\" into English.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Natural language processing** (NLP) describes the field of getting computers to understand language how we as humans do. Natural language processing has many, many applications including:\n",
    "- voice-to-text services for people who are hard of hearing.\n",
    "- text-to-voice services for people who have difficulty reading.\n",
    "- automated chatbots for organizations.\n",
    "- translation services.\n",
    "\n",
    "Generally when we get text data, strings aren't broken out into individual words or even sentences. We might have a full tweet, full chapter of a book, or full .pdf file all in one long string.\n",
    "\n",
    "Today, we're diving into the practical side of NLP: taking text data and breaking it out into words that we can then leverage in machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd       \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello,\n",
      "I saw your contact information on LinkedIn. I have carefully read through your profile and you seem to have an outstanding personality. This is one major reason why I am in contact with you. My name is Mr. Valery Grayfer Chairman of the Board of Directors of PJSC \"LUKOIL\". I am 86 years old and I was diagnosed with cancer 2 years ago. I will be going in for an operation later this week. I decided to WILL/Donate the sum of 8,750,000.00 Euros(Eight Million Seven Hundred And Fifty Thousand Euros Only etc. etc.\n"
     ]
    }
   ],
   "source": [
    "# Define spam text.\n",
    "spam = 'Hello,\\nI saw your contact information on LinkedIn. I have carefully read through your profile and you seem to have an outstanding personality. This is one major reason why I am in contact with you. My name is Mr. Valery Grayfer Chairman of the Board of Directors of PJSC \"LUKOIL\". I am 86 years old and I was diagnosed with cancer 2 years ago. I will be going in for an operation later this week. I decided to WILL/Donate the sum of 8,750,000.00 Euros(Eight Million Seven Hundred And Fifty Thousand Euros Only etc. etc.'\n",
    "\n",
    "print(spam)\n",
    "\n",
    "# this is a string \n",
    "# but what kind of information do machine learning \n",
    "# algo take? \n",
    "# --> Numerical \n",
    "\n",
    "#  what is the path to structured numerical data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing \n",
    "\n",
    "When dealing with text data, there are common pre-processing steps. We won't necessarily use all of them every time we deal with text data.\n",
    "\n",
    "- Remove special characters\n",
    "- Tokenizing\n",
    "- Lemmatizing/Stemming\n",
    "- Stop word removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing special characters & Tokenizing\n",
    "\n",
    "We need to remove unnecessary characters when cleaning text data (punctuation, symbols, etc.). This can be done with RegEx (more on that in a bit).\n",
    "\n",
    "When we \"**tokenize**\" data, we take it and split it up into distinct chunks based on some pattern.\n",
    "\n",
    "If we use a RegEx tokenizer, we often can do these steps together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello,\\ni saw your contact information on linkedin.',\n",
       " 'i have carefully read through your profile and you seem to have an outstanding personality.',\n",
       " 'this is one major reason why i am in contact with you.',\n",
       " 'my name is mr. valery grayfer chairman of the board of directors of pjsc \"lukoil\".',\n",
       " 'i am 86 years old and i was diagnosed with cancer 2 years ago.',\n",
       " 'i will be going in for an operation later this week.',\n",
       " 'i decided to will/donate the sum of 8,750,000.00 euros(eight million seven hundred and fifty thousand euros only etc.',\n",
       " 'etc.']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentence tokenizer\n",
    "\n",
    "sent_tokenize(spam.lower())\n",
    "\n",
    "# tokenizing in sentences \n",
    "# often not what we want "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello,\n",
      "i saw your contact information on linkedin.\n",
      "i have carefully read through your profile and you seem to have an outstanding personality.\n",
      "this is one major reason why i am in contact with you.\n",
      "my name is mr. valery grayfer chairman of the board of directors of pjsc \"lukoil\".\n",
      "i am 86 years old and i was diagnosed with cancer 2 years ago.\n",
      "i will be going in for an operation later this week.\n",
      "i decided to will/donate the sum of 8,750,000.00 euros(eight million seven hundred and fifty thousand euros only etc.\n",
      "etc.\n"
     ]
    }
   ],
   "source": [
    "# currencies will have issue, cause has a dot which may end the sentence. \n",
    "\n",
    "for sent in sent_tokenize(spam.lower()):\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " ',',\n",
       " 'i',\n",
       " 'saw',\n",
       " 'your',\n",
       " 'contact',\n",
       " 'information',\n",
       " 'on',\n",
       " 'linkedin',\n",
       " '.',\n",
       " 'i',\n",
       " 'have',\n",
       " 'carefully',\n",
       " 'read',\n",
       " 'through',\n",
       " 'your',\n",
       " 'profile',\n",
       " 'and',\n",
       " 'you',\n",
       " 'seem',\n",
       " 'to',\n",
       " 'have',\n",
       " 'an',\n",
       " 'outstanding',\n",
       " 'personality',\n",
       " '.',\n",
       " 'this',\n",
       " 'is',\n",
       " 'one',\n",
       " 'major',\n",
       " 'reason',\n",
       " 'why',\n",
       " 'i',\n",
       " 'am',\n",
       " 'in',\n",
       " 'contact',\n",
       " 'with',\n",
       " 'you',\n",
       " '.',\n",
       " 'my',\n",
       " 'name',\n",
       " 'is',\n",
       " 'mr.',\n",
       " 'valery',\n",
       " 'grayfer',\n",
       " 'chairman',\n",
       " 'of',\n",
       " 'the',\n",
       " 'board',\n",
       " 'of',\n",
       " 'directors',\n",
       " 'of',\n",
       " 'pjsc',\n",
       " '``',\n",
       " 'lukoil',\n",
       " \"''\",\n",
       " '.',\n",
       " 'i',\n",
       " 'am',\n",
       " '86',\n",
       " 'years',\n",
       " 'old',\n",
       " 'and',\n",
       " 'i',\n",
       " 'was',\n",
       " 'diagnosed',\n",
       " 'with',\n",
       " 'cancer',\n",
       " '2',\n",
       " 'years',\n",
       " 'ago',\n",
       " '.',\n",
       " 'i',\n",
       " 'will',\n",
       " 'be',\n",
       " 'going',\n",
       " 'in',\n",
       " 'for',\n",
       " 'an',\n",
       " 'operation',\n",
       " 'later',\n",
       " 'this',\n",
       " 'week',\n",
       " '.',\n",
       " 'i',\n",
       " 'decided',\n",
       " 'to',\n",
       " 'will/donate',\n",
       " 'the',\n",
       " 'sum',\n",
       " 'of',\n",
       " '8,750,000.00',\n",
       " 'euros',\n",
       " '(',\n",
       " 'eight',\n",
       " 'million',\n",
       " 'seven',\n",
       " 'hundred',\n",
       " 'and',\n",
       " 'fifty',\n",
       " 'thousand',\n",
       " 'euros',\n",
       " 'only',\n",
       " 'etc',\n",
       " '.',\n",
       " 'etc',\n",
       " '.']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word tokenizer\n",
    "\n",
    "word_tokenize(spam.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate RegExp Tokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+') ## We'll talk about this in a moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Run\" Tokenizer\n",
    "spam_tokens = tokenizer.tokenize(spam.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " 'i',\n",
       " 'saw',\n",
       " 'your',\n",
       " 'contact',\n",
       " 'information',\n",
       " 'on',\n",
       " 'linkedin',\n",
       " 'i',\n",
       " 'have',\n",
       " 'carefully',\n",
       " 'read',\n",
       " 'through',\n",
       " 'your',\n",
       " 'profile',\n",
       " 'and',\n",
       " 'you',\n",
       " 'seem',\n",
       " 'to',\n",
       " 'have',\n",
       " 'an',\n",
       " 'outstanding',\n",
       " 'personality',\n",
       " 'this',\n",
       " 'is',\n",
       " 'one',\n",
       " 'major',\n",
       " 'reason',\n",
       " 'why',\n",
       " 'i',\n",
       " 'am',\n",
       " 'in',\n",
       " 'contact',\n",
       " 'with',\n",
       " 'you',\n",
       " 'my',\n",
       " 'name',\n",
       " 'is',\n",
       " 'mr',\n",
       " 'valery',\n",
       " 'grayfer',\n",
       " 'chairman',\n",
       " 'of',\n",
       " 'the',\n",
       " 'board',\n",
       " 'of',\n",
       " 'directors',\n",
       " 'of',\n",
       " 'pjsc',\n",
       " 'lukoil',\n",
       " 'i',\n",
       " 'am',\n",
       " '86',\n",
       " 'years',\n",
       " 'old',\n",
       " 'and',\n",
       " 'i',\n",
       " 'was',\n",
       " 'diagnosed',\n",
       " 'with',\n",
       " 'cancer',\n",
       " '2',\n",
       " 'years',\n",
       " 'ago',\n",
       " 'i',\n",
       " 'will',\n",
       " 'be',\n",
       " 'going',\n",
       " 'in',\n",
       " 'for',\n",
       " 'an',\n",
       " 'operation',\n",
       " 'later',\n",
       " 'this',\n",
       " 'week',\n",
       " 'i',\n",
       " 'decided',\n",
       " 'to',\n",
       " 'will',\n",
       " 'donate',\n",
       " 'the',\n",
       " 'sum',\n",
       " 'of',\n",
       " '8',\n",
       " '750',\n",
       " '000',\n",
       " '00',\n",
       " 'euros',\n",
       " 'eight',\n",
       " 'million',\n",
       " 'seven',\n",
       " 'hundred',\n",
       " 'and',\n",
       " 'fifty',\n",
       " 'thousand',\n",
       " 'euros',\n",
       " 'only',\n",
       " 'etc',\n",
       " 'etc']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show Results\n",
    "spam_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>In comparing the original text to our tokenized version of the text, we converted one long string into a list of strings. What other changes occurred?</summary>\n",
    "\n",
    "- All strings were converted to lower case.\n",
    "- All punctuation was removed. (This was done using **regular expressions**.)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Briefly: Regular Expressions\n",
    "\n",
    "Regular Expressions, or RegEx, is a helpful tool for detecting patterns in text. \n",
    "- This is a tool of which you should be aware!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([], 'hello'),\n",
       " ([], 'i'),\n",
       " ([], 'saw'),\n",
       " ([], 'your'),\n",
       " ([], 'contact'),\n",
       " ([], 'information'),\n",
       " ([], 'on'),\n",
       " ([], 'linkedin'),\n",
       " ([], 'i'),\n",
       " ([], 'have'),\n",
       " ([], 'carefully'),\n",
       " ([], 'read'),\n",
       " ([], 'through'),\n",
       " ([], 'your'),\n",
       " ([], 'profile'),\n",
       " ([], 'and'),\n",
       " ([], 'you'),\n",
       " ([], 'seem'),\n",
       " ([], 'to'),\n",
       " ([], 'have'),\n",
       " ([], 'an'),\n",
       " ([], 'outstanding'),\n",
       " ([], 'personality'),\n",
       " ([], 'this'),\n",
       " ([], 'is'),\n",
       " ([], 'one'),\n",
       " ([], 'major'),\n",
       " ([], 'reason'),\n",
       " ([], 'why'),\n",
       " ([], 'i'),\n",
       " ([], 'am'),\n",
       " ([], 'in'),\n",
       " ([], 'contact'),\n",
       " ([], 'with'),\n",
       " ([], 'you'),\n",
       " ([], 'my'),\n",
       " ([], 'name'),\n",
       " ([], 'is'),\n",
       " ([], 'mr'),\n",
       " ([], 'valery'),\n",
       " ([], 'grayfer'),\n",
       " ([], 'chairman'),\n",
       " ([], 'of'),\n",
       " ([], 'the'),\n",
       " ([], 'board'),\n",
       " ([], 'of'),\n",
       " ([], 'directors'),\n",
       " ([], 'of'),\n",
       " ([], 'pjsc'),\n",
       " ([], 'lukoil'),\n",
       " ([], 'i'),\n",
       " ([], 'am'),\n",
       " (['86'], '86'),\n",
       " ([], 'years'),\n",
       " ([], 'old'),\n",
       " ([], 'and'),\n",
       " ([], 'i'),\n",
       " ([], 'was'),\n",
       " ([], 'diagnosed'),\n",
       " ([], 'with'),\n",
       " ([], 'cancer'),\n",
       " (['2'], '2'),\n",
       " ([], 'years'),\n",
       " ([], 'ago'),\n",
       " ([], 'i'),\n",
       " ([], 'will'),\n",
       " ([], 'be'),\n",
       " ([], 'going'),\n",
       " ([], 'in'),\n",
       " ([], 'for'),\n",
       " ([], 'an'),\n",
       " ([], 'operation'),\n",
       " ([], 'later'),\n",
       " ([], 'this'),\n",
       " ([], 'week'),\n",
       " ([], 'i'),\n",
       " ([], 'decided'),\n",
       " ([], 'to'),\n",
       " ([], 'will'),\n",
       " ([], 'donate'),\n",
       " ([], 'the'),\n",
       " ([], 'sum'),\n",
       " ([], 'of'),\n",
       " (['8'], '8'),\n",
       " (['750'], '750'),\n",
       " (['000'], '000'),\n",
       " (['00'], '00'),\n",
       " ([], 'euros'),\n",
       " ([], 'eight'),\n",
       " ([], 'million'),\n",
       " ([], 'seven'),\n",
       " ([], 'hundred'),\n",
       " ([], 'and'),\n",
       " ([], 'fifty'),\n",
       " ([], 'thousand'),\n",
       " ([], 'euros'),\n",
       " ([], 'only'),\n",
       " ([], 'etc'),\n",
       " ([], 'etc')]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(re.findall('\\d+', i), i) for i in spam_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RegEx in Python 3 understands `\\d+` to identify numeric digits. Therefore, the above code searched through `spam_tokens` to see if any numeric digits were in there. \n",
    "\n",
    "A `RegexpTokenizer` splits a string into substrings using regular expressions.\n",
    "\n",
    "The following example is pulled from [this site](http://www.nltk.org/_modules/nltk/tokenize/regexp.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good muffins cost $3.88\n",
      "in New York.  Please buy me\n",
      "two of them.\n",
      "\n",
      "Thanks.\n"
     ]
    }
   ],
   "source": [
    "# Define and print string.\n",
    "s = \"Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\n\\nThanks.\"\n",
    "\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate tokenizer.\n",
    "# match any alpha numeric \n",
    "tokenizer_1 = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good',\n",
       " 'muffins',\n",
       " 'cost',\n",
       " '$3.88',\n",
       " 'in',\n",
       " 'New',\n",
       " 'York',\n",
       " '.',\n",
       " 'Please',\n",
       " 'buy',\n",
       " 'me',\n",
       " 'two',\n",
       " 'of',\n",
       " 'them',\n",
       " '.',\n",
       " 'Thanks',\n",
       " '.']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run tokenizer.\n",
    "tokenizer_1.tokenize(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tokenizer_1` splits tokens up by spaces or by periods that are not attached to a digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good',\n",
       " 'muffins',\n",
       " 'cost',\n",
       " '$3.88',\n",
       " 'in',\n",
       " 'New',\n",
       " 'York.',\n",
       " 'Please',\n",
       " 'buy',\n",
       " 'me',\n",
       " 'two',\n",
       " 'of',\n",
       " 'them.',\n",
       " 'Thanks.']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate tokenizer by space \n",
    "# gaps True -> take out white space \n",
    "\n",
    "tokenizer_2 = RegexpTokenizer('\\s+', gaps=True)\n",
    "\n",
    "# Run tokenizer.\n",
    "\n",
    "tokenizer_2.tokenize(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tokenizer_2` will identify the spaces. By setting `gaps = True`, we're grabbing everything else: thus, we're splitting our tokens up by spaces.\n",
    "- If you changed to `gaps = False`, you'll return only the whitespaces!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good', 'New', 'York', 'Please', 'Thanks']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate tokenizer -> to keep all CAPS in front \n",
    "tokenizer_3 = RegexpTokenizer('[A-Z]\\w+')\n",
    "\n",
    "# Run tokenizer.\n",
    "tokenizer_3.tokenize(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tokenizer_3` returns only words that begin with a capital letter.\n",
    "\n",
    "As you can imagine, using RegEx _can_ be incredibly helpful if you want to find text matching a specific pattern.\n",
    "- People used to use two spaces after a period to split sentences up; you could use RegEx to detect that pattern and tokenize on entire sentences.\n",
    "- Chapters in a book could be titled \"Chapter\" followed by a number; you could use RegEx to detect that pattern and tokenize a book by its chapters.\n",
    "- When Python libraries are upgraded, syntax changes! Perhaps you want to detect a certain pattern of syntax so you can update your code efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/regex.png)\n",
    "\n",
    "[_from xkcd_](https://xkcd.com/1171/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We want to go from subreddit post -> bunch of words \n",
    "\n",
    "- Which regex should i choose to get the most valid words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizing & Stemming\n",
    "\n",
    "- \"He is *running* really fast!\"\n",
    "- \"He *ran* the race.\"\n",
    "- \"He *runs* a five-minute mile.\"\n",
    "\n",
    "If we wanted a computer to interpret these sentences, I might count up how many times I see each word. The computer will treat words like \"running,\" \"ran,\" and \"runs\" differently... but they mean very similar things (in this context)!\n",
    "\n",
    "**Lemmatizing** and **stemming** are two forms of shortening words so we can combine similar forms of the same word.\n",
    "\n",
    "When we \"**lemmatize**\" data, we take words and attempt to return their *lemma*, or the base/dictionary form of a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate lemmatizer. \n",
    "# refer to dictionary form of a word and convert to its roots \n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " 'i',\n",
       " 'saw',\n",
       " 'your',\n",
       " 'contact',\n",
       " 'information',\n",
       " 'on',\n",
       " 'linkedin',\n",
       " 'i',\n",
       " 'have',\n",
       " 'carefully',\n",
       " 'read',\n",
       " 'through',\n",
       " 'your',\n",
       " 'profile',\n",
       " 'and',\n",
       " 'you',\n",
       " 'seem',\n",
       " 'to',\n",
       " 'have',\n",
       " 'an',\n",
       " 'outstanding',\n",
       " 'personality',\n",
       " 'this',\n",
       " 'is',\n",
       " 'one',\n",
       " 'major',\n",
       " 'reason',\n",
       " 'why',\n",
       " 'i',\n",
       " 'am',\n",
       " 'in',\n",
       " 'contact',\n",
       " 'with',\n",
       " 'you',\n",
       " 'my',\n",
       " 'name',\n",
       " 'is',\n",
       " 'mr',\n",
       " 'valery',\n",
       " 'grayfer',\n",
       " 'chairman',\n",
       " 'of',\n",
       " 'the',\n",
       " 'board',\n",
       " 'of',\n",
       " 'director',\n",
       " 'of',\n",
       " 'pjsc',\n",
       " 'lukoil',\n",
       " 'i',\n",
       " 'am',\n",
       " '86',\n",
       " 'year',\n",
       " 'old',\n",
       " 'and',\n",
       " 'i',\n",
       " 'wa',\n",
       " 'diagnosed',\n",
       " 'with',\n",
       " 'cancer',\n",
       " '2',\n",
       " 'year',\n",
       " 'ago',\n",
       " 'i',\n",
       " 'will',\n",
       " 'be',\n",
       " 'going',\n",
       " 'in',\n",
       " 'for',\n",
       " 'an',\n",
       " 'operation',\n",
       " 'later',\n",
       " 'this',\n",
       " 'week',\n",
       " 'i',\n",
       " 'decided',\n",
       " 'to',\n",
       " 'will',\n",
       " 'donate',\n",
       " 'the',\n",
       " 'sum',\n",
       " 'of',\n",
       " '8',\n",
       " '750',\n",
       " '000',\n",
       " '00',\n",
       " 'euro',\n",
       " 'eight',\n",
       " 'million',\n",
       " 'seven',\n",
       " 'hundred',\n",
       " 'and',\n",
       " 'fifty',\n",
       " 'thousand',\n",
       " 'euro',\n",
       " 'only',\n",
       " 'etc',\n",
       " 'etc']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[lemmatizer.lemmatize(i) for i in spam_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize tokens.\n",
    "tokens_lem = [lemmatizer.lemmatize(i) for i in spam_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directors director\n",
      "years year\n",
      "was wa\n",
      "years year\n",
      "euros euro\n",
      "euros euro\n"
     ]
    }
   ],
   "source": [
    "# Compare tokens to lemmatized version.\n",
    "# found plural version \n",
    "# from was to wa \n",
    "\n",
    "for token,tokenized in list(zip(spam_tokens, tokens_lem)):\n",
    "    if token != tokenized:\n",
    "        print (token,tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('directors', 'director'),\n",
       " ('years', 'year'),\n",
       " ('was', 'wa'),\n",
       " ('years', 'year'),\n",
       " ('euros', 'euro'),\n",
       " ('euros', 'euro')]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print only those lemmatized tokens that are different.\n",
    "[(spam_tokens[i], tokens_lem[i]) for i in range(len(spam_tokens)) if spam_tokens[i] != tokens_lem[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatizing is usually the more correct and precise way of handling things from a grammatical point of view, but also might not have much of an effect.\n",
    "\n",
    "We can also do this on individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'computer'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lemmatize the word \"computers.\"\n",
    "lemmatizer.lemmatize(\"computers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'computation'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lemmatize the word \"computation\"\n",
    "lemmatizer.lemmatize(\"computation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'computationally'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lemmatize the word \"computationally\"\n",
    "lemmatizer.lemmatize(\"computationally\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we \"**stem**\" data, we take words and attempt to return a base form of the word. It tends to be cruder than using lemmatization. There's a [method developed by Porter in 1980](https://www.cs.toronto.edu/~frank/csc2501/Readings/R2_Porter/Porter-1980.pdf) that explains the algorithm used below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate PorterStemmer.\n",
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stem tokens.\n",
    "stem_spam = [p_stemmer.stem(i) for i in spam_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hello', 'hello'),\n",
       " ('i', 'i'),\n",
       " ('saw', 'saw'),\n",
       " ('your', 'your'),\n",
       " ('contact', 'contact'),\n",
       " ('information', 'inform'),\n",
       " ('on', 'on'),\n",
       " ('linkedin', 'linkedin'),\n",
       " ('i', 'i'),\n",
       " ('have', 'have'),\n",
       " ('carefully', 'care'),\n",
       " ('read', 'read'),\n",
       " ('through', 'through'),\n",
       " ('your', 'your'),\n",
       " ('profile', 'profil'),\n",
       " ('and', 'and'),\n",
       " ('you', 'you'),\n",
       " ('seem', 'seem'),\n",
       " ('to', 'to'),\n",
       " ('have', 'have'),\n",
       " ('an', 'an'),\n",
       " ('outstanding', 'outstand'),\n",
       " ('personality', 'person'),\n",
       " ('this', 'thi'),\n",
       " ('is', 'is'),\n",
       " ('one', 'one'),\n",
       " ('major', 'major'),\n",
       " ('reason', 'reason'),\n",
       " ('why', 'whi'),\n",
       " ('i', 'i'),\n",
       " ('am', 'am'),\n",
       " ('in', 'in'),\n",
       " ('contact', 'contact'),\n",
       " ('with', 'with'),\n",
       " ('you', 'you'),\n",
       " ('my', 'my'),\n",
       " ('name', 'name'),\n",
       " ('is', 'is'),\n",
       " ('mr', 'mr'),\n",
       " ('valery', 'valeri'),\n",
       " ('grayfer', 'grayfer'),\n",
       " ('chairman', 'chairman'),\n",
       " ('of', 'of'),\n",
       " ('the', 'the'),\n",
       " ('board', 'board'),\n",
       " ('of', 'of'),\n",
       " ('directors', 'director'),\n",
       " ('of', 'of'),\n",
       " ('pjsc', 'pjsc'),\n",
       " ('lukoil', 'lukoil'),\n",
       " ('i', 'i'),\n",
       " ('am', 'am'),\n",
       " ('86', '86'),\n",
       " ('years', 'year'),\n",
       " ('old', 'old'),\n",
       " ('and', 'and'),\n",
       " ('i', 'i'),\n",
       " ('was', 'wa'),\n",
       " ('diagnosed', 'diagnos'),\n",
       " ('with', 'with'),\n",
       " ('cancer', 'cancer'),\n",
       " ('2', '2'),\n",
       " ('years', 'year'),\n",
       " ('ago', 'ago'),\n",
       " ('i', 'i'),\n",
       " ('will', 'will'),\n",
       " ('be', 'be'),\n",
       " ('going', 'go'),\n",
       " ('in', 'in'),\n",
       " ('for', 'for'),\n",
       " ('an', 'an'),\n",
       " ('operation', 'oper'),\n",
       " ('later', 'later'),\n",
       " ('this', 'thi'),\n",
       " ('week', 'week'),\n",
       " ('i', 'i'),\n",
       " ('decided', 'decid'),\n",
       " ('to', 'to'),\n",
       " ('will', 'will'),\n",
       " ('donate', 'donat'),\n",
       " ('the', 'the'),\n",
       " ('sum', 'sum'),\n",
       " ('of', 'of'),\n",
       " ('8', '8'),\n",
       " ('750', '750'),\n",
       " ('000', '000'),\n",
       " ('00', '00'),\n",
       " ('euros', 'euro'),\n",
       " ('eight', 'eight'),\n",
       " ('million', 'million'),\n",
       " ('seven', 'seven'),\n",
       " ('hundred', 'hundr'),\n",
       " ('and', 'and'),\n",
       " ('fifty', 'fifti'),\n",
       " ('thousand', 'thousand'),\n",
       " ('euros', 'euro'),\n",
       " ('only', 'onli'),\n",
       " ('etc', 'etc'),\n",
       " ('etc', 'etc')]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare tokens to stemmed version.\n",
    "list(zip(spam_tokens, stem_spam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('information', 'inform'),\n",
       " ('carefully', 'care'),\n",
       " ('profile', 'profil'),\n",
       " ('outstanding', 'outstand'),\n",
       " ('personality', 'person'),\n",
       " ('this', 'thi'),\n",
       " ('why', 'whi'),\n",
       " ('valery', 'valeri'),\n",
       " ('directors', 'director'),\n",
       " ('years', 'year'),\n",
       " ('was', 'wa'),\n",
       " ('diagnosed', 'diagnos'),\n",
       " ('years', 'year'),\n",
       " ('going', 'go'),\n",
       " ('operation', 'oper'),\n",
       " ('this', 'thi'),\n",
       " ('decided', 'decid'),\n",
       " ('donate', 'donat'),\n",
       " ('euros', 'euro'),\n",
       " ('hundred', 'hundr'),\n",
       " ('fifty', 'fifti'),\n",
       " ('euros', 'euro'),\n",
       " ('only', 'onli')]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print only those stemmed tokens that are different.\n",
    "\n",
    "[(spam_tokens[i], stem_spam[i]) for i in range(len(spam_tokens)) if spam_tokens[i] != stem_spam[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do this on individual words as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'comput'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stem the word \"computer\"\n",
    "p_stemmer.stem(\"computer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'comput'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stem the word \"computers\"\n",
    "p_stemmer.stem(\"computers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'comput'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stem the word \"computation\"\n",
    "p_stemmer.stem(\"computation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'comput'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stem the word \"computationally\"\n",
    "p_stemmer.stem(\"computationally\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Word Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following sentence has had stop words (and punctuation) removed:\n",
    "\n",
    "\"Answer great question life universe everything said deep thought said deep thought paused forty two said deep thought infinite majesty calm.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What book is the above sentence from?</summary>\n",
    "\n",
    "The Hitchhiker's Guide to the Galaxy!\n",
    "    \n",
    "![](./images/hgg.jpg)\n",
    "    \n",
    "The original quote reads:  \n",
    "...\"The Answer to the Great Question...\"  \n",
    "\"Yes..!\"  \n",
    "\"Of Life, the Universe and Everything...\" said Deep Thought.  \n",
    "\"Yes...!\"  \n",
    "\"Is...\" said Deep Thought, and paused.  \n",
    "\"Yes...!\"  \n",
    "\"Is...\"  \n",
    "\"Yes...!!!...?\"  \n",
    "\"Forty-two,\" said Deep Thought, with infinite majesty and calm.”\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>If you were familiar with the book, how did you know what book the sentence was from?</summary>\n",
    "\n",
    "Removing stop words did not remove key identifying words such as \"life\", \"universe\", \"everything\", and \"forty-two\".\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Based on this, how would you define stop words?</summary>\n",
    "\n",
    "Stop words are words that have little to no significance or meaning. They are common words that only add to the grammatical structure and flow of the sentence, so it is still relatively easy to identify the contents of sentences without stop words.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# Print English stopwords.\n",
    "print(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords from \"spam_tokens.\"\n",
    "no_stop_words = [token for token in spam_tokens if token not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'saw', 'contact', 'information', 'linkedin', 'carefully', 'read', 'profile', 'seem', 'outstanding', 'personality', 'one', 'major', 'reason', 'contact', 'name', 'mr', 'valery', 'grayfer', 'chairman', 'board', 'directors', 'pjsc', 'lukoil', '86', 'years', 'old', 'diagnosed', 'cancer', '2', 'years', 'ago', 'going', 'operation', 'later', 'week', 'decided', 'donate', 'sum', '8', '750', '000', '00', 'euros', 'eight', 'million', 'seven', 'hundred', 'fifty', 'thousand', 'euros', 'etc', 'etc']\n"
     ]
    }
   ],
   "source": [
    "# Check it\n",
    "print(no_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the purpose of Project 3 \n",
    "# we insist you add the obvious words in the stopword list \n",
    "\n",
    "# in other words we ask that you pre-process \n",
    "# your data to remove the name of the subreddit \n",
    "# that you have chosen \n",
    "\n",
    "# so if doing stocks and bonds \n",
    "# need to manually remove stocks and bonds \n",
    "# as well as any associated words \"stock\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Sentiment Analysis\n",
    "\n",
    "![](./images/sent.jpeg)\n",
    "\n",
    "[Sentiment analysis](https://www.kdnuggets.com/2018/08/emotion-sentiment-analysis-practitioners-guide-nlp-5.html) is an area of natural language processing in which we seek to classify text as having positive or negative emotion.\n",
    "\n",
    "Let's build a simple function that can classify text as either having positive or negative sentiment.\n",
    "\n",
    "What words tell us whether certain text is positive?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag of words model \n",
    "# we take the text\n",
    "# we emit some words \"words \" -> depending on how we tokenize and how we shorten \n",
    "# then now we count those words \n",
    "# run: 3 \n",
    "# happy: 10\n",
    "# eat: 4 \n",
    "\n",
    "# Let's come up with a list of positive and negative words we might observe.\n",
    "\n",
    "positive_words = ['delight', 'good', 'great', 'awesome', 'tremendous', 'fabulous', 'amazing', 'stellar']\n",
    "negative_words = ['garbage', 'sad', 'trash', 'ugly', 'bad', 'disgusting', 'terrible', 'gross']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-25T10:03:50.010475Z",
     "start_time": "2017-10-25T10:03:50.006364Z"
    }
   },
   "outputs": [],
   "source": [
    "def simple_sentiment(text):\n",
    "    # Instantiate tokenizer.\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')  \n",
    "    # Tokenize text.\n",
    "    tokens = tokenizer.tokenize(text.lower()) \n",
    "    # Instantiate stemmer.\n",
    "    p_stemmer = PorterStemmer()\n",
    "    # Stem words.\n",
    "    stemmed_words = [p_stemmer.stem(i) for i in tokens]\n",
    "    # Stem our positive/negative words.\n",
    "    positive_stems = [p_stemmer.stem(i) for i in positive_words]\n",
    "    negative_stems = [p_stemmer.stem(i) for i in negative_words]\n",
    "\n",
    "    # Count \"positive\" words.\n",
    "    positive_count = sum([1 for i in stemmed_words if i in positive_stems])\n",
    "    # Count \"negative\" words\n",
    "    negative_count = sum([1 for i in stemmed_words if i in negative_stems])\n",
    "    # Calculate Sentiment Percentage \n",
    "    return round((positive_count - negative_count) / len(tokens), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run our sentiment analyzer on our spam email.\n",
    "simple_sentiment(spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three not-so-random Chipotle reviews.\n",
    "\n",
    "yelp_1 = \"No Chipotle should ever have a 2 out of 5 star rating on Yelp. Especially not this one. As a regular (usually two or three visits a week), I have never been dissatisfied with a single meal here. It's Chipotle, so you know you'll pay $8 (after tax) for a chicken bowl and be full and satisfied afterwards. \\n The employees are friendly and give generous portions. Seating is limited, but there is a place you can stand and eat near the window, which is where I always eat. I'm sitting down eight hours a day at the office anyway - standing and eating here is probably extending my lifespan. \\nThe line gets line long during lunch, but it moves fast. Dinner time is amazing - rarely a line and the portions are extra generous during this time.\\n This fairly new Chipotle is at a great location, near McPherson Square. It's right next to my office and gym so it's perfect for me. \\nBottom line: if you're craving Chipotle and are worried about the other reviews and low ratings for this location, don't be. It's my favorite Chipotle location in the DC area, and that's not an exaggeration.\"\n",
    "\n",
    "yelp_2 = \"DISGUSTING LONG HAIR THREADED THROUGH CHICKEN IN BURRITO BOWL.\\n There was a long blonde hair threaded through my chicken as I was eating a burrito bowl.  I did not notice until it was too late and the HAIR ENTERED MY MOUTH as I was eating and I grossly pulled the hair out.\\n I calmly walked up to the register to inform them that there was hair in my food. The register person was busy, I understand that, but I was promptly ignored like my issue was not a big deal.  He proceeded to get his manager, 'Leslie' I believe.  She was not apologetic at all and offered no condolences. She did however offer a refund, but I didn't care about the money, I just wanted to eat food without eating someone's hair as a side dish.\\n The second time I went back up, a different person, the general manager Peris, was more apologetic and handled the situation better. He ended up getting Leslie to file a report, but who knows if they submitted it or not.\\n Suffice to say, if you dont want food in your hair, dont eat here.\"\n",
    "\n",
    "yelp_3 = \"First time going to this Chipotle.  The line was very quick and the food was fresh.  But as I started eating a notice that the food was very salty.  I started separating my bowl after two bites.  I ordered a bowl with white rice, black beans, chicken, sour cream, cheese and lettuce.  I tasted everything separately.  Once I tasted the Chicken by it self it was unbearable.  It taste like someone pouring the entire bottle of salt on tge chicken.  I tried to take most the chicken out the bowl but still I could not bear the taste of the salt.  So I ended up throwing the damn bowl away.  $8.00 down the drain.  SMH.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"No Chipotle should ever have a 2 out of 5 star rating on Yelp. Especially not this one. As a regular (usually two or three visits a week), I have never been dissatisfied with a single meal here. It's Chipotle, so you know you'll pay $8 (after tax) for a chicken bowl and be full and satisfied afterwards. \\n The employees are friendly and give generous portions. Seating is limited, but there is a place you can stand and eat near the window, which is where I always eat. I'm sitting down eight hours a day at the office anyway - standing and eating here is probably extending my lifespan. \\nThe line gets line long during lunch, but it moves fast. Dinner time is amazing - rarely a line and the portions are extra generous during this time.\\n This fairly new Chipotle is at a great location, near McPherson Square. It's right next to my office and gym so it's perfect for me. \\nBottom line: if you're craving Chipotle and are worried about the other reviews and low ratings for this location, don't be. It's my favorite Chipotle location in the DC area, and that's not an exaggeration.\""
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"DISGUSTING LONG HAIR THREADED THROUGH CHICKEN IN BURRITO BOWL.\\n There was a long blonde hair threaded through my chicken as I was eating a burrito bowl.  I did not notice until it was too late and the HAIR ENTERED MY MOUTH as I was eating and I grossly pulled the hair out.\\n I calmly walked up to the register to inform them that there was hair in my food. The register person was busy, I understand that, but I was promptly ignored like my issue was not a big deal.  He proceeded to get his manager, 'Leslie' I believe.  She was not apologetic at all and offered no condolences. She did however offer a refund, but I didn't care about the money, I just wanted to eat food without eating someone's hair as a side dish.\\n The second time I went back up, a different person, the general manager Peris, was more apologetic and handled the situation better. He ended up getting Leslie to file a report, but who knows if they submitted it or not.\\n Suffice to say, if you dont want food in your hair, dont eat here.\""
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First time going to this Chipotle.  The line was very quick and the food was fresh.  But as I started eating a notice that the food was very salty.  I started separating my bowl after two bites.  I ordered a bowl with white rice, black beans, chicken, sour cream, cheese and lettuce.  I tasted everything separately.  Once I tasted the Chicken by it self it was unbearable.  It taste like someone pouring the entire bottle of salt on tge chicken.  I tried to take most the chicken out the bowl but still I could not bear the taste of the salt.  So I ended up throwing the damn bowl away.  $8.00 down the drain.  SMH.'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate sentiment of yelp_1.\n",
    "simple_sentiment(yelp_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.01"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate sentiment of yelp_2.\n",
    "simple_sentiment(yelp_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate sentiment of yelp_3.\n",
    "simple_sentiment(yelp_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary> What are some shortcomings of this method? </summary>\n",
    "\n",
    "- Primarily, we're limited to the positive/negative words we came up with.\n",
    "- If someone wrote \"not good\" or \"not bad,\" our sentiment function would probably treat \"not good\" as positive or neutral... but it's probably supposed to mean negative!\n",
    "- The ordering of the words doesn't matter here, which is not how language generally works.\n",
    "- We haven't corrected for misspellings.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple of ways to proceed with sentiment analysis:\n",
    "\n",
    "1. If you have already-labeled data, you can build a supervised learning model.\n",
    "2. If you don't have labeled data, you can use a Lexicon that has already been built/trained for sentiment analysis.\n",
    "    - There are a bunch of these and which to use depends on your purpose/data. Here are just a few that are available:\n",
    "        - AFINN lexicon\n",
    "        - MPQA subjectivity lexicon\n",
    "        - SentiWordNet\n",
    "        - VADER lexicon\n",
    "\n",
    "We will use the VADER (Valence Aware Dictionary and sEntiment Reasoner) lexicon to analyze the sentiments of our reviews!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Sentiment Intensity Analyzer\n",
    "sent = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.046, 'neu': 0.799, 'pos': 0.155, 'compound': 0.9795}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate sentiment of yelp_1.\n",
    "sent.polarity_scores(yelp_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.078, 'neu': 0.875, 'pos': 0.047, 'compound': -0.5847}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate sentiment of yelp_2.\n",
    "sent.polarity_scores(yelp_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.067, 'neu': 0.89, 'pos': 0.043, 'compound': -0.5718}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate sentiment of yelp_3.\n",
    "sent.polarity_scores(yelp_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try analyzing the sentiment of IMDb movie reviews. The data is from [Kaggle](https://www.kaggle.com/c/word2vec-nlp-tutorial#part-1-for-beginners-bag-of-words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-24T15:40:07.099103Z",
     "start_time": "2017-10-24T15:40:05.659674Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read in training data.\n",
    "reviews = pd.read_csv(\"./data/labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-24T15:40:07.680295Z",
     "start_time": "2017-10-24T15:40:07.659485Z"
    }
   },
   "outputs": [],
   "source": [
    "# View the first five rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine a review.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment of the review.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does this match the sentiment given in the training data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interview Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>When processing text, what can you do about frequently occurring words, like \"a,\" \"and,\" \"the,\" etc.?</summary>\n",
    "\n",
    "- These words, called \"stopwords,\" can either be kept or removed.\n",
    "    - If we think these words do help explain our $Y$ variable, we might keep them. (For example, if we're classifying the era of a poem, the frequency of the word \"the\" may be helpful information!)\n",
    "    - If we think these words don't help explain our $Y$ variable, we might remove them. (For example, in sentiment analysis, we might not think that people who use \"the\" more or less frequently are happier or angrier.)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple things to note:\n",
    "1. NLP broadly describes: \n",
    "    - how we can get unstructured text data into a more structured form that can be interpreted by computers, and \n",
    "    - algorithms for interpreting text data.\n",
    "2. That does not mean these tools we used today work to the exclusion of other methods. You can and should include other variables in your model!\n",
    "    - For example, maybe the length of a review tells us something about how much people liked/disliked the movie, or maybe additional information about the reviewer (i.e. geography, age, how many reviews they had submitted) has predictive value."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 226,
   "position": {
    "height": "40px",
    "left": "1070px",
    "right": "20px",
    "top": "120px",
    "width": "333px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
