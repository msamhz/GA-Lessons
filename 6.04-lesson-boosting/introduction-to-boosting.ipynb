{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Introduction to Boosting\n",
    "\n",
    "_Authors: Kiefer Katovich (SF), Matt Brems (DC)_\n",
    "\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "- Understand the differences between bagging and boosting.\n",
    "\n",
    "`Bagging is essentially bootstrapping aggregating. We take a random resampling with replacement of data of same size and create models, aggregating it to give a better prediction`\n",
    "\n",
    "`Boosting helps to train the model based on misclassified samples by adding weights to the weaker base learners` \n",
    "\n",
    "- Understand how boosting is an ensemble method.\n",
    "\n",
    "`**Base model fitting is an iterative procedure**: It cannot be run in parallel.`\n",
    "\n",
    "`**Weights are assigned to observations to indicate their \"importance:\"** Samples with higher weights are given higher influence on the total error of the next model, prioritizing those observations.`\n",
    "\n",
    "`**Weights change at each iteration with the goal of correcting the errors/misclassifications of the previous iteration**: The first base estimator is fit with uniform weights on the observations.`\n",
    "\n",
    "`**Final prediction is typically constructed by a weighted vote**: Weights for each base model depend on their training errors or misclassification rates.`\n",
    "\n",
    "- Learn the pros and cons to using boosting models.\n",
    "\n",
    "`Higher performance but very time consuming to tune hyperparameters` \n",
    "\n",
    "- Learn the effect of boosting on the bias-variance trade-off.\n",
    "\n",
    "`To reduce bias as essentially weak learners have low variance and high bias`\n",
    "\n",
    "- Learn the math and procedure for AdaBoost, the \"classic\" boosting model.\n",
    "\n",
    "- Understand the differences between AdaBoost and gradient-boosting models.\n",
    "\n",
    "`AdaBoost is about reweighting the preceding model's errors in subsequent iterations.`\n",
    "`Gradient boosting is about fitting subsequent models to the residuals of the last model.`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lesson Guide\n",
    "\n",
    "- [Boosting as an Ensemble Method](#intro)\n",
    "- [Pros and Cons of Boosting](#pros-cons)\n",
    "- [A Visual Description of Bagging vs. Boosting](#viz)\n",
    "- [Boosting and the Bias-Variance Trade-Off](#bias-variance)\n",
    "- [AdaBoost](#adaboost)\n",
    "    - [Training Example: Weights](#ex-weights)\n",
    "- [AdaBoost Visualization](#adaboost-viz)\n",
    "- [Gradient Boosting Models](#gradient)\n",
    "- [Gradient Boosting Visualization](#gboost-viz)\n",
    "- [The difference Between AdaBoost and Gradient Boosting](#the-difference-between-the-adaboost-and-gradient-boosting)\n",
    "- [Additional Resources](#resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='intro'></a>\n",
    "\n",
    "## Boosting as an Ensemble Method\n",
    "\n",
    "---\n",
    "\n",
    "Boosting is another ensemble method with a different approach to bagging. Boosting takes a weak base learner and tries to make it a strong learner by retraining it on the misclassified samples.\n",
    "\n",
    "1) **Base model fitting is an iterative procedure**: It cannot be run in parallel.\n",
    "- **Weights are assigned to observations to indicate their \"importance:\"** Samples with higher weights are given higher influence on the total error of the next model, prioritizing those observations.\n",
    "- **Weights change at each iteration with the goal of correcting the errors/misclassifications of the previous iteration**: The first base estimator is fit with uniform weights on the observations.\n",
    "- **Final prediction is typically constructed by a weighted vote**: Weights for each base model depend on their training errors or misclassification rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='pros-cons'></a>\n",
    "## Pros and Cons of Boosting\n",
    "\n",
    "---\n",
    "\n",
    "### Pros\n",
    "\n",
    "- Achieves higher performance than bagging when the hyperparameters are properly tuned.\n",
    "- Works equally well for classification and regression.\n",
    "- Can use \"robust\" loss functions that make the model resistant to outliers.\n",
    "\n",
    "---\n",
    "\n",
    "### Cons\n",
    "\n",
    "- Difficult and time consuming to properly tune hyperparameters.\n",
    "- Cannot be parallelized like bagging (bad scalability when there are huge amounts of data).\n",
    "- Higher risk of overfitting compared to bagging.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='viz'></a>\n",
    "![boostvsbag](./images/BoostingVSBagging.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='bias-variance'></a>\n",
    "## Boosting and the Bias-Variance Trade-Off\n",
    "\n",
    "---\n",
    "\n",
    "Recall that **bagging aims to reduce variance**.\n",
    "\n",
    "**Boosting aims to reduce bias** (and can reduce variance a bit as well)!\n",
    "\n",
    "### Why?\n",
    "\n",
    "The rationale/theory behind boosting is to combine **many weak learners into a single strong learner.**\n",
    "\n",
    "Instead of using deep/full decision trees like in bagging, **boosting uses shallow/high-bias base estimators.**\n",
    "\n",
    "Thus, each weak learner has:\n",
    "\n",
    "- Low variance.\n",
    "- High bias.\n",
    "\n",
    "It uses iterative fitting to explain error/misclassification unexplained by the previous base models and reduces bias without increasing variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTES\n",
    "\n",
    "Our goal is to build B trees \n",
    "\n",
    "at each stage we use full data \n",
    "\n",
    "we begin by just weighing each row equally\n",
    "then we fit a tree \n",
    "after we compute the accuracy of the tree - acc \n",
    "\n",
    "we also will identify rows that were misclassified \n",
    "\n",
    "\n",
    "update the weights of according to the rows that \n",
    "we dibt snoky taje a vote \n",
    "we must take weighted vote accoding to how trustworthy each tree was \n",
    "\n",
    "acc 0.9 \n",
    "acc 0.73 different tree\n",
    "...\n",
    "\n",
    "up to B trees ->> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='adaboost'></a>\n",
    "## AdaBoost\n",
    "\n",
    "---\n",
    "\n",
    "AdaBoost is the original boosting algorithm. Predictions from AdaBoost follow this formula:\n",
    "\n",
    "\n",
    "### $$ AdaBoost(X) = sign\\left(\\sum_{t=1}^T\\alpha_t h_t(X)\\right) $$\n",
    "\n",
    "Where:\n",
    "\n",
    "$AdaBoost(X)$ is the classification predictions for $y$ using predictor matrix $X$.\n",
    "\n",
    "$T$ is the set of \"weak learners.‚Äù\n",
    "\n",
    "$\\alpha_t$ is the contribution weight for weak learner $t$.\n",
    "\n",
    "$h_t(X)$ is the prediction of weak learner $t$.\n",
    "\n",
    "$y$ is binary **with values of negative one and one.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The core principle of AdaBoost is to **fit a sequence of weak learners** (i.e., models that are only slightly better than random guessing, such as a single-split tree) **on repeatedly modified versions of the data**. After each fit, the importance weights on each observation need to be updated. \n",
    "\n",
    "The predictions are then combined through a weighted majority vote (or sum) to produce the final prediction. AdaBoost, like all boosting ensemble methods, focuses the next model's fit on the misclassifications/weaknesses of the prior models.\n",
    "\n",
    "All training examples start with equal importance weighting. When we finish training a classifier, we update the importance weighting of the classifier itself, represented by alpha $\\alpha$:\n",
    "\n",
    "### $$ \\alpha_t = \\frac{1}{2}ln \\left(\\frac{1-\\epsilon_t}{\\epsilon_t}\\right) \\text{where } \\epsilon_t < 1$$\n",
    "\n",
    "Where $\\epsilon_t$ is the misclassification rate for the current classifier:\n",
    "\n",
    "### $$ \\epsilon_t = \\frac{\\text{misclassifications}_t}{\\text{observations}_t} $$\n",
    "\n",
    "As iterations continue, **examples that are difficult to predict receive ever-increasing influence**. Each subsequent weak learner is thereby forced to concentrate on the examples that are missed by the previous ones in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='adaboost-viz'></a>\n",
    "## AdaBoost Visualization\n",
    "\n",
    "![boostvsbag](./images/adaboost-viz.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_alpha(acc):\n",
    "    error = 1 - acc \n",
    "    return 0.5 * np.log((1-error)/error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2027325540540821"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_alpha(0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "more accurate more weight "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='ex-weights'></a>\n",
    "### Training Example Weights\n",
    "\n",
    "AdaBoost sets up a weight vector on the observations, denoted as $D_t$ where $t$ is the current model iteration. $D_t$ is a probability distribution that determines how likely it is that a given observation will be selected as part of the training set for the current estimator.\n",
    "\n",
    "The $\\alpha$ weighting of the last-fit estimator is used in the equation for the weighting distribution. The update equation is:\n",
    "\n",
    "### $$ D_{t+1}(i) = D_t(i) \\cdot e^{-\\alpha_t \\cdot y_i \\cdot h_t(x_i)} = D_t(i) \\cdot (e^{\\alpha_t})^{-y_i \\cdot h_t(x_i)}.$$\n",
    "\n",
    "- $D_t$ is the vector of observation indices, $D_t(i)$ is the ith datapoint in $D_t$,\n",
    "- $x_i$ is the observation at the index, $y_i$ is the target, and\n",
    "- $h_t$ is the previous model fit in the boosting chain.\n",
    "\n",
    "> Note that $e^{\\alpha_t}$ is always positive, and $-y_i h_t(x_i)$ is either -1 (if the sample is correctly classified) or +1 (if the sample is incorrectly classified).\n",
    "\n",
    "Lastly, we'll divide the weights by the sum of weights to normalize them, ensuring that they sum to one and form a probability distribution:\n",
    "\n",
    "### $$ D_{t+1}(i) = \\frac{D_{t+1}(i)}{\\sum_{i=1}^N D_{t+1}(i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-yht(x)  will give prediction of the Tth tree\n",
    "\n",
    "what happens if yI and prediction is the same --> + * + become + but with the -yhx becomes -1  (hence will help to reduce the weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we compute D_t and D_t+1 \n",
    "# these are both vectors of weights \n",
    "# that vector has length equivalent to the number \n",
    "# of rows in our dataset\n",
    "\n",
    "# learning rate -> if higher learning rate, actual D_(t+1) that we use,\n",
    "# will be closer to D(t+1)\n",
    "\n",
    "#if the learning rate is lower \n",
    "# then the actual D(t+1) will be clsoer to D_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='gradient'></a>\n",
    "## Gradient Boosting Models\n",
    "\n",
    "---\n",
    "\n",
    "Gradient boosting classifiers are a generalization of boosting to arbitrary, differentiable loss functions. The intuition behind this mechanism is to:\n",
    "\n",
    "1. Fit a model $F$ to the data.\n",
    "2. Look at the difference between our observed $y$ and our model $F$. (The $y_i - F(x_i)$ can be thought of as residuals!)\n",
    "3. Fit a second model, $F_2$, to (roughly) the residuals $y_i - F(x_i)$.\n",
    "4. Aggregate your model $F$ and $F_2$. While we won't get into the details now, we can interpret residuals as negative gradients. By doing this, we can apply our gradient descent algorithm to optimize our loss and generalize this to many loss functions.\n",
    "\n",
    "GBRT is an accurate and effective off-the-shelf procedure that can be used for both regression and classification problems. Gradient tree boosting models are used in a variety of areas, including web search ranking and ecology.\n",
    "\n",
    "[This presentation](http://www.ccs.neu.edu/home/vip/teach/MLcourse/4_boosting/slides/gradient_boosting.pdf) shows a visual example of AdaBoost at work and how AdaBoost laid the groundwork for gradient boosting classifiers.\n",
    "\n",
    "**The advantages of GBRT are:**\n",
    "\n",
    "- Natural handling of mixed data types (= heterogeneous features).\n",
    "- Predictive power.\n",
    "- Robustness to outliers in output space (via robust loss functions).\n",
    "\n",
    "**The disadvantages of GBRT are:**\n",
    "- Scalability: Due to the sequential nature of boosting, it can hardly be parallelized.\n",
    "- Difficult hyperparameters to tune.\n",
    "\n",
    "\n",
    "> _For more detailed explanations, see [here](https://www.quora.com/What-is-an-intuitive-explanation-of-Gradient-Boosting) and [here](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3885826/)._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='gboost-viz'></a>\n",
    "## Gradient Boosting Visualization\n",
    "![boostvsbag](./images/gboost-viz.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='the-difference-between-the-adaboost-and-gradient-boosting'></a>\n",
    "## The Difference Between the AdaBoost and Gradient Boosting\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "- AdaBoost is about reweighting the preceding model's errors in subsequent iterations.\n",
    "- Gradient boosting is about fitting subsequent models to the residuals of the last model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground\n",
    "\n",
    "---\n",
    "\n",
    "Let's practice boosting in `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's next?\n",
    "\n",
    "---\n",
    "\n",
    "There is no lab today!  You can choose your own adventure instead.  Below are some options for how you may want to spend your time.\n",
    "\n",
    "- Practice boosting on a your own dataset.\n",
    "- Take a deep-dive into how AdaBoost works by implementing it from scratch in the `adaboost-from-scratch.ipynb` notebook. (Solutions provided)\n",
    "- `XGBoost` is a popular boosting package, seperate from `sklearn`.  (Many Kaggle competitions have been won with XGBoost.)  Try installing it and using it on a sample dataset (maybe the diabetes data included with this repo.) [http://xgboost.readthedocs.io/en/latest/](http://xgboost.readthedocs.io/en/latest/)\n",
    "- Work on project 3\n",
    "- Catch up on labs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='resources'></a>\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "---\n",
    "\n",
    "- [Random Forest on Wikipedia](https://en.wikipedia.org/wiki/Random_forest)\n",
    "- [Quora Question on Random Forests](https://www.quora.com/How-does-randomization-in-a-random-forest-work?redirected_qid=212859)\n",
    "- [Scikit-Learn Ensemble Methods](http://scikit-learn.org/stable/modules/ensemble.html)\n",
    "- [Scikit-Learn Random Forest Classifiers](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "- [Academic Introduction to Adaptive Boosting](http://rob.schapire.net/papers/explaining-adaboost.pdf)\n",
    "- [Stack Exchange AdaBoost vs. Gradient Boosting](http://stats.stackexchange.com/questions/164233/intuitive-explanations-of-differences-between-gradient-boosting-trees-gbm-ad)\n",
    "- [A Gentle Introduction to Gradient Boosting](http://www.ccs.neu.edu/home/vip/teach/MLcourse/4_boosting/slides/gradient_boosting.pdf)\n",
    "- [Quora on Intuitive Explanations of AdaBoost](https://www.quora.com/What-is-AdaBoost)\n",
    "- A Lighter [Math Introduction](http://www.ccs.neu.edu/home/vip/teach/MLcourse/4_boosting/slides/gradient_boosting.pdf) to AdaBoosting and Gradient Boosting\n",
    "- A [Walk Through](https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/) on Tuning Gradient Boosting Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
