{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "- [Data Importing](#Data-Importing)\n",
    "- [Classification metrics](#Classification-metrics)\n",
    "- [Vectorizer Selection via GridSeachCV](#Vectorizer-Selection-via-GridSeachCV)\n",
    "- [Hyper-parameter tuning](#Hyper-parameter-tuning)\n",
    "- [Error Analysis](#Error-Analysis)\n",
    "- [Redefine Data set and run models](#Redefine-Data-set-and-run-models)\n",
    "- [Production Model Selection](#Production-Model-Selection)\n",
    "- [Conclusion and Recommendation](#Conclusion-and-Recommendation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import string\n",
    "string.punctuation\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import contractions\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, accuracy_score, \\\n",
    "    plot_roc_curve, roc_auc_score, recall_score, precision_score, f1_score\n",
    "from transformers import pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk import word_tokenize   \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "import matplotlib.dates as mdate\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "\n",
    "from sklearn.model_selection import(\n",
    "    cross_val_score,\n",
    "    train_test_split,\n",
    "    GridSearchCV\n",
    ")\n",
    "\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler,\n",
    "    PolynomialFeatures\n",
    ")\n",
    "from datetime import timezone\n",
    "import datetime\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, VotingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Data/df.csv')\n",
    "df.drop(columns = 'Unnamed: 0', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for Null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "author                   0\n",
       "subreddit                0\n",
       "selftext                 0\n",
       "title                    0\n",
       "created_utc              0\n",
       "datetime                 0\n",
       "link_flair_css_class     0\n",
       "alltext                  0\n",
       "length_text              0\n",
       "wrdcount_text            0\n",
       "month                    0\n",
       "day                      0\n",
       "clean_text               1\n",
       "ttl_post                 0\n",
       "user_contribute_where    0\n",
       "stem_clean_text          1\n",
       "lemmi_clean_text         1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Model Prep: Create `X` and `y` variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our features will be:\n",
    "- `subreddit`\n",
    "- `stem_clean_text`\n",
    "- `lemmi_clean_text`\n",
    "\n",
    "For our baseline modelling will be using lemmi_clean_text\n",
    "\n",
    "Our target will be `subreddit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['lemmi_clean_text']\n",
    "y = df['subreddit']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible classification metrics that can be used how well my model perform are *Accuracy, Misclassification Rate, Sensitivity (True Positive Rate), Specificity (True Negative Rate), Precision (Positive Predictive Value), F1 Score and ROC AUC*.\n",
    "\n",
    "For this project I will be mainly using *Accuracy* accompanied with confusion matrix to determine effectiveness of my models\n",
    "\n",
    "For further evaluations, I will be using Specificity to choose the best model that helps gauge on how best to minimize this in order to reduce missing out identifying users potentionally having PTSD.\n",
    "\n",
    "I will also use ROC to gave a gauge the degree of overlap between the words in post from the two subreddits. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Accuracy Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total post: 20228\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    0.500395\n",
       "0    0.499605\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Total post: {len(df)}')\n",
    "df['subreddit'].value_counts(normalize= True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 10000 post added into each class, the classes are very well balanced from both sides \n",
    "\n",
    "Also, from the above, we can see the ***baseline accuracy*** is at 50.1%. \n",
    "If nothing ws done for the classification model, and just to assign every post to the PTSD class, i would classify 50.1% of the post correctly. \n",
    "\n",
    "Therefore, any classification model designed for this data must have an accuracy higher than the ***baseline accuracy*** of 50.1%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizer Selection via GridSeachCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider two NLP vectorizers and iterate over parameters below with Logistic regression model first.\n",
    "- *Count Vectorizer*\n",
    "- *TF-IDF Vectorizer*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embed Stopword list into vectorizers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create stop word list \n",
    "stopwordlist = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# to input in subreddit headers so that we can exclude it for model\n",
    "headers = ['ptsd', 'anxiety','anxious', \"cptsd\", 'trauma','traumatized','traumas',\"posttraumatic\",'stress','disorder',\"traumatic\", 'www', 'https', 'mentalgrenade', 'com']\n",
    "for head in headers:\n",
    "    stopwordlist.append(head)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Pipeline with:\n",
    "# Estimator: LogisticRegression (both)\n",
    "# Transformer: CountVectorizer - pipe1, TfidfVectorizer - pipe2\n",
    "\n",
    "pipe1 = Pipeline([('tvec', TfidfVectorizer(stop_words= stopwordlist)),\n",
    "                  ('gboost', GradientBoostingClassifier())\n",
    "                 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ccp_alpha',\n",
       " 'criterion',\n",
       " 'init',\n",
       " 'learning_rate',\n",
       " 'loss',\n",
       " 'max_depth',\n",
       " 'max_features',\n",
       " 'max_leaf_nodes',\n",
       " 'min_impurity_decrease',\n",
       " 'min_samples_leaf',\n",
       " 'min_samples_split',\n",
       " 'min_weight_fraction_leaf',\n",
       " 'n_estimators',\n",
       " 'n_iter_no_change',\n",
       " 'random_state',\n",
       " 'subsample',\n",
       " 'tol',\n",
       " 'validation_fraction',\n",
       " 'verbose',\n",
       " 'warm_start']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GradientBoostingClassifier()._get_param_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['memory', 'steps', 'verbose']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe1._get_param_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe1_params = {'tvec__max_features': [1000],\n",
    "                'tvec__min_df': [0.02,0.01],\n",
    "                'tvec__max_df': [0.7,0.8],\n",
    "                'tvec__ngram_range': [(1,1)],\n",
    "                'gboost__max_depth': [4,5,6],\n",
    "                'gboost__n_estimators': [150,200],\n",
    "                'gboost__learning_rate': [.12,.15,.2]\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating two separate GridSearchCV objects for:\n",
    "# CountVectorizer and TfidfVectorizer\n",
    "\n",
    "gs_pipe1 = GridSearchCV(pipe1, param_grid=pipe1_params, cv=5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting GridSearchCV with CountVectorizer transformer on X_train and y_train.\n",
    "\n",
    "gs_pipe1.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gboost__learning_rate': 0.12,\n",
       " 'gboost__max_depth': 4,\n",
       " 'gboost__n_estimators': 150,\n",
       " 'tvec__max_df': 0.7,\n",
       " 'tvec__max_features': 1000,\n",
       " 'tvec__min_df': 0.02,\n",
       " 'tvec__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_pipe1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7952665398954626"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_pipe1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7929602531144948"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_pipe1.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8679058730472612"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_pipe1.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "592c428e42f6e4511884ace7c9b495d167f60b904bd88ca4683897c5c1f20061"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('myenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
