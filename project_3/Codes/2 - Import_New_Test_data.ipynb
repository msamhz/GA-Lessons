{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0758aac",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0758aac",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86e229a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import string\n",
    "string.punctuation\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "import numpy as np\n",
    "from datetime import timezone\n",
    "import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "import string\n",
    "string.punctuation\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import contractions\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from nltk import word_tokenize   \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "import matplotlib.dates as mdate\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler,\n",
    "    PolynomialFeatures\n",
    ")\n",
    "from datetime import timezone\n",
    "import datetime\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e077b6",
   "metadata": {},
   "source": [
    "## Create Function to retrieve dataset from reddit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5799751a",
   "metadata": {},
   "source": [
    "Start from 1st April 2021 and later. Take from PTSD subreddit --> time_from = 1617235200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb685143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function that takes in search name and limit for search and time\n",
    "\n",
    "def get_data(title_name, data_min, column, time_from = 1617235200):\n",
    "    # initiate date to search\n",
    "    date = time_from\n",
    "\n",
    "    #instatiate main df for PTSD\n",
    "    df = pd.DataFrame(columns = column)\n",
    "\n",
    "\n",
    "    while df.shape[0]<data_min:\n",
    "        url = 'https://api.pushshift.io/reddit/search/submission'\n",
    "        params ={\n",
    "            'subreddit': title_name,\n",
    "            'before': date,\n",
    "            'size': 70\n",
    "        }\n",
    "        # create temporary df \n",
    "        data_temp = pd.DataFrame(requests.get(url,params).json()['data'])[column]\n",
    "\n",
    "        # create list for index with [removed], '', [deleted] and NAN values\n",
    "        index_df = list(\n",
    "            data_temp[\n",
    "                (data_temp['selftext'] == '[removed]')\n",
    "                | (data_temp['selftext'] == '')\n",
    "                | (data_temp['selftext'] == '[deleted]')\n",
    "                | (data_temp['selftext'].isnull())\n",
    "                \n",
    "            ].index)\n",
    "\n",
    "        # drop down all in index list \n",
    "        data_temp = data_temp.drop(index = index_df)\n",
    "        data_temp.drop_duplicates(subset=['selftext', 'title'], keep='last', inplace=True)\n",
    "        # concat with main df\n",
    "        df = pd.concat([df,data_temp])\n",
    "\n",
    "        # print to check on shape \n",
    "        print(df.shape)\n",
    "\n",
    "        # set up new date for searching after \n",
    "        date = df.iloc[-1,-1]\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc7861a",
   "metadata": {},
   "source": [
    "To draw out CPTSD subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7a21d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54, 5)\n",
      "(114, 5)\n",
      "(174, 5)\n",
      "(233, 5)\n",
      "(300, 5)\n",
      "(360, 5)\n",
      "(421, 5)\n",
      "(483, 5)\n",
      "(550, 5)\n",
      "(616, 5)\n",
      "(684, 5)\n",
      "(746, 5)\n",
      "(806, 5)\n",
      "(871, 5)\n",
      "(938, 5)\n",
      "(1007, 5)\n",
      "(1072, 5)\n",
      "(1141, 5)\n",
      "(1211, 5)\n",
      "(1273, 5)\n",
      "(1341, 5)\n",
      "(1411, 5)\n",
      "(1479, 5)\n",
      "(1547, 5)\n",
      "(1615, 5)\n",
      "(1682, 5)\n",
      "(1742, 5)\n",
      "(1794, 5)\n",
      "(1852, 5)\n",
      "(1911, 5)\n",
      "(1959, 5)\n",
      "(2028, 5)\n",
      "(2084, 5)\n",
      "(2140, 5)\n",
      "(2202, 5)\n",
      "(2260, 5)\n",
      "(2309, 5)\n",
      "(2369, 5)\n",
      "(2428, 5)\n",
      "(2490, 5)\n",
      "(2543, 5)\n",
      "(2595, 5)\n",
      "(2658, 5)\n",
      "(2725, 5)\n",
      "(2795, 5)\n",
      "(2863, 5)\n",
      "(2933, 5)\n",
      "(2999, 5)\n",
      "(3065, 5)\n",
      "(3133, 5)\n",
      "(3200, 5)\n",
      "(3267, 5)\n",
      "(3333, 5)\n",
      "(3401, 5)\n",
      "(3464, 5)\n",
      "(3530, 5)\n",
      "(3595, 5)\n",
      "(3660, 5)\n",
      "(3728, 5)\n",
      "(3794, 5)\n",
      "(3860, 5)\n",
      "(3930, 5)\n",
      "(3993, 5)\n",
      "(4062, 5)\n",
      "(4128, 5)\n",
      "(4192, 5)\n",
      "(4259, 5)\n",
      "(4323, 5)\n",
      "(4387, 5)\n",
      "(4455, 5)\n",
      "(4508, 5)\n",
      "(4569, 5)\n",
      "(4627, 5)\n",
      "(4678, 5)\n",
      "(4733, 5)\n",
      "(4792, 5)\n",
      "(4843, 5)\n",
      "(4895, 5)\n",
      "(4953, 5)\n",
      "(5010, 5)\n",
      "(5072, 5)\n",
      "(5139, 5)\n",
      "(5208, 5)\n",
      "(5272, 5)\n",
      "(5336, 5)\n",
      "(5404, 5)\n",
      "(5470, 5)\n",
      "(5536, 5)\n",
      "(5602, 5)\n",
      "(5669, 5)\n",
      "(5738, 5)\n",
      "(5802, 5)\n",
      "(5867, 5)\n",
      "(5933, 5)\n",
      "(6000, 5)\n",
      "(6067, 5)\n",
      "(6135, 5)\n",
      "(6202, 5)\n",
      "(6270, 5)\n",
      "(6336, 5)\n",
      "(6404, 5)\n",
      "(6469, 5)\n",
      "(6539, 5)\n",
      "(6607, 5)\n",
      "(6670, 5)\n",
      "(6739, 5)\n",
      "(6807, 5)\n",
      "(6874, 5)\n",
      "(6944, 5)\n",
      "(7012, 5)\n",
      "(7077, 5)\n",
      "(7140, 5)\n",
      "(7209, 5)\n",
      "(7277, 5)\n",
      "(7345, 5)\n",
      "(7412, 5)\n",
      "(7480, 5)\n",
      "(7548, 5)\n",
      "(7616, 5)\n",
      "(7685, 5)\n",
      "(7750, 5)\n",
      "(7815, 5)\n",
      "(7881, 5)\n",
      "(7950, 5)\n",
      "(8017, 5)\n",
      "(8086, 5)\n",
      "(8154, 5)\n",
      "(8217, 5)\n",
      "(8281, 5)\n",
      "(8347, 5)\n",
      "(8413, 5)\n",
      "(8482, 5)\n",
      "(8549, 5)\n",
      "(8616, 5)\n",
      "(8686, 5)\n",
      "(8752, 5)\n",
      "(8819, 5)\n",
      "(8884, 5)\n",
      "(8949, 5)\n",
      "(9016, 5)\n",
      "(9078, 5)\n",
      "(9146, 5)\n",
      "(9214, 5)\n",
      "(9280, 5)\n",
      "(9350, 5)\n",
      "(9414, 5)\n",
      "(9482, 5)\n",
      "(9552, 5)\n",
      "(9617, 5)\n",
      "(9683, 5)\n",
      "(9749, 5)\n",
      "(9815, 5)\n",
      "(9884, 5)\n",
      "(9950, 5)\n",
      "(10020, 5)\n"
     ]
    }
   ],
   "source": [
    "col = ['author', 'subreddit', 'selftext', 'title', 'created_utc']\n",
    "\n",
    "# creating df for PTSD\n",
    "df_ptsd = get_data('CPTSD', 10000, col)\n",
    "\n",
    "# Create another column for datetime \n",
    "df_ptsd['datetime'] = df_ptsd['created_utc'].apply(lambda x: pd.to_datetime(x, unit='s'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed807f4",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627dba33",
   "metadata": {},
   "source": [
    "Combining selftext and title into alltext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7700f0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create all text column for self text and title combined \n",
    "\n",
    "df_ptsd['alltext'] = df_ptsd['selftext'] + ' ' + df_ptsd['title']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37350505",
   "metadata": {},
   "source": [
    "Calculate length of text and title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48135dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to create calculate length of text and title \n",
    "\n",
    "df_ptsd['length_text'] = df_ptsd['alltext'].map(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac78a02b",
   "metadata": {},
   "source": [
    "Word count per post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eac90625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to create calculate status word cout of text and title \n",
    "tokenizer = RegexpTokenizer('\\s+', gaps=True)\n",
    "\n",
    "def tokenizer_func(text):\n",
    "    return len(tokenizer.tokenize(text))\n",
    "\n",
    "df_ptsd['wrdcount_text'] = df_ptsd['alltext'].apply(lambda x: tokenizer_func(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6092ff07",
   "metadata": {},
   "source": [
    "Check for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4ac4dfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "author           0\n",
       "subreddit        0\n",
       "selftext         0\n",
       "title            0\n",
       "created_utc      0\n",
       "datetime         0\n",
       "alltext          0\n",
       "length_text      0\n",
       "wrdcount_text    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# found NAN coming from the link filter\n",
    "df_ptsd.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbca4d0a",
   "metadata": {},
   "source": [
    "I will now **create the target variable** to be used in classification modelling. From this point forward in all classification models, the **positive class (1) will be *r/PTSD*** and the **negative class (0) will be *r/Anxiety***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4fbdbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change Ptsd to 1 and Anxiety to 0\n",
    "\n",
    "df_ptsd['subreddit'] = df_ptsd['subreddit'].map(lambda x: 1 if x == 'ptsd' else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23faac2b",
   "metadata": {},
   "source": [
    "Create datetime object into another column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02d7779e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting 'created_utc' column from epoch time to datetime object.\n",
    "\n",
    "df_ptsd['datetime'] = df_ptsd['created_utc'].map(lambda ts: datetime.datetime.fromtimestamp(ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cbf792c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_month(month):\n",
    "    convert = month.to_pydatetime().month\n",
    "    month_dict = {\n",
    "        1: 'Jan',\n",
    "        2: 'Feb',\n",
    "        3: 'Mar',\n",
    "        4: 'April',\n",
    "        5: 'May',\n",
    "        6: 'June',\n",
    "        7: 'July',\n",
    "        8: \"Aug\",\n",
    "        9: 'Sept',\n",
    "        10: 'Oct',\n",
    "        11: 'Nov',\n",
    "        12: 'Dec'\n",
    "    }\n",
    "\n",
    "    return month_dict[convert]\n",
    "\n",
    "df_ptsd['month'] = df_ptsd['datetime'].apply(lambda x: x.to_pydatetime().month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2bbb729",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ptsd['day'] = df_ptsd['datetime'].apply(lambda x: x.strftime('%A'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2403cedd",
   "metadata": {},
   "source": [
    "### Cleaning text - Without Lemmatizing/stemming\n",
    "\n",
    "I will be defining a function that helps to do the following: \n",
    "\n",
    "- **Remove HTML artefacts** using *BeautifulSoup* library.\n",
    "- **Expand all contractions** using *contractions* library.\n",
    "- **Remove all numbers, punctuations and special characters, except '-'** to keep hyphenated words, using *re* library.\n",
    "- **Convert all text to lowercase**.\n",
    "- **Tokenize all words (hyphenated words stay hyphenated)** using *RegexpTokenizer* from *nltk.tokenize*.\n",
    "- **Remove all stopwords** using the english stopwords list from *nltk.corpus*.\n",
    "- **Remove subreddit names** *('PTSD', 'Anxiety')* and other related words to avoid **target leakage**.\n",
    "- **Join all tokenized words** into a string separated by spaces.\n",
    "\n",
    "Stemming/Lemmatizing of words will be done during error analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9155e12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to clean all_text column following the steps mentioned above.\n",
    "def clean_text(text_raw):\n",
    "    \n",
    "    # Removing any and all HTML artefacts from the text (just in case reddit API still left behind some).\n",
    "    text_html_removed = BeautifulSoup(text_raw).get_text()\n",
    "    \n",
    "    # Expanding all contractions using the contractions library (for example isn't is converted to is not).\n",
    "    text_contrac_fixed = contractions.fix(text_html_removed)\n",
    "    \n",
    "    # Using regex to remove all numbers, punctuations and special characters (except '-' to keep hyphenated words).\n",
    "    text_punc_removed = re.sub(r'[^A-Za-z\\-\\s]', ' ', text_contrac_fixed)\n",
    "    \n",
    "    # Tokenizing the text after converting it to lowercase.\n",
    "    # Regex here tokenizes by every character except one or more ('+') consecutive word characters ('\\w') or hyphens ('-').\n",
    "    text_tokens_list = RegexpTokenizer(r'[\\w\\-]+').tokenize(text_punc_removed.lower())\n",
    "    \n",
    "    # Removing stopwords using the english stopwords list from nltk.corpus.\n",
    "    text_stopwords_removed = [word for word in text_tokens_list if word not in set(nltk.corpus.stopwords.words('english'))]\n",
    "    \n",
    "    # Removing subreddit name from text (target leakage).\n",
    "    text_cleaned = [word for word in text_stopwords_removed if word not in set(['ptsd', 'anxious', 'anxiety',\"cptsd\", 'trauma','traumatized','traumas',\"posttraumatic\",'stress','disorder',\"traumatic\"])]\n",
    "    \n",
    "    # Returning a string of all tokenized words joined together with spaces in between.\n",
    "    return \" \".join(text_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34743510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply function to clean text\n",
    "df_ptsd['clean_text']= df_ptsd['alltext'].apply(lambda x: clean_text(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac779789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate lemmatizer.\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# define function that takes in words from paragraph, and lemmitize and join back as paragraph\n",
    "def lemmi_words(text):\n",
    "    return ' '.join([lemmatizer.lemmatize(i) for i in text.split(' ')])\n",
    "\n",
    "df_ptsd['lemmi_clean_text'] = df_ptsd['clean_text'].map(lambda x: lemmi_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c06533f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_export = df_ptsd[['author','subreddit','lemmi_clean_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "161f18e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# record csv\n",
    "df_export.to_csv('../Data/TEST_UNSEEN.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
