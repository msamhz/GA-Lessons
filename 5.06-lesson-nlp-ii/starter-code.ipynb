{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# NLP II: `CountVectorizer`, `TfidfVectorizer`, and Modeling\n",
    "\n",
    "_Authors: Dave Yerrington (SF), Justin Pounders (ATL), Riley Dallas (ATX), Matt Brems (DC), Noelle Brown (DEN)_\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"https://snag.gy/uvESGH.jpg\" alt=\"drawing\" width=\"800\"/>\n",
    "\n",
    "### $$\n",
    "\\begin{eqnarray*}\n",
    "\\textbf{Fun Fact:  } \\text{Word Clouds} &\\neq& \\text{Data Science}\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "[If you want to generate a word cloud in the shape of something **for art only**, check here.](https://medium.com/hackernoon/what-real-fake-news-says-about-obamas-presidency-4bf42be71ff1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Learning Objectives\n",
    "---\n",
    "\n",
    "- Extract features from unstructured text by fitting and transforming with `CountVectorizer` and `TfidfVectorizer`.\n",
    "- Describe how CountVectorizers and TF-IDFVectorizers work.\n",
    "- Understand `stop_words`, `max_features`, `min_df`, `max_df`, and `ngram_range`.\n",
    "- Implement `CountVectorizer` and `TfidfVectorizer` in a spam classification model.\n",
    "- Use `GridSearchCV` and `Pipeline` with `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "\n",
    "# Import CountVectorizer and TFIDFVectorizer from feature_extraction.text.\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reminder of the Data Science Process\n",
    "1. Define problem.\n",
    "2. Gather data.\n",
    "3. Explore data.\n",
    "    - Yes, we can still do EDA with text data!\n",
    "    - We also have to pre-process our text data to prepare it for modeling.\n",
    "4. Model with data.\n",
    "5. Evaluate model.\n",
    "6. Answer problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Text Feature Extraction\n",
    "\n",
    "The models we've learned, like linear regression, logistic regression, and k-nearest neighbors, take in an `X` and a `y` variable.\n",
    "- `X` is a matrix/dataframe of real numbers.\n",
    "- `y` is a vector/series of real numbers.\n",
    "\n",
    "Text data (also called natural language data) is not already organized as a matrix or vector of real numbers. We say that this data is **unstructured**.\n",
    "\n",
    "> This lesson will focus on how to transform our unstructured text data into a numeric `X` matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Classification Model\n",
    "\n",
    "One common application of NLP is predicting \"spam\" vs. \"ham,\" or \"spam\" vs. \"not spam.\"\n",
    "\n",
    "Can we predict real vs. promotional texts just based on what is written?\n",
    "\n",
    "> This data set was taken from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data.\n",
    "spam = pd.read_csv('./data/SMSSpamCollection',\n",
    "                 sep='\\t',\n",
    "                 names=['label', 'message'])\n",
    "\n",
    "# Check out first five rows.\n",
    "spam.head()\n",
    "\n",
    "spam['label'] = spam['label'].map(lambda x: 1 if x == 'spam' else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            message\n",
       "0      0  Go until jurong point, crazy.. Available only ...\n",
       "1      0                      Ok lar... Joking wif u oni...\n",
       "2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      0  U dun say so early hor... U c already then say...\n",
       "4      0  Nah I don't think he goes to usf, he lives aro...\n",
       "5      1  FreeMsg Hey there darling it's been 3 week's n...\n",
       "6      0  Even my brother is not like to speak with me. ...\n",
       "7      0  As per your request 'Melle Melle (Oru Minnamin...\n",
       "8      1  WINNER!! As a valued network customer you have...\n",
       "9      1  Had your mobile 11 months or more? U R entitle..."
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic terminology\n",
    "\n",
    "---\n",
    "\n",
    "- A collection of text is a **document**. \n",
    "    - You can think of a document as a row in your feature matrix.\n",
    "- A collection of documents is a **corpus**. \n",
    "    - You can think of your full dataframe as the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>In this specific example, what is a document?</summary>\n",
    "    \n",
    "- Each text message in our data set is one document. \n",
    "- There are 5,572 documents in our corpus.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get our data\n",
    "---\n",
    "\n",
    "Convert ham/spam into binary labels:\n",
    "- 0 for ham\n",
    "- 1 for spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create label column\n",
    "spam['label'] = spam['label'].map(lambda x: 1 if x == 'spam' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    5572\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up our data for modeling:\n",
    "- `X` will be the `message` column. **NOTE**: `CountVectorizer` requires a vector, so make sure you set `X` to be a `pandas` Series, **not** a DataFrame.\n",
    "- `y` will be the `label` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = spam['message']\n",
    "y = spam['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Go until jurong point, crazy.. Available only ...\n",
       "1                           Ok lar... Joking wif u oni...\n",
       "2       Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3       U dun say so early hor... U c already then say...\n",
       "4       Nah I don't think he goes to usf, he lives aro...\n",
       "                              ...                        \n",
       "5567    This is the 2nd time we have tried 2 contact u...\n",
       "5568                 Will ü b going to esplanade fr home?\n",
       "5569    Pity, * was in mood for that. So...any other s...\n",
       "5570    The guy did some bitching but I acted like i'd...\n",
       "5571                           Rofl. Its true to its name\n",
       "Name: message, Length: 5572, dtype: object"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check what we need to check in a classification problem.\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1.0\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts(normalize=True)\n",
    "\n",
    "# not balanced --> we need to stratify y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into the training and testing sets.\n",
    "\n",
    "# stratify y \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.33,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing\n",
    "\n",
    "Let's review some of the pre-processing steps for text data:\n",
    "\n",
    "- Remove special characters\n",
    "- Tokenizing\n",
    "- Lemmatizing/Stemming\n",
    "- Stop word removal\n",
    "\n",
    "`CountVectorizer` actually can do a lot of this for us! It is important to keep these steps in mind in case you want to change the default methods used for each of these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `CountVectorizer`\n",
    "---\n",
    "\n",
    "The easiest way for us to convert text data into a structured, numeric `X` dataframe is to use `CountVectorizer`.\n",
    "\n",
    "- **Count**: Count up how many times a token is observed in a given document.\n",
    "- **Vectorizer**: Create a column (also known as a vector) that stores those counts.\n",
    "\n",
    "![](./images/countvectorizer2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a CountVectorizer.\n",
    "# can input stop_words list (can include all the subreddits headers \n",
    "\n",
    "# can put in max features (only include top 1000 words)\n",
    "# min_df = occurs in 10 documnets if = 100 \n",
    "cvec = CountVectorizer(stop_words = ['to','from'], max_features = 1000, min_df = 0.01, ngram_range= (1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(max_features=1000, min_df=0.01, ngram_range=(1, 3),\n",
       "                stop_words=['to', 'from'])"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the vectorizer on our corpus.\n",
    "cvec.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1829                             May i call You later Pls\n",
       "1651    I dont have any of your file in my bag..i was ...\n",
       "2310    I have 2 sleeping bags, 1 blanket and paper an...\n",
       "3119    R u saying i should re order the slippers cos ...\n",
       "1054       Jay's getting really impatient and belligerent\n",
       "                              ...                        \n",
       "2608    :-) yeah! Lol. Luckily i didn't have a starrin...\n",
       "4086    Orange brings you ringtones from all time Char...\n",
       "432     Does she usually take fifteen fucking minutes ...\n",
       "5073    WIN a £200 Shopping spree every WEEK Starting ...\n",
       "3519                Are you willing to go for apps class.\n",
       "Name: message, Length: 3733, dtype: object"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Transform the corpus.\n",
    "X_train = cvec.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3733x221 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 26808 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converted to sparse matrix \n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = cvec.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['150p',\n",
       " '16',\n",
       " '18',\n",
       " 'about',\n",
       " 'after',\n",
       " 'again',\n",
       " 'all',\n",
       " 'already',\n",
       " 'also',\n",
       " 'always',\n",
       " 'am',\n",
       " 'amp',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'anything',\n",
       " 'are',\n",
       " 'are you',\n",
       " 'as',\n",
       " 'ask',\n",
       " 'at',\n",
       " 'babe',\n",
       " 'back',\n",
       " 'be',\n",
       " 'been',\n",
       " 'before',\n",
       " 'but',\n",
       " 'buy',\n",
       " 'by',\n",
       " 'call',\n",
       " 'call me',\n",
       " 'can',\n",
       " 'care',\n",
       " 'cash',\n",
       " 'chat',\n",
       " 'claim',\n",
       " 'com',\n",
       " 'come',\n",
       " 'contact',\n",
       " 'cos',\n",
       " 'could',\n",
       " 'customer',\n",
       " 'da',\n",
       " 'day',\n",
       " 'dear',\n",
       " 'did',\n",
       " 'do',\n",
       " 'do you',\n",
       " 'doing',\n",
       " 'don',\n",
       " 'dont',\n",
       " 'every',\n",
       " 'feel',\n",
       " 'find',\n",
       " 'for',\n",
       " 'for the',\n",
       " 'free',\n",
       " 'friends',\n",
       " 'get',\n",
       " 'give',\n",
       " 'go',\n",
       " 'going',\n",
       " 'good',\n",
       " 'got',\n",
       " 'great',\n",
       " 'gt',\n",
       " 'gud',\n",
       " 'had',\n",
       " 'happy',\n",
       " 'has',\n",
       " 'have',\n",
       " 'have won',\n",
       " 'he',\n",
       " 'help',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hey',\n",
       " 'hi',\n",
       " 'him',\n",
       " 'his',\n",
       " 'home',\n",
       " 'hope',\n",
       " 'how',\n",
       " 'if',\n",
       " 'if you',\n",
       " 'im',\n",
       " 'in',\n",
       " 'in the',\n",
       " 'is',\n",
       " 'is the',\n",
       " 'it',\n",
       " 'its',\n",
       " 'just',\n",
       " 'know',\n",
       " 'last',\n",
       " 'late',\n",
       " 'later',\n",
       " 'leave',\n",
       " 'let',\n",
       " 'life',\n",
       " 'like',\n",
       " 'll',\n",
       " 'lol',\n",
       " 'lor',\n",
       " 'love',\n",
       " 'lt',\n",
       " 'lt gt',\n",
       " 'make',\n",
       " 'many',\n",
       " 'me',\n",
       " 'meet',\n",
       " 'message',\n",
       " 'min',\n",
       " 'miss',\n",
       " 'mobile',\n",
       " 'money',\n",
       " 'more',\n",
       " 'morning',\n",
       " 'msg',\n",
       " 'much',\n",
       " 'my',\n",
       " 'need',\n",
       " 'new',\n",
       " 'nice',\n",
       " 'night',\n",
       " 'no',\n",
       " 'not',\n",
       " 'now',\n",
       " 'number',\n",
       " 'of',\n",
       " 'off',\n",
       " 'oh',\n",
       " 'ok',\n",
       " 'on',\n",
       " 'one',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'out',\n",
       " 'over',\n",
       " 'phone',\n",
       " 'pick',\n",
       " 'place',\n",
       " 'please',\n",
       " 'pls',\n",
       " 'prize',\n",
       " 're',\n",
       " 'really',\n",
       " 'reply',\n",
       " 'right',\n",
       " 'said',\n",
       " 'say',\n",
       " 'see',\n",
       " 'send',\n",
       " 'sent',\n",
       " 'service',\n",
       " 'she',\n",
       " 'should',\n",
       " 'so',\n",
       " 'some',\n",
       " 'something',\n",
       " 'sorry',\n",
       " 'still',\n",
       " 'stop',\n",
       " 'sure',\n",
       " 'take',\n",
       " 'tell',\n",
       " 'text',\n",
       " 'thanks',\n",
       " 'that',\n",
       " 'the',\n",
       " 'them',\n",
       " 'then',\n",
       " 'there',\n",
       " 'they',\n",
       " 'thing',\n",
       " 'think',\n",
       " 'this',\n",
       " 'this is',\n",
       " 'time',\n",
       " 'today',\n",
       " 'tomorrow',\n",
       " 'too',\n",
       " 'txt',\n",
       " 'uk',\n",
       " 'up',\n",
       " 'ur',\n",
       " 'urgent',\n",
       " 'us',\n",
       " 've',\n",
       " 'wait',\n",
       " 'want',\n",
       " 'was',\n",
       " 'wat',\n",
       " 'way',\n",
       " 'we',\n",
       " 'week',\n",
       " 'well',\n",
       " 'what',\n",
       " 'when',\n",
       " 'when you',\n",
       " 'where',\n",
       " 'which',\n",
       " 'who',\n",
       " 'why',\n",
       " 'will',\n",
       " 'will be',\n",
       " 'win',\n",
       " 'with',\n",
       " 'won',\n",
       " 'work',\n",
       " 'would',\n",
       " 'www',\n",
       " 'yeah',\n",
       " 'yes',\n",
       " 'you',\n",
       " 'you are',\n",
       " 'you have',\n",
       " 'you know',\n",
       " 'your']"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>150p</th>\n",
       "      <th>16</th>\n",
       "      <th>18</th>\n",
       "      <th>about</th>\n",
       "      <th>after</th>\n",
       "      <th>again</th>\n",
       "      <th>all</th>\n",
       "      <th>already</th>\n",
       "      <th>also</th>\n",
       "      <th>always</th>\n",
       "      <th>...</th>\n",
       "      <th>work</th>\n",
       "      <th>would</th>\n",
       "      <th>www</th>\n",
       "      <th>yeah</th>\n",
       "      <th>yes</th>\n",
       "      <th>you</th>\n",
       "      <th>you are</th>\n",
       "      <th>you have</th>\n",
       "      <th>you know</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3728</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3729</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3730</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3731</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3732</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3733 rows × 221 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      150p  16  18  about  after  again  all  already  also  always  ...  \\\n",
       "0        0   0   0      0      0      0    0        0     0       0  ...   \n",
       "1        0   0   0      0      0      0    0        0     0       0  ...   \n",
       "2        0   0   0      0      0      0    0        0     0       0  ...   \n",
       "3        0   0   0      0      0      0    0        0     0       0  ...   \n",
       "4        0   0   0      0      0      0    0        0     0       0  ...   \n",
       "...    ...  ..  ..    ...    ...    ...  ...      ...   ...     ...  ...   \n",
       "3728     0   0   0      0      0      0    0        0     0       0  ...   \n",
       "3729     0   0   0      0      0      0    1        0     0       0  ...   \n",
       "3730     0   0   0      0      0      0    0        0     0       0  ...   \n",
       "3731     0   0   0      0      0      0    0        0     0       0  ...   \n",
       "3732     0   0   0      0      0      0    0        0     0       0  ...   \n",
       "\n",
       "      work  would  www  yeah  yes  you  you are  you have  you know  your  \n",
       "0        0      0    0     0    0    1        0         0         0     0  \n",
       "1        1      0    0     0    0    2        0         0         0     1  \n",
       "2        0      0    0     0    0    0        0         0         0     0  \n",
       "3        0      0    0     0    0    0        0         0         0     0  \n",
       "4        0      0    0     0    0    0        0         0         0     0  \n",
       "...    ...    ...  ...   ...  ...  ...      ...       ...       ...   ...  \n",
       "3728     0      0    0     1    0    1        0         0         0     0  \n",
       "3729     0      0    0     0    0    1        0         0         0     0  \n",
       "3730     0      0    0     0    1    0        0         0         0     0  \n",
       "3731     0      0    0     0    0    0        0         0         0     0  \n",
       "3732     0      0    0     0    0    1        0         0         0     0  \n",
       "\n",
       "[3733 rows x 221 columns]"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just to check how many columsn and rows and inspect what is taken out\n",
    "# columns 1000 columns only \n",
    "# \n",
    "#  \n",
    "df = pd.DataFrame(\n",
    "    X_train.todense(),\n",
    "    columns = cvec.get_feature_names()\n",
    ")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['150p', '16', '18', 'about', 'after', 'again', 'all', 'already', 'also',\n",
       "       'always',\n",
       "       ...\n",
       "       'work', 'would', 'www', 'yeah', 'yes', 'you', 'you are', 'you have',\n",
       "       'you know', 'your'],\n",
       "      dtype='object', length=221)"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/countvectorizer.png\" alt=\"drawing\" width=\"750\"/>\n",
    "\n",
    "[Source](https://towardsdatascience.com/nlp-learning-series-part-2-conventional-methods-for-text-classification-40f2839dd061)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have unstructured text data, there is a lot of information in that text data.\n",
    "- When we force unstructured text data to follow a \"spreadsheet\" or \"dataframe\" structure, we might lose some of that information.\n",
    "- For example, CountVectorizer creates a vector (column) for each token and counts up the number of occurrences of each token in each document.\n",
    "\n",
    "Our tokens are now stored as a **bag-of-words**. This is a simplified way of looking at and storing our data. \n",
    "- Bag-of-words representations discard grammar, order, and structure in the text but track occurrences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we could fit a model (like a logistic regression model or $k$-nearest neighbors model) using our transformed data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What might be some of the advantages of using this bag-of-words approach when modeling?</summary>\n",
    "\n",
    "- Efficient to store.\n",
    "- Efficient to model.\n",
    "- Keeps a decent amount of information.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What might be some of the disadvantages of using this bag-of-words approach when modeling?</summary>\n",
    "\n",
    "- Since bag-of-words models discard grammar, order, structure, and context, we lose a decent amount of information.\n",
    "- Phrases like \"not bad\" or \"not good\" won't be interpreted properly.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, let's examine some of the different hyperparameters of `CountVectorizer`:\n",
    "- `stop_words`\n",
    "- `max_features`, `max_df`, `min_df`\n",
    "- `ngram_range`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Remind me: what is a hyperparameter?</summary>\n",
    "\n",
    "- A hyperparameter is a built-in option that affects our model, but our model cannot learn these from our data!\n",
    "- Examples of hyperparameters include:\n",
    "    - the value of $k$ and the distance metric in $k$-nearest neighbors,\n",
    "    - our regularization constants $\\alpha$ or $C$ in linear and logistic regression.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "\n",
    "---\n",
    "\n",
    "Some words are so common that they may not provide legitimate information about the $Y$ variable we're trying to predict.\n",
    "\n",
    "Let's see what our top-occurring words are right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert X_train into a DataFrame.\n",
    "# We will not actually use this for modeling,\n",
    "# this is just to visualize what is happening\n",
    "# to and from removed \n",
    "\n",
    "X_train_df = pd.DataFrame(X_train.todense(), \n",
    "                          columns=cvec.get_feature_names())\n",
    "\n",
    "# plot top occuring words\n",
    "X_train_df.sum().sort_values(ascending=False).head(10).plot(kind='barh');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What do you think of the top 10 words?</summary>\n",
    "\n",
    "- These are pretty much all stop words!\n",
    "- Using a bag-of-words approach with these words in there might not add anything meaningful to our analysis.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'move', 'how', 'none', 'others', 'anyway', 'became', 'further', 'whither', 'several', 'as', 'only', 'therefore', 'moreover', 'less', 'once', 'anywhere', 'meanwhile', 'whether', 'wherever', 'another', 'every', 'be', 'whom', 'am', 'her', 'name', 'nowhere', 'the', 'whole', 'bill', 'nine', 'up', 'done', 'each', 'four', 'please', 'may', 'hundred', 'never', 'wherein', 'perhaps', 'has', 'to', 'almost', 'de', 'sometimes', 'our', 'something', 'full', 'somewhere', 'becoming', 'many', 'former', 'beyond', 'before', 'can', 'since', 'yours', 'sincere', 'least', 'are', 'thin', 're', 'not', 'again', 'except', 'with', 'become', 'between', 'why', 'ie', 'here', 'thus', 'inc', 'detail', 'him', 'under', 'there', 'per', 'everywhere', 'indeed', 'all', 'serious', 'i', 'nothing', 'yet', 'if', 'via', 'them', 'empty', 'anyone', 'hereupon', 'whereas', 'whenever', 'cry', 'whence', 'although', 'among', 'do', 'front', 'beside', 'first', 'but', 'find', 'however', 'onto', 'show', 'after', 'off', 'seem', 'get', 'his', 'both', 'itself', 'otherwise', 'ever', 'at', 'eg', 'very', 'above', 'elsewhere', 'third', 'anyhow', 'whatever', 'your', 'you', 'whereupon', 'eight', 'it', 'everything', 'side', 'becomes', 'those', 'three', 'whereafter', 'take', 'hasnt', 'somehow', 'while', 'no', 'into', 'thence', 'than', 'amoungst', 'bottom', 'same', 'whoever', 'either', 'against', 'hereafter', 'forty', 'everyone', 'system', 'they', 'already', 'ours', 'herein', 'for', 'mostly', 'now', 'during', 'fill', 'namely', 'seeming', 'due', 'must', 'thru', 'down', 'we', 'until', 'an', 'twelve', 'rather', 'last', 'a', 'this', 'back', 'nevertheless', 'noone', 'across', 'ten', 'fifteen', 'thick', 'else', 'anything', 'thereby', 'below', 'then', 'us', 'ourselves', 'five', 'afterwards', 'over', 'most', 'whose', 'still', 'of', 'made', 'couldnt', 'is', 'in', 'been', 'these', 'amount', 'behind', 'mine', 'latter', 'therein', 'seemed', 'about', 'also', 'along', 'such', 'might', 'because', 'have', 'interest', 'un', 'much', 'towards', 'when', 'fifty', 'often', 'go', 'without', 'describe', 'around', 'was', 'hereby', 'cant', 'latterly', 'sometime', 'me', 'themselves', 'always', 'where', 'nor', 'neither', 'on', 'from', 'whereby', 'alone', 'upon', 'someone', 'were', 'yourself', 'ltd', 'seems', 'besides', 'more', 'or', 'should', 'even', 'himself', 'put', 'con', 'had', 'throughout', 'which', 'few', 'one', 'two', 'own', 'next', 'twenty', 'myself', 'could', 'top', 'thereafter', 'any', 'though', 'call', 'she', 'will', 'give', 'herself', 'their', 'through', 'eleven', 'its', 'beforehand', 'what', 'amongst', 'by', 'fire', 'that', 'found', 'my', 'and', 'other', 'toward', 'etc', 'together', 'well', 'thereupon', 'co', 'so', 'formerly', 'would', 'mill', 'part', 'too', 'he', 'nobody', 'hence', 'hers', 'out', 'six', 'some', 'keep', 'see', 'cannot', 'who', 'enough', 'being', 'within', 'sixty', 'yourselves'})\n"
     ]
    }
   ],
   "source": [
    "# Let's look at sklearn's stopwords.\n",
    "print(CountVectorizer(stop_words = 'english').get_stop_words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CountVectorizer` gives you the option to eliminate stopwords from your corpus when instantiating your vectorizer.\n",
    "\n",
    "```python\n",
    "cvec = CountVectorizer(stop_words='english')\n",
    "```\n",
    "\n",
    "You can optionally pass your own list of stopwords that you'd like to remove.\n",
    "```python\n",
    "cvec = CountVectorizer(stop_words=['list', 'of', 'words', 'to', 'stop'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Vocabulary size\n",
    "\n",
    "---\n",
    "One downside to `CountVectorizer` is the size of its vocabulary (`cvec.get_feature_names()`) can get really large. We're creating one column for every unique token in your corpus of data!\n",
    "\n",
    "There are three hyperparameters to help you control this.\n",
    "\n",
    "1. You can set `max_features` to only include the $N$ most popular vocabulary words in the corpus.\n",
    "\n",
    "```python\n",
    "cvec = CountVectorizer(max_features=1_000) # Only the top 1,000 words from the entire corpus will be saved\n",
    "```\n",
    "\n",
    "2. You can tell `CountVectorizer` to only consider words that occur in **at least** some number of documents.\n",
    "\n",
    "```python\n",
    "cvec = CountVectorizer(min_df=2) # A word must occur in at least two documents from the corpus\n",
    "```\n",
    "\n",
    "3. Conversely, you can tell `CountVectorizer` to only consider words that occur in **at most** some percentage of documents.\n",
    "\n",
    "```python\n",
    "cvec = CountVectorizer(max_df=.98) # Ignore words that occur in > 98% of the documents from the corpus\n",
    "```\n",
    "\n",
    "Both `max_df` and `min_df` can accept either an integer or a float.\n",
    "- An integer tells us the number of documents.\n",
    "- A float tells us the percentage of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Why might we want to control these vocabulary size hyperparameters?</summary>\n",
    "    \n",
    "- If we have too many features, our models may take a **very** long time to fit.\n",
    "- Control for overfitting/underfitting.\n",
    "- Words in 99% of documents or words occuring in only one document might not be very informative.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Gram Range\n",
    "---\n",
    "\n",
    "`CountVectorizer` has the ability to capture $n$-word phrases, also called $n$-grams. Consider the following:\n",
    "\n",
    "> The quick brown fox jumped over the lazy dog.\n",
    "\n",
    "In the example sentence, the 2-grams are:\n",
    "- 'the quick'\n",
    "- 'quick brown'\n",
    "- 'brown fox'\n",
    "- 'fox jumped'\n",
    "- 'jumped over'\n",
    "- 'over the'\n",
    "- 'the lazy'\n",
    "- 'lazy dog'\n",
    "\n",
    "The `ngram_range` determines what $n$-grams should be considered as features.\n",
    "\n",
    "```python\n",
    "cvec = CountVectorizer(ngram_range=(1,2)) # Captures every 1-gram and every 2-gram\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>How many 3-grams would be generated from the phrase \"the quick brown fox jumped over the lazy dog?\"</summary>\n",
    "\n",
    "- Seven 3-grams.\n",
    "    - 'the quick brown'\n",
    "    - 'quick brown fox'\n",
    "    - 'brown fox jumped'\n",
    "    - 'fox jumped over'\n",
    "    - 'jumped over the'\n",
    "    - 'over the lazy'\n",
    "    - 'the lazy dog'\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Why might we want to change ngram_range to something other than (1,1)?</summary>\n",
    "\n",
    "- We can work with multi-word phrases like \"not good\" or \"very hot.\"\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "---\n",
    "\n",
    "We may want to test lots of different values of hyperparameters in our CountVectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Is CountVectorizer an estimator or a transformer?</summary>\n",
    "    \n",
    "- A transformer.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Why do we need a pipeline to GridSearch over our CountVectorizer hyperparameters?</summary>\n",
    "    \n",
    "- The CountVectorizer is a transformer.\n",
    "- Transformers have .fit() and .transform() methods, but cannot do .predict().\n",
    "- In order to GridSearch over hyperparameters, we need some way to score our model performance.\n",
    "- A pipeline stacks together one or more transformers with an estimator at the end. The estimator allows us to .predict() and get a score!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.33,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline accuracy\n",
    "\n",
    "We need to calculate baseline accuracy in order to tell if our model is better than null model (predicting the plurality class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naïve Bayes\n",
    "\n",
    "I am only going to scratch the surface of this algorithm. For more details on it, check out the resources in the [README](../README.md)!\n",
    "\n",
    "Naïve Bayes relies on [Bayes theorem](https://www.mathsisfun.com/data/bayes-theorem.html), which we will officially cover in a later week. Right now, just know that we rely on our prior knowledge to calculate probabilities.\n",
    "\n",
    "In order to understand Bayes theorem, we need to remember conditional probabilities. A quick example to understand this intuitively:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>If you pick a card from a standard 52-card deck, what is the probability of drawing a queen given the card is a heart?</summary>\n",
    "    \n",
    "I have told you the condition: that the card is a heart. Therefore, we only have 13 options to choose from, since there are 13 hearts in a deck of cards. Out of these, only 1 card is a queen (there is one queen in each suit), so the probability of drawing a queen given the card is a heart is 1/13.\n",
    "    \n",
    "It is important to note here that the probability of drawing a queen given the card is a heart is not the same as the probability of drawing a heart given the card is a queen! This would be 1/4.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we know $P(B|A)$, Bayes theorem allows us to calculate the probability of $P(A|B)$ by relating the probability of $P(A|B)$ to $P(B|A)$. \n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\text{Bayes' Theorem: } P(A|B) &=& \\frac{P(B|A)P(A)}{P(B)}\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "- Let $A$ be that a message is spam.\n",
    "- Let $B$ represent the words used in the message.\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\text{Bayes' Theorem: } P(A|B) &=& \\frac{P(B|A)P(A)}{P(B)} \\\\\n",
    "\\Rightarrow P(\\text{message is spam}|\\text{words in message}) &=& \\frac{P(\\text{words in message}|\\text{message is spam})P(\\text{message is spam})}{P(\\text{words in message})}\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "We want to calculate the probability that a post is spam **given** the words that are in the message! Our model can learn this from the training data.\n",
    "\n",
    "**Naïve Bayes** makes the assumption that all features are independent of one another (this is why it is called *naïve*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Why is this assumption not realistic with our data?</summary>\n",
    "    \n",
    "Text data is never independent! Certain words can change the context of a sentence when used with other words. The way language works, we have words that are more or less likely to follow other words.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite this assumption not being realistic with NLP data, we still use Naïve Bayes pretty frequently.\n",
    "- It's a very fast modeling algorithm (which is great especially when we have lots of features and/or lots of data!).\n",
    "- It is often an excellent classifier, outperforming more complicated models.\n",
    "\n",
    "There are three common types of Naive Bayes models: Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes.\n",
    "- How do we pick which of the three models to use? It depends on our $X$ variable.\n",
    "    - [Bernoulli Naive Bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB): when we have 0/1 variables.\n",
    "    - [Multinomial Naive Bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB): when our variables are positive integers.\n",
    "    - [Gaussian Naive Bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB): when our features are Normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set a pipeline up with two stages:\n",
    "# 1. CountVectorizer (transformer)\n",
    "# 2. Multinomial Naive Bayes (estimator)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('lr', LogisticRegression())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('cvec', CountVectorizer()), ('lr', LogisticRegression())],\n",
       " 'verbose': False,\n",
       " 'cvec': CountVectorizer(),\n",
       " 'lr': LogisticRegression(),\n",
       " 'cvec__analyzer': 'word',\n",
       " 'cvec__binary': False,\n",
       " 'cvec__decode_error': 'strict',\n",
       " 'cvec__dtype': numpy.int64,\n",
       " 'cvec__encoding': 'utf-8',\n",
       " 'cvec__input': 'content',\n",
       " 'cvec__lowercase': True,\n",
       " 'cvec__max_df': 1.0,\n",
       " 'cvec__max_features': None,\n",
       " 'cvec__min_df': 1,\n",
       " 'cvec__ngram_range': (1, 1),\n",
       " 'cvec__preprocessor': None,\n",
       " 'cvec__stop_words': None,\n",
       " 'cvec__strip_accents': None,\n",
       " 'cvec__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'cvec__tokenizer': None,\n",
       " 'cvec__vocabulary': None,\n",
       " 'lr__C': 1.0,\n",
       " 'lr__class_weight': None,\n",
       " 'lr__dual': False,\n",
       " 'lr__fit_intercept': True,\n",
       " 'lr__intercept_scaling': 1,\n",
       " 'lr__l1_ratio': None,\n",
       " 'lr__max_iter': 100,\n",
       " 'lr__multi_class': 'auto',\n",
       " 'lr__n_jobs': None,\n",
       " 'lr__penalty': 'l2',\n",
       " 'lr__random_state': None,\n",
       " 'lr__solver': 'lbfgs',\n",
       " 'lr__tol': 0.0001,\n",
       " 'lr__verbose': 0,\n",
       " 'lr__warm_start': False}"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `GridSearchCV`\n",
    "---\n",
    "\n",
    "At this point, you could use your `pipeline` object as a model:\n",
    "\n",
    "```python\n",
    "# Estimate how your model will perform on unseen data\n",
    "cross_val_score(pipe, X_train, y_train, cv=3).mean() \n",
    "\n",
    "# Fit your model\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Training score\n",
    "pipe.score(X_train, y_train)\n",
    "\n",
    "# Test score\n",
    "pipe.score(X_test, y_test)\n",
    "```\n",
    "\n",
    "Since we want to tune over the `CountVectorizer`, we'll load our `pipeline` object into `GridSearchCV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('cvec', CountVectorizer()),\n",
       "                                       ('lr', LogisticRegression())]),\n",
       "             param_grid={'cvec__max_df': [0.9, 0.95],\n",
       "                         'cvec__max_features': [2000, 3000, 4000, 5000],\n",
       "                         'cvec__min_df': [2, 3],\n",
       "                         'cvec__ngram_range': [(1, 1), (1, 2)]})"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in data.\n",
    "spam = pd.read_csv('./data/SMSSpamCollection',\n",
    "                 sep='\\t',\n",
    "                 names=['label', 'message'])\n",
    "\n",
    "# Check out first five rows.\n",
    "spam.head()\n",
    "\n",
    "spam['label'] = spam['label'].map(lambda x: 1 if x == 'spam' else 0)\n",
    "\n",
    "X = spam['message']\n",
    "y = spam['label']\n",
    "\n",
    "# Redefine training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.33,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=42)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('lr', LogisticRegression())\n",
    "])\n",
    "\n",
    "pipe_params = {\n",
    "    'cvec__max_features': [2_000, 3_000, 4_000, 5_000],\n",
    "    'cvec__min_df': [2, 3],\n",
    "    'cvec__max_df': [.9, .95],\n",
    "    'cvec__ngram_range': [(1,1), (1,2)]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, # what object are we optimizing?\n",
    "                  param_grid=pipe_params, # what parameters values are we searching?\n",
    "                  cv=5) # 5-fold cross-validation\n",
    "\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "      <th>Coefficients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1717</th>\n",
       "      <td>txt</td>\n",
       "      <td>2.063162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1312</th>\n",
       "      <td>reply</td>\n",
       "      <td>1.741525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1514</th>\n",
       "      <td>text</td>\n",
       "      <td>1.706561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>call</td>\n",
       "      <td>1.658859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1725</th>\n",
       "      <td>uk</td>\n",
       "      <td>1.591523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1323</th>\n",
       "      <td>ringtone</td>\n",
       "      <td>1.579737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>chat</td>\n",
       "      <td>1.363192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>com</td>\n",
       "      <td>1.357640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374</th>\n",
       "      <td>service</td>\n",
       "      <td>1.324262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>150p</td>\n",
       "      <td>1.256773</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Features  Coefficients\n",
       "1717       txt      2.063162\n",
       "1312     reply      1.741525\n",
       "1514      text      1.706561\n",
       "267       call      1.658859\n",
       "1725        uk      1.591523\n",
       "1323  ringtone      1.579737\n",
       "319       chat      1.363192\n",
       "350        com      1.357640\n",
       "1374   service      1.324262\n",
       "22        150p      1.256773"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to create dataframe of highest coefficients \n",
    "\n",
    "df = pd.DataFrame([[x,y] for x,y in zip(gs.best_estimator_[0].get_feature_names(), gs.best_estimator_[1].coef_.tolist()[0])])\n",
    "df.rename(columns= {0: 'Features', 1: 'Coefficients'}, inplace = True)\n",
    "df.sort_values(by = 'Coefficients', ascending = False).head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9823167558527228\n"
     ]
    }
   ],
   "source": [
    "# What's the best score?\n",
    "print(gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.996249665148674\n"
     ]
    }
   ],
   "source": [
    "# Score model on training set.\n",
    "print(gs.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9804241435562806\n"
     ]
    }
   ],
   "source": [
    "# Score model on testing set.\n",
    "print(gs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Is accuracy the best score here?</summary>\n",
    "\n",
    "Since we are classifying whether or not a message is spam, I care more about minimizing false positives here (maximizing for specificity). I prefer for my important emails to go to my inbox (true negatives) and potentially have a few spam messages go to my inbox (false negative) than miss an important email that was incorrectly classified as spam (false positive). \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "preds = gs.predict(X_test)\n",
    "\n",
    "# Save confusion matrix values\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, preds).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# View confusion matrix\n",
    "cmd = plot_confusion_matrix(gs, X_test, y_test, cmap='Blues', values_format='d', display_labels=['Not Spam,','Spam'])\n",
    "cmd.ax_.set(xlabel='Predicted', ylabel='True');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specificity: 0.9974874371859297\n"
     ]
    }
   ],
   "source": [
    "# Calculate the specificity\n",
    "\n",
    "spec = tn / (tn + fp)\n",
    "\n",
    "print('Specificity:', spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Congratulations! We've used `CountVectorizer` to transform our text data into something we can pass into a model.\n",
    "\n",
    "But what if we want to do something more than just count up the occurrence of each token?\n",
    "\n",
    "## Term Frequency-Inverse Document Frequency (TF-IDF) Vectorizer\n",
    "\n",
    "---\n",
    "\n",
    "When modeling, which word do you think tends to be the most helpful?\n",
    "- Words that are common across all documents.\n",
    "- Words that are rare across all documents.\n",
    "- Words that are rare across some documents, and common across some documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Answer:</summary>\n",
    "\n",
    "- Words that are common in certain documents but rare in other documents tend to be more informative than words that are common in all documents or rare in all documents.\n",
    "- Example: If we were examining poetry over time, the word \"thine\" might be common in some documents but rare in most documents. The word \"thine\" is probably pretty informative in this case.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF is a score that tells us which words are important to one document, relative to all other documents. Words that occur often in one document but don't occur in many documents contain more predictive power.\n",
    "\n",
    "Variations of the TF-IDF score are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query.\n",
    "- If you want to see how it can be calculated, check out [the Wikipedia page](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) and [`sklearn`](https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting) page.\n",
    "\n",
    "<img src=\"./images/tfidfvectorizer.png\" alt=\"drawing\" width=\"750\"/>\n",
    "\n",
    "[Source](https://towardsdatascience.com/nlp-learning-series-part-2-conventional-methods-for-text-classification-40f2839dd061)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Practice Using the `TfidfVectorizer`\n",
    "\n",
    "---\n",
    "\n",
    "`sklearn` provides a TF-IDF vectorizer that works similarly to the CountVectorizer.\n",
    "- The arguments `stop_words`, `max_features`, `min_df`, `max_df`, and `ngram_range` also work here.\n",
    "\n",
    "As you did above, instantiate the default `TfidfVectorizer`, then fit the spam and ham data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-283-ddfd28fe7831>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-283-ddfd28fe7831>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    tvec =\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the transformer.\n",
    "tvec ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the top words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# convert training data to dataframe\n",
    "X_train_df = pd.DataFrame(tvec.fit_transform(X_train).todense(), \n",
    "                          columns=tvec.get_feature_names())\n",
    "\n",
    "# plot top occuring words\n",
    "X_train_df.sum().sort_values(ascending=False).head(10).plot(kind='barh');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling Using the `TfidfVectorizer`\n",
    "\n",
    "Let's set up a pipeline using tf-idf and Multinomial Naive Bayes.\n",
    "\n",
    "<details><summary>What's the problem with this?</summary>\n",
    "\n",
    "- Technically, we are supposed to have positive integers to use Multinomial Naive Bayes. Tf-idf does not give us positive integers.\n",
    "- However, it will still work. Even the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB) says \"The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work.\"\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set a pipeline up with two stages:\n",
    "# 1. tf-idf vectorizer (transformer)\n",
    "# 2. Multinomial Naive Bayes (estimator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search over the following values of hyperparameters:\n",
    "# Maximum number of features fit: 2000, 3000, 4000, 5000\n",
    "# No stop words and english stop words\n",
    "# Check (individual tokens) and also check (individual tokens and 2-grams).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate GridSearchCV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit GridSearch to training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score model on training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score model on testing set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "\n",
    "# Save confusion matrix values\n",
    "\n",
    "# Calculate the specificity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (BONUS) How is the information from vectorizers stored efficiently?\n",
    "\n",
    "When you CountVectorize the training text messages, you get 3,733 rows and 6,935 features... this is 25,888,355 entries. That's a lot of data to store in a dataframe!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>How many of these values are zero?</summary>\n",
    "\n",
    "- Over 99% of all values are zero!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of storing all those zeroes, `sklearn` automatically stores these as a sparse matrix. It saves **a lot** of space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.33,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=42)\n",
    "\n",
    "cvec = CountVectorizer()\n",
    "\n",
    "X_train = cvec.fit_transform(X_train)\n",
    "\n",
    "print(type(X_train))\n",
    "print(X_train[0])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
