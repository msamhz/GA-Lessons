{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![GA Logo](https://camo.githubusercontent.com/6ce15b81c1f06d716d753a61f5db22375fa684da/68747470733a2f2f67612d646173682e73332e616d617a6f6e6177732e636f6d2f70726f64756374696f6e2f6173736574732f6c6f676f2d39663838616536633963333837313639306533333238306663663535376633332e706e67) Naive Bayes\n",
    "\n",
    "We've looked at the Naive Bayes classifier from a probability point of view. Now let's apply code to it to a natural language processing problem.\n",
    "\n",
    "### Before we begin... what is natural language processing?\n",
    "\n",
    "- If I'm explaining this to my non-technical peers, natural language processing is just a way for us to get computers to understand written language the way you and I do.\n",
    "\n",
    "- If I'm explaining this to someone with a more technical background, natural language processing is a set of tools that represent words as numbers. This is commonly done by feature engineering (i.e. turning words into columns in your dataframe), but more complicated methods exist.\n",
    "\n",
    "You'll often see natural language processing abbreviated as **NLP**.\n",
    "\n",
    "You and I will use these social media posts in a Naive Bayes classification model to predict whether a post comes from Twitter or Facebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First: some data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the .csv file.\n",
    "\n",
    "df = pd.read_csv(\"./unprocessed_tweets.csv\", encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spam' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-f69c01a460e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m# you can infer that the probability from data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mspam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnormalize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# lets say youre using smoe data set taht someone created mechanical turk? Amazon\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spam' is not defined"
     ]
    }
   ],
   "source": [
    "# 'Hello r u interested'\n",
    "# P *hello | spam)\n",
    "# P(r | Hello, Spam) --> P(r | spam)\n",
    "# P(u | Hello r, spam) -- > P(u | spam)\n",
    "# P (interested | hello r u ) --> P (interested | spam)\n",
    "\n",
    "# Naive bayes is compatible with bags of words model \n",
    "# when we compute the probability, we just multiply everything together \n",
    "# P(hello | spam)^2 * P(r | spam) *P(u |spam) *P(interested | spam)\n",
    "\n",
    "#because we want to compare with P(not spam) --> we cancel away the denominator P(helo r u interested)\n",
    "# P(spam | \"hello r u interested\")\n",
    "# first thing we do --> P(hello r u interested | spam) * P (spam) / P(hello r u interested)\n",
    "# Second, cancel P(hello r u interested)  * P(spam)\n",
    "\n",
    "# some bayesian terminology:\n",
    "# prior (or prior probability) --> P(spam) vs P(not spam)\n",
    "# you can infer that the probability from data \n",
    "\n",
    "# spam.value_counts(normalize = True)\n",
    "\n",
    "# lets say youre using smoe data set taht someone created mechanical turk? Amazon \n",
    "# 50-50 of spam vs not spam for my DS workflow \n",
    "# spam.value_counts(normalize = True)\n",
    "\n",
    "\n",
    "# in log space we add \n",
    "# but in normal space we multiply "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out the columns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>_golden</th>\n",
       "      <th>_unit_state</th>\n",
       "      <th>_trusted_judgments</th>\n",
       "      <th>_last_judgment_at</th>\n",
       "      <th>audience</th>\n",
       "      <th>audience:confidence</th>\n",
       "      <th>bias</th>\n",
       "      <th>bias:confidence</th>\n",
       "      <th>message</th>\n",
       "      <th>...</th>\n",
       "      <th>orig__golden</th>\n",
       "      <th>audience_gold</th>\n",
       "      <th>bias_gold</th>\n",
       "      <th>bioid</th>\n",
       "      <th>embed</th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>message_gold</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>766192484</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>1</td>\n",
       "      <td>8/4/15 21:17</td>\n",
       "      <td>national</td>\n",
       "      <td>1.0</td>\n",
       "      <td>partisan</td>\n",
       "      <td>1.0</td>\n",
       "      <td>policy</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>R000596</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>3.83249E+17</td>\n",
       "      <td>From: Trey Radel (Representative from Florida)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>twitter</td>\n",
       "      <td>RT @nowthisnews: Rep. Trey Radel (R- #FL) slam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>766192485</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>1</td>\n",
       "      <td>8/4/15 21:20</td>\n",
       "      <td>national</td>\n",
       "      <td>1.0</td>\n",
       "      <td>partisan</td>\n",
       "      <td>1.0</td>\n",
       "      <td>attack</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M000355</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>3.11208E+17</td>\n",
       "      <td>From: Mitch McConnell (Senator from Kentucky)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>twitter</td>\n",
       "      <td>VIDEO - #Obamacare:  Full of Higher Costs and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>766192486</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>1</td>\n",
       "      <td>8/4/15 21:14</td>\n",
       "      <td>national</td>\n",
       "      <td>1.0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0</td>\n",
       "      <td>support</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S001180</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>3.39069E+17</td>\n",
       "      <td>From: Kurt Schrader (Representative from Oregon)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>twitter</td>\n",
       "      <td>Please join me today in remembering our fallen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>766192487</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>1</td>\n",
       "      <td>8/4/15 21:08</td>\n",
       "      <td>national</td>\n",
       "      <td>1.0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0</td>\n",
       "      <td>policy</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C000880</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>2.98528E+17</td>\n",
       "      <td>From: Michael Crapo (Senator from Idaho)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>twitter</td>\n",
       "      <td>RT @SenatorLeahy: 1st step toward Senate debat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>766192488</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>1</td>\n",
       "      <td>8/4/15 21:26</td>\n",
       "      <td>national</td>\n",
       "      <td>1.0</td>\n",
       "      <td>partisan</td>\n",
       "      <td>1.0</td>\n",
       "      <td>policy</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>U000038</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>4.07643E+17</td>\n",
       "      <td>From: Mark Udall (Senator from Colorado)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>twitter</td>\n",
       "      <td>.@amazon delivery #drones show need to update ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    _unit_id  _golden _unit_state  _trusted_judgments _last_judgment_at  \\\n",
       "0  766192484    False   finalized                   1      8/4/15 21:17   \n",
       "1  766192485    False   finalized                   1      8/4/15 21:20   \n",
       "2  766192486    False   finalized                   1      8/4/15 21:14   \n",
       "3  766192487    False   finalized                   1      8/4/15 21:08   \n",
       "4  766192488    False   finalized                   1      8/4/15 21:26   \n",
       "\n",
       "   audience  audience:confidence      bias  bias:confidence  message  ...  \\\n",
       "0  national                  1.0  partisan              1.0   policy  ...   \n",
       "1  national                  1.0  partisan              1.0   attack  ...   \n",
       "2  national                  1.0   neutral              1.0  support  ...   \n",
       "3  national                  1.0   neutral              1.0   policy  ...   \n",
       "4  national                  1.0  partisan              1.0   policy  ...   \n",
       "\n",
       "   orig__golden  audience_gold  bias_gold    bioid  \\\n",
       "0           NaN            NaN        NaN  R000596   \n",
       "1           NaN            NaN        NaN  M000355   \n",
       "2           NaN            NaN        NaN  S001180   \n",
       "3           NaN            NaN        NaN  C000880   \n",
       "4           NaN            NaN        NaN  U000038   \n",
       "\n",
       "                                               embed           id  \\\n",
       "0  <blockquote class=\"twitter-tweet\" width=\"450\">...  3.83249E+17   \n",
       "1  <blockquote class=\"twitter-tweet\" width=\"450\">...  3.11208E+17   \n",
       "2  <blockquote class=\"twitter-tweet\" width=\"450\">...  3.39069E+17   \n",
       "3  <blockquote class=\"twitter-tweet\" width=\"450\">...  2.98528E+17   \n",
       "4  <blockquote class=\"twitter-tweet\" width=\"450\">...  4.07643E+17   \n",
       "\n",
       "                                              label message_gold   source  \\\n",
       "0    From: Trey Radel (Representative from Florida)          NaN  twitter   \n",
       "1     From: Mitch McConnell (Senator from Kentucky)          NaN  twitter   \n",
       "2  From: Kurt Schrader (Representative from Oregon)          NaN  twitter   \n",
       "3          From: Michael Crapo (Senator from Idaho)          NaN  twitter   \n",
       "4          From: Mark Udall (Senator from Colorado)          NaN  twitter   \n",
       "\n",
       "                                                text  \n",
       "0  RT @nowthisnews: Rep. Trey Radel (R- #FL) slam...  \n",
       "1  VIDEO - #Obamacare:  Full of Higher Costs and ...  \n",
       "2  Please join me today in remembering our fallen...  \n",
       "3  RT @SenatorLeahy: 1st step toward Senate debat...  \n",
       "4  .@amazon delivery #drones show need to update ...  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See the first five rows.\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all values with an \"audience confidence,\" \"bias\n",
    "# confidence,\" or \"message confidence\" score below 1.\n",
    "\n",
    "df = df[(df['audience:confidence'] >= 1) & (df['bias:confidence'] >= 1) & (df['message:confidence'] >= 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove extra columns.\n",
    "\n",
    "df = df[['_unit_id', '_trusted_judgments', 'audience',\n",
    "         'bias', 'message', 'label', 'source', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relabel columns.\n",
    "\n",
    "df.columns = ['unit_id', 'trusted_judgments', 'audience_feature',\n",
    "              'bias_feature', 'message_feature', 'label_feature',\n",
    "              'source_feature', 'text_feature']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NAs.\n",
    "df.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index.\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have social media data! This includes almost 5,000 messages on either Twitter or Facebook from various politicians. We can use the features we generated to predict things like whether the source is Twitter or Facebook, whether the bias is neutral or partisan, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4888, 8)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many rows and columns do we have?\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unit_id</th>\n",
       "      <th>trusted_judgments</th>\n",
       "      <th>audience_feature</th>\n",
       "      <th>bias_feature</th>\n",
       "      <th>message_feature</th>\n",
       "      <th>label_feature</th>\n",
       "      <th>source_feature</th>\n",
       "      <th>text_feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>766192484</td>\n",
       "      <td>1</td>\n",
       "      <td>national</td>\n",
       "      <td>partisan</td>\n",
       "      <td>policy</td>\n",
       "      <td>From: Trey Radel (Representative from Florida)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>RT @nowthisnews: Rep. Trey Radel (R- #FL) slam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>766192485</td>\n",
       "      <td>1</td>\n",
       "      <td>national</td>\n",
       "      <td>partisan</td>\n",
       "      <td>attack</td>\n",
       "      <td>From: Mitch McConnell (Senator from Kentucky)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>VIDEO - #Obamacare:  Full of Higher Costs and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>766192486</td>\n",
       "      <td>1</td>\n",
       "      <td>national</td>\n",
       "      <td>neutral</td>\n",
       "      <td>support</td>\n",
       "      <td>From: Kurt Schrader (Representative from Oregon)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>Please join me today in remembering our fallen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>766192487</td>\n",
       "      <td>1</td>\n",
       "      <td>national</td>\n",
       "      <td>neutral</td>\n",
       "      <td>policy</td>\n",
       "      <td>From: Michael Crapo (Senator from Idaho)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>RT @SenatorLeahy: 1st step toward Senate debat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>766192488</td>\n",
       "      <td>1</td>\n",
       "      <td>national</td>\n",
       "      <td>partisan</td>\n",
       "      <td>policy</td>\n",
       "      <td>From: Mark Udall (Senator from Colorado)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>.@amazon delivery #drones show need to update ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     unit_id  trusted_judgments audience_feature bias_feature message_feature  \\\n",
       "0  766192484                  1         national     partisan          policy   \n",
       "1  766192485                  1         national     partisan          attack   \n",
       "2  766192486                  1         national      neutral         support   \n",
       "3  766192487                  1         national      neutral          policy   \n",
       "4  766192488                  1         national     partisan          policy   \n",
       "\n",
       "                                      label_feature source_feature  \\\n",
       "0    From: Trey Radel (Representative from Florida)        twitter   \n",
       "1     From: Mitch McConnell (Senator from Kentucky)        twitter   \n",
       "2  From: Kurt Schrader (Representative from Oregon)        twitter   \n",
       "3          From: Michael Crapo (Senator from Idaho)        twitter   \n",
       "4          From: Mark Udall (Senator from Colorado)        twitter   \n",
       "\n",
       "                                        text_feature  \n",
       "0  RT @nowthisnews: Rep. Trey Radel (R- #FL) slam...  \n",
       "1  VIDEO - #Obamacare:  Full of Higher Costs and ...  \n",
       "2  Please join me today in remembering our fallen...  \n",
       "3  RT @SenatorLeahy: 1st step toward Senate debat...  \n",
       "4  .@amazon delivery #drones show need to update ...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View first five rows.\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may note that there are some extra symbols in the data. This is a common problem in natural language processing, especially when dealing with social media (think emoji, hashtags, links, etc.), but we're going to ignore that for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's use Naive Bayes to predict whether a social media post was featured on Facebook or Twitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "facebook    0.510843\n",
       "twitter     0.489157\n",
       "Name: source_feature, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['source_feature'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Engineer a feature to turn `source_feature` into a 1/0 column, where 1 indicates `Twitter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['twitter'] = df['source_feature'].apply(lambda x: 1 if x == 'twitter' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.510843\n",
       "1    0.489157\n",
       "Name: twitter, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['twitter'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE: Since we are solving a classification problem, what potential issue should I check for here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Split our data into `X` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['text_feature']]\n",
    "y = df['twitter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @nowthisnews: Rep. Trey Radel (R- #FL) slam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VIDEO - #Obamacare:  Full of Higher Costs and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Please join me today in remembering our fallen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @SenatorLeahy: 1st step toward Senate debat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.@amazon delivery #drones show need to update ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        text_feature\n",
       "0  RT @nowthisnews: Rep. Trey Radel (R- #FL) slam...\n",
       "1  VIDEO - #Obamacare:  Full of Higher Costs and ...\n",
       "2  Please join me today in remembering our fallen...\n",
       "3  RT @SenatorLeahy: 1st step toward Senate debat...\n",
       "4  .@amazon delivery #drones show need to update ..."
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Split our data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Turn our text into features. [Documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CountVectorizer.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Instantiate our CountVectorizer.\n",
    "cvec = CountVectorizer(\n",
    "    max_features= 500,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit our CountVectorizer on the training data and transform training data.\n",
    "X_train_cvec = pd.DataFrame(cvec.fit_transform(X_train['text_feature']).todense(),\n",
    "                          columns = cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform our testing data with the already-fit CountVectorizer.\n",
    "X_test_cvec = pd.DataFrame(cvec.transform(X_test['text_feature']).todense(),\n",
    "                         columns = cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature analysis\n",
    "\n",
    "# logistic regresion, we had coefficients \n",
    "# we used get feature names to observerve wheter one coefficient \n",
    "# on the given word was big or small \n",
    "\n",
    "# NB we had the relative feature_log_probs \n",
    "# we used get_feature_names to observe whether the relative \n",
    "# feature_log_prob for the given word was big or small \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_compressed = cvec.fit_transform(X_train)\n",
    "X_test_compressed  = cvec.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Fit a Naive Bayes model!\n",
    "\n",
    "<details><summary> Which Naive Bayes model should we pick, and why? </summary>\n",
    "    \n",
    "- The columns of X are all integer counts, so MultinomialNB is the best choice here.\n",
    "- BernoulliNB is best when we have 0/1 counts in all columns of X. (a.k.a. dummy variables)\n",
    "- GaussianNB is best when the columns of X are Normally distributed. (Practically, though, it gets used whenever BernoulliNB and MultinomialNB are inappropriate.)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our model!\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate our model!\n",
    "\n",
    "nb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember earlier that I said we had the opportunity to set priors. We could do so here if we wanted, but we'll stick with the default and allow `sklearn` to estimate priors from the training data directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1, 3666]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-414824ffd011>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Fit our model!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_compressed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    610\u001b[0m         \u001b[0mself\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    611\u001b[0m         \"\"\"\n\u001b[1;32m--> 612\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    613\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_features_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36m_check_X_y\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    475\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 477\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    478\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_update_class_log_prior\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_prior\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    431\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 433\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    434\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    829\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 831\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    832\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    833\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    260\u001b[0m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0m\u001b[0;32m    263\u001b[0m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1, 3666]"
     ]
    }
   ],
   "source": [
    "# Fit our model!\n",
    "\n",
    "model = nb.fit(X_train_compressed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-2bbb3e67f39d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Generate our predictions!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_cvec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate our predictions!\n",
    "\n",
    "predictions = model.predict(X_test_cvec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary> How might we evaluate our model's performance? </summary>\n",
    "\n",
    "- Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "- Sensitivity = TP / (TP + FN)\n",
    "- Specificity = TN / (TN + FP)\n",
    "- Precision = TP / (TP + FP)\n",
    "- AUC ROC\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary> If we have to select only one, which one should we choose? </summary>\n",
    "\n",
    "- It depends on how exactly you define \"positive\" and \"negative.\" In this case, it probably doesn't really matter - incorrectly mistaking a tweet for a Facebook post doesn't seem much better or worse than incorrectly mistaking a Facebook post for a tweet. \n",
    "- Because I believe false positives and false negatives are equally as bad, I'd probably use accuracy.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score our model on the training set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score our model on the testing set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary> What should we do in this case? </summary>\n",
    "\n",
    "- Our model appears *slightly* overfit. We could:\n",
    "    - try to collect more data, \n",
    "    - try using fewer features by setting `max_features` to a smaller number when instantiating our CountVectorizer,\n",
    "    - try TF-IDF Vectorizer,\n",
    "    - try a non-default prior **if you have subject-matter expertise**.\n",
    "- Rather than regularizing, [online answers](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) suggest using a different model entirely.\n",
    "- Our training performance and testing performance are pretty close, though, so there may not be a lot of changes required.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the confusion matrix function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a confusion matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"True Negatives: %s\" % tn)\n",
    "print(\"False Positives: %s\" % fp)\n",
    "print(\"False Negatives: %s\" % fn)\n",
    "print(\"True Positives: %s\" % tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature analysis \n",
    "# --> for the given model, you have to know what the numbers are what they mean\n",
    "# \n",
    "# logistic regression - look at the coefficients \n",
    "# multinomial NB - look at relative FLP and interpret\n",
    "#  XYZ model -- ??\n",
    "\n",
    "\n",
    "# confusion matrix \n",
    "# --> what kind of post does your model do best on \n",
    "# --> what kind of post does your model fail on \n",
    "\n",
    "# Make your own data to test your model \n",
    "#  --> ''To the moon\" <-- test this \n",
    "# -- \"Yesterday i went to NTUC and everyting so expensive!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary> By default, what does a true negative mean here? </summary>\n",
    "\n",
    "- True negatives are things we correctly predict to be negative.\n",
    "- In this case, since Twitter = 1, a true negative means I correctly predict something is a Facebook post.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "<details><summary> By default, what does a false positive mean here? </summary>\n",
    "\n",
    "- False positives are things we falsely predict to be positive.\n",
    "- In this case, since Twitter = 1, a false positive means I incorrectly preidct something is a tweet (when it's really a Facebook post).\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
